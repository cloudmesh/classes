<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Technologies &#8212; Big Data Classes</title>
    
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/bootswatch-3.3.6/cerulean/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     'Draft',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-3.3.6/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lessons" href="../lesson/index.html" />
    <link rel="prev" title="HID Assignment" href="hids-techs.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">
          i524</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="index.html">Overview</a></li>
                <li><a href="lectures.html">Lectures</a></li>
                <li><a href="../lesson/index.html">Lessons</a></li>
                <li><a href="../changelog.html">Changes</a></li>
                <li><a href="https://github.com/cloudmesh/classes">Fork</a></li>
                <li><a href="https://piazza.com/class/ix39m27czn5uw">Piazza</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="preface/index.html">Preface</a><ul>
<li class="toctree-l2"><a class="reference internal" href="preface/about.html">About</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/disclaimer.html">Disclaimer</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/convention.html">Conventions</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/instructors.html">Instructors</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/instructors.html#teaching-assistants">Teaching Assistants</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html">i524 Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#meeting-times">Meeting Times</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#online-meetings">Online Meetings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#who-can-take-the-class">Who can take the class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#homework">Homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#open-source-publication-of-homework">Open Source Publication of Homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#piazza">Piazza</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tips-on-how-to-achieve-your-best">Tips on how to achieve your best</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#submissions">Submissions</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#selected-project-ideas">Selected Project Ideas</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#software-project">Software Project</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#report-format">Report Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#github-repositories">Github repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#code-repositories-deliverables">Code Repositories Deliverables</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#learning-outcomes">Learning Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#academic-integrity-policy">Academic Integrity Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#links">Links</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#course-numbers">Course Numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="calendar.html">I524 Calendar</a><ul>
<li class="toctree-l2"><a class="reference internal" href="calendar.html#comments">Comments</a></li>
<li class="toctree-l2"><a class="reference internal" href="calendar.html#official-university-calendar">Official University calendar</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lectures.html">I524 Lectures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures">Lectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-theory-track">Lectures - Theory Track</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-collaboration-track">Lectures - Collaboration Track</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-systems">Lectures - Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#unreleased-lectures">Unreleased Lectures</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hids-techs.html">HID Assignment</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Technologies</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#workflow-orchestration">Workflow-Orchestration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#application-and-analytics">Application and Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#application-hosting-frameworks">Application Hosting Frameworks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#high-level-programming">High level Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="#streams">Streams</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-programming-model-and-runtime-spmd-mapreduce">Basic Programming model and runtime, SPMD, MapReduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inter-process-communication-collectives">Inter process communication Collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="#in-memory-databases-caches">In-memory databases/caches</a></li>
<li class="toctree-l2"><a class="reference internal" href="#object-relational-mapping">Object-relational mapping</a></li>
<li class="toctree-l2"><a class="reference internal" href="#extraction-tools">Extraction Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sql-newsql">SQL(NewSQL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nosql">NoSQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#file-management">File management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-transport">Data Transport</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cluster-resource-management">Cluster Resource Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#file-systems">File systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interoperability">Interoperability</a></li>
<li class="toctree-l2"><a class="reference internal" href="#devops">DevOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="#iaas-management-from-hpc-to-hypervisors">IaaS Management from HPC to hypervisors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cross-cutting-functions">Cross-Cutting Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#new-technologies-to-be-integrated">New Technologies to be integrated</a></li>
<li class="toctree-l2"><a class="reference internal" href="#excersise">Excersise</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../lesson/index.html">Lessons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../lesson/ansible/index.html">Ansible</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/cloud/index.html">Cloud (under construction)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/contrib/index.html">Contributing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/doc/index.html">Writing Documents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/linux/index.html">Linux</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/org/index.html">Oragnization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/prg/index.html">Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/data/index.html">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/projects/index.html">Software Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/projects/draft.html">Software Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/iaas/index.html">IaaS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/chef.html">Chef</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/comparison.html">Compaprison of Configuration management software</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/composite_cluster.html">Composite Clusters (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/deployment.html">Dynamic deployment of arbitrary X software on VC (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/hadoop.html">Hadoop Virtual Cluster Installation (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/juju.html">Ubuntu Juju</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/mongodb_cluster.html">MongoDB Virtual Cluster (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/openmpi_cluster.html">OpenMPI Virtual Cluster (Under Preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/openstack_heat.html">OpenStack Heat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/other.html">Other (Under Preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/overview.html">DevOps (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/overview_vc.html">Overview Virtual Cluster (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/puppet.html">Puppet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/saltstack-ex1-output.html">Full Message of <code class="docutils literal"><span class="pre">salt-call</span></code> Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/saltstack.html">SaltStack</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-do-i-ask-a-question">How do I ask a question?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-prerequisites-for-this-class">What are the prerequisites for this class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-is-this-class-not-hosted-on-edx">Why is this class not hosted on EdX?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-are-you-not-using-canvas-for-communicating-with-students">Why are you not using CANVAS for communicating with students?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-are-you-using-github-for-submitting-projects-and-papers">Why are you using github for submitting projects and papers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-full-time-student-at-iupui-can-i-take-the-online-version">I am full time student at IUPUI. Can I take the online version?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-a-residential-student-at-iu-can-i-take-the-online-version-only">I am a residential student at IU. Can I take the online version only?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#the-class-is-full-what-do-i-do">The class is full what do I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-to-buy-a-textbook">Do I need to buy a textbook?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-is-there-no-textbook">Why is there no textbook?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-a-computer-to-participate-in-this-class">Do I need a computer to participate in this class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#representative-bibliography">Representative Bibliography</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#where-is-the-official-iu-calendar-for-the-fall">Where is the official IU calendar for the Fall?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-write-a-research-article-on-computer-science">How to write a research article on computer science?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#which-bibliography-manager-is-required-for-the-class">Which bibliography manager is required for the class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-use-endnote-or-other-bibliography-managers">Can I use endnote or other bibliography managers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#plagiarism-test-and-resources-related-to-that">Plagiarism test and resources related to that</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-many-hours-will-this-course-take-to-work-on-every-week">How many hours will this course take to work on every week?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#is-all-classes-material-final">Is all classes material final?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-changes-to-the-web-page">What are the changes to the web page?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-lectures-should-i-learn-when">What lectures should I learn when?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-are-you-doing-the-papers">I524: Why are you doing the papers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-are-there-no-homework-to-test-me-on-skills-such-as-ansible-or-python">I524: Why are there no homework to test me on skills such as ansible or python?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-not-use-chef-or-another-devops-framework">I524: Why not use chef or another DevOps framework?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-lost">I am lost?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-do-not-like-technology-topic-project-etc">I do not like Technology/Topic/Project/etc?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-not-able-to-attend-the-online-hours">I am not able to attend the online hours</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-to-attend-the-online-sessions">Do I need to attend the online sessions?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-leaning-outcomes">What are the leaning outcomes?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#there-are-so-many-messages-on-piazza-i-can-not-keep-up">There are so many messages on Piazza I can not keep up.</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-find-the-hosting-web-confusing">I find the hosting Web confusing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-i-do-not-know-python-what-do-i-do">I524: I do not know python. What do I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-solve-merge-conflict-in-pull-request">How to solve merge conflict in Pull Request?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#building-cloudmesh-classes-in-local-machine">Building cloudmesh/classes in local machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-sole-merge-conflict-in-a-pull-request">How to sole Merge Conflict in a Pull Request?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#cheat-sheet-for-linux-commands">Cheat sheet for Linux commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#tips-techlist-1-homework">Tips: TechList.1 homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#techlist-1-and-paper-1-pagecount">Techlist 1 and Paper 1 : Pagecount</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#tips-to-install-virtualbox">Tips to Install Virtualbox</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-generate-the-ssh-key-on-ubuntu-vm">Do I generate the SSH key on Ubuntu VM ?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#ways-to-run-ubuntu-on-windows-10">Ways to run Ubuntu on Windows 10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#don-t-use-anaconda">Don&#8217;t use Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#using-ssh-key-for-git-push">Using SSH Key for Git Push</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-properly-research-a-bibtex-entry">How to properly research a bibtex entry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-differnt-entry-types-and-fields">What are the differnt entry types and fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-write-the-papers-on-osx">Can I write the papers on OSX?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-is-the-nature-of-team-collaboration-on-papers">What is the nature of team collaboration on papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#id1">What is the nature of team collaboration on papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-due-dates-for-assignments">What are the due dates for assignments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-good-places-to-find-refernce-entries">What are good places to find refernce entries?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#matplotlib-installation">Matplotlib Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../todo.html">Todos</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../todo.html#general">General</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-unreleased">%%version%% (unreleased)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id1">3.0.9 (2017-01-30)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id2">3.0.8 (2017-01-22)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id6">3.0.7 (2017-01-20)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id11">3.0.6 (2017-01-11)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id14">3.0.5 (2017-01-11)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id19">3.0.4 (2017-01-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id20">3.0.3 (2017-01-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id23">3.0.2 (2017-01-07)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id24">3.0.1 (2017-01-06)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id25">3.0 (2017-01-06)</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Technologies</a><ul>
<li><a class="reference internal" href="#workflow-orchestration">Workflow-Orchestration</a></li>
<li><a class="reference internal" href="#application-and-analytics">Application and Analytics</a></li>
<li><a class="reference internal" href="#application-hosting-frameworks">Application Hosting Frameworks</a></li>
<li><a class="reference internal" href="#high-level-programming">High level Programming</a></li>
<li><a class="reference internal" href="#streams">Streams</a></li>
<li><a class="reference internal" href="#basic-programming-model-and-runtime-spmd-mapreduce">Basic Programming model and runtime, SPMD, MapReduce</a></li>
<li><a class="reference internal" href="#inter-process-communication-collectives">Inter process communication Collectives</a></li>
<li><a class="reference internal" href="#in-memory-databases-caches">In-memory databases/caches</a></li>
<li><a class="reference internal" href="#object-relational-mapping">Object-relational mapping</a></li>
<li><a class="reference internal" href="#extraction-tools">Extraction Tools</a></li>
<li><a class="reference internal" href="#sql-newsql">SQL(NewSQL)</a></li>
<li><a class="reference internal" href="#nosql">NoSQL</a></li>
<li><a class="reference internal" href="#file-management">File management</a></li>
<li><a class="reference internal" href="#data-transport">Data Transport</a></li>
<li><a class="reference internal" href="#cluster-resource-management">Cluster Resource Management</a></li>
<li><a class="reference internal" href="#file-systems">File systems</a></li>
<li><a class="reference internal" href="#interoperability">Interoperability</a></li>
<li><a class="reference internal" href="#devops">DevOps</a></li>
<li><a class="reference internal" href="#iaas-management-from-hpc-to-hypervisors">IaaS Management from HPC to hypervisors</a></li>
<li><a class="reference internal" href="#cross-cutting-functions">Cross-Cutting Functions</a><ul>
<li><a class="reference internal" href="#monitoring">Monitoring</a></li>
<li><a class="reference internal" href="#security-privacy">Security &amp; Privacy</a></li>
<li><a class="reference internal" href="#distributed-coordination">Distributed Coordination</a></li>
<li><a class="reference internal" href="#message-and-data-protocols">Message and Data Protocols</a></li>
</ul>
</li>
<li><a class="reference internal" href="#new-technologies-to-be-integrated">New Technologies to be integrated</a></li>
<li><a class="reference internal" href="#excersise">Excersise</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
              <li class="hidden-sm"></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="preface/index.html">Preface</a><ul>
<li class="toctree-l2"><a class="reference internal" href="preface/about.html">About</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/disclaimer.html">Disclaimer</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/convention.html">Conventions</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/instructors.html">Instructors</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/instructors.html#teaching-assistants">Teaching Assistants</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html">i524 Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#meeting-times">Meeting Times</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#online-meetings">Online Meetings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#who-can-take-the-class">Who can take the class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#homework">Homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#open-source-publication-of-homework">Open Source Publication of Homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#piazza">Piazza</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tips-on-how-to-achieve-your-best">Tips on how to achieve your best</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#submissions">Submissions</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#selected-project-ideas">Selected Project Ideas</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#software-project">Software Project</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#report-format">Report Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#github-repositories">Github repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#code-repositories-deliverables">Code Repositories Deliverables</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#learning-outcomes">Learning Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#academic-integrity-policy">Academic Integrity Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#links">Links</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#course-numbers">Course Numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="calendar.html">I524 Calendar</a><ul>
<li class="toctree-l2"><a class="reference internal" href="calendar.html#comments">Comments</a></li>
<li class="toctree-l2"><a class="reference internal" href="calendar.html#official-university-calendar">Official University calendar</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lectures.html">I524 Lectures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures">Lectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-theory-track">Lectures - Theory Track</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-collaboration-track">Lectures - Collaboration Track</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-systems">Lectures - Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#unreleased-lectures">Unreleased Lectures</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hids-techs.html">HID Assignment</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Technologies</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#workflow-orchestration">Workflow-Orchestration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#application-and-analytics">Application and Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#application-hosting-frameworks">Application Hosting Frameworks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#high-level-programming">High level Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="#streams">Streams</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-programming-model-and-runtime-spmd-mapreduce">Basic Programming model and runtime, SPMD, MapReduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inter-process-communication-collectives">Inter process communication Collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="#in-memory-databases-caches">In-memory databases/caches</a></li>
<li class="toctree-l2"><a class="reference internal" href="#object-relational-mapping">Object-relational mapping</a></li>
<li class="toctree-l2"><a class="reference internal" href="#extraction-tools">Extraction Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sql-newsql">SQL(NewSQL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nosql">NoSQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#file-management">File management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-transport">Data Transport</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cluster-resource-management">Cluster Resource Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#file-systems">File systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interoperability">Interoperability</a></li>
<li class="toctree-l2"><a class="reference internal" href="#devops">DevOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="#iaas-management-from-hpc-to-hypervisors">IaaS Management from HPC to hypervisors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cross-cutting-functions">Cross-Cutting Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#new-technologies-to-be-integrated">New Technologies to be integrated</a></li>
<li class="toctree-l2"><a class="reference internal" href="#excersise">Excersise</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../lesson/index.html">Lessons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../lesson/ansible/index.html">Ansible</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/cloud/index.html">Cloud (under construction)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/contrib/index.html">Contributing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/doc/index.html">Writing Documents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/linux/index.html">Linux</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/org/index.html">Oragnization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/prg/index.html">Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/data/index.html">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/projects/index.html">Software Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/projects/draft.html">Software Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/iaas/index.html">IaaS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/chef.html">Chef</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/comparison.html">Compaprison of Configuration management software</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/composite_cluster.html">Composite Clusters (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/deployment.html">Dynamic deployment of arbitrary X software on VC (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/hadoop.html">Hadoop Virtual Cluster Installation (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/juju.html">Ubuntu Juju</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/mongodb_cluster.html">MongoDB Virtual Cluster (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/openmpi_cluster.html">OpenMPI Virtual Cluster (Under Preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/openstack_heat.html">OpenStack Heat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/other.html">Other (Under Preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/overview.html">DevOps (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/overview_vc.html">Overview Virtual Cluster (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/puppet.html">Puppet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/saltstack-ex1-output.html">Full Message of <code class="docutils literal"><span class="pre">salt-call</span></code> Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/saltstack.html">SaltStack</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-do-i-ask-a-question">How do I ask a question?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-prerequisites-for-this-class">What are the prerequisites for this class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-is-this-class-not-hosted-on-edx">Why is this class not hosted on EdX?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-are-you-not-using-canvas-for-communicating-with-students">Why are you not using CANVAS for communicating with students?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-are-you-using-github-for-submitting-projects-and-papers">Why are you using github for submitting projects and papers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-full-time-student-at-iupui-can-i-take-the-online-version">I am full time student at IUPUI. Can I take the online version?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-a-residential-student-at-iu-can-i-take-the-online-version-only">I am a residential student at IU. Can I take the online version only?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#the-class-is-full-what-do-i-do">The class is full what do I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-to-buy-a-textbook">Do I need to buy a textbook?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-is-there-no-textbook">Why is there no textbook?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-a-computer-to-participate-in-this-class">Do I need a computer to participate in this class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#representative-bibliography">Representative Bibliography</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#where-is-the-official-iu-calendar-for-the-fall">Where is the official IU calendar for the Fall?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-write-a-research-article-on-computer-science">How to write a research article on computer science?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#which-bibliography-manager-is-required-for-the-class">Which bibliography manager is required for the class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-use-endnote-or-other-bibliography-managers">Can I use endnote or other bibliography managers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#plagiarism-test-and-resources-related-to-that">Plagiarism test and resources related to that</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-many-hours-will-this-course-take-to-work-on-every-week">How many hours will this course take to work on every week?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#is-all-classes-material-final">Is all classes material final?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-changes-to-the-web-page">What are the changes to the web page?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-lectures-should-i-learn-when">What lectures should I learn when?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-are-you-doing-the-papers">I524: Why are you doing the papers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-are-there-no-homework-to-test-me-on-skills-such-as-ansible-or-python">I524: Why are there no homework to test me on skills such as ansible or python?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-not-use-chef-or-another-devops-framework">I524: Why not use chef or another DevOps framework?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-lost">I am lost?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-do-not-like-technology-topic-project-etc">I do not like Technology/Topic/Project/etc?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-not-able-to-attend-the-online-hours">I am not able to attend the online hours</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-to-attend-the-online-sessions">Do I need to attend the online sessions?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-leaning-outcomes">What are the leaning outcomes?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#there-are-so-many-messages-on-piazza-i-can-not-keep-up">There are so many messages on Piazza I can not keep up.</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-find-the-hosting-web-confusing">I find the hosting Web confusing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-i-do-not-know-python-what-do-i-do">I524: I do not know python. What do I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-solve-merge-conflict-in-pull-request">How to solve merge conflict in Pull Request?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#building-cloudmesh-classes-in-local-machine">Building cloudmesh/classes in local machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-sole-merge-conflict-in-a-pull-request">How to sole Merge Conflict in a Pull Request?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#cheat-sheet-for-linux-commands">Cheat sheet for Linux commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#tips-techlist-1-homework">Tips: TechList.1 homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#techlist-1-and-paper-1-pagecount">Techlist 1 and Paper 1 : Pagecount</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#tips-to-install-virtualbox">Tips to Install Virtualbox</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-generate-the-ssh-key-on-ubuntu-vm">Do I generate the SSH key on Ubuntu VM ?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#ways-to-run-ubuntu-on-windows-10">Ways to run Ubuntu on Windows 10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#don-t-use-anaconda">Don&#8217;t use Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#using-ssh-key-for-git-push">Using SSH Key for Git Push</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-properly-research-a-bibtex-entry">How to properly research a bibtex entry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-differnt-entry-types-and-fields">What are the differnt entry types and fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-write-the-papers-on-osx">Can I write the papers on OSX?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-is-the-nature-of-team-collaboration-on-papers">What is the nature of team collaboration on papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#id1">What is the nature of team collaboration on papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-due-dates-for-assignments">What are the due dates for assignments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-good-places-to-find-refernce-entries">What are good places to find refernce entries?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#matplotlib-installation">Matplotlib Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../todo.html">Todos</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../todo.html#general">General</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-unreleased">%%version%% (unreleased)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id1">3.0.9 (2017-01-30)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id2">3.0.8 (2017-01-22)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id6">3.0.7 (2017-01-20)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id11">3.0.6 (2017-01-11)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id14">3.0.5 (2017-01-11)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id19">3.0.4 (2017-01-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id20">3.0.3 (2017-01-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id23">3.0.2 (2017-01-07)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id24">3.0.1 (2017-01-06)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id25">3.0 (2017-01-06)</a></li>
</ul>
</li>
</ul>
</ul>
</li><ul>
<li><a class="reference internal" href="#">Technologies</a><ul>
<li><a class="reference internal" href="#workflow-orchestration">Workflow-Orchestration</a></li>
<li><a class="reference internal" href="#application-and-analytics">Application and Analytics</a></li>
<li><a class="reference internal" href="#application-hosting-frameworks">Application Hosting Frameworks</a></li>
<li><a class="reference internal" href="#high-level-programming">High level Programming</a></li>
<li><a class="reference internal" href="#streams">Streams</a></li>
<li><a class="reference internal" href="#basic-programming-model-and-runtime-spmd-mapreduce">Basic Programming model and runtime, SPMD, MapReduce</a></li>
<li><a class="reference internal" href="#inter-process-communication-collectives">Inter process communication Collectives</a></li>
<li><a class="reference internal" href="#in-memory-databases-caches">In-memory databases/caches</a></li>
<li><a class="reference internal" href="#object-relational-mapping">Object-relational mapping</a></li>
<li><a class="reference internal" href="#extraction-tools">Extraction Tools</a></li>
<li><a class="reference internal" href="#sql-newsql">SQL(NewSQL)</a></li>
<li><a class="reference internal" href="#nosql">NoSQL</a></li>
<li><a class="reference internal" href="#file-management">File management</a></li>
<li><a class="reference internal" href="#data-transport">Data Transport</a></li>
<li><a class="reference internal" href="#cluster-resource-management">Cluster Resource Management</a></li>
<li><a class="reference internal" href="#file-systems">File systems</a></li>
<li><a class="reference internal" href="#interoperability">Interoperability</a></li>
<li><a class="reference internal" href="#devops">DevOps</a></li>
<li><a class="reference internal" href="#iaas-management-from-hpc-to-hypervisors">IaaS Management from HPC to hypervisors</a></li>
<li><a class="reference internal" href="#cross-cutting-functions">Cross-Cutting Functions</a><ul>
<li><a class="reference internal" href="#monitoring">Monitoring</a></li>
<li><a class="reference internal" href="#security-privacy">Security &amp; Privacy</a></li>
<li><a class="reference internal" href="#distributed-coordination">Distributed Coordination</a></li>
<li><a class="reference internal" href="#message-and-data-protocols">Message and Data Protocols</a></li>
</ul>
</li>
<li><a class="reference internal" href="#new-technologies-to-be-integrated">New Technologies to be integrated</a></li>
<li><a class="reference internal" href="#excersise">Excersise</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<p></p>
<hr>
<small>
<b>Links</b>
<ul>
  <li><a href="https://cloudmesh.github.io/classes/faq.html">FAQ</a> </li>  
  <li><a href="https://cloudmesh.github.io/classes/i524/index.html#online-meetings">Meetings</a> </li>
  <li><a
  href="https://iu.instructure.com/courses/1603897/assignments/syllabus">Zoom</a>
  <li> <a href="https://cloudmesh.github.io/classes/i524/technologies.html#excersise">HW Techlist.1</a>
  </li>
  <li> <a href="https://cloudmesh.github.io/classes/i524/paper1-hw.html#paper-1-homework">HW Paper.1</a>
  </li>
  <li> <a href="https://cloudmesh.github.io/classes/i524/python-homework.html">HW Python</a>
  </li>
  <li> <a href="https://cloudmesh.github.io/classes/i524/techlist-peer-review.html">TechList Peer Review</a>
  </li>
  <li> <a href="https://cloudmesh.github.io/classes/i524/calendar.html#i524-calendar">Calendar</a>
  </li>
  <li> <a href="https://cloudmesh.github.io/classes/"> <img src="https://cloudmesh.github.io/classes/_static/html.jpg" width="40" height="40" alt="pdf"> Web Page</a>
  </li>
  <li>
    <a href="https://cloudmesh.github.io/classes/i524-notes.pdf">
       <img src="https://cloudmesh.github.io/classes/_static/pdf.png" width="40" height="40" alt="pdf"> (Incomplete)</a>
  </li>
</ul>
</small>

<p></p>
<hr>
<form action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
        </div>
      </div>
    <div class="col-md-9 content">
      
  <div class="section" id="technologies">
<span id="index-0"></span><h1>Technologies<a class="headerlink" href="#technologies" title="Permalink to this headline"></a></h1>
<p>In this section we find a number of technologies that are related to
big data. Certainly a number of these projects are hosted as an Apache
project. One important resource for a general list of all apache
projects is at</p>
<ul class="simple">
<li>Apache projects: <a class="reference external" href="https://projects.apache.org/projects.html?category">https://projects.apache.org/projects.html?category</a></li>
</ul>
<div class="section" id="workflow-orchestration">
<h2>Workflow-Orchestration<a class="headerlink" href="#workflow-orchestration" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p class="first">ODE</p>
<p>Apache ODE (Orchestration Director Engine) is an open source
implementation of the WS-BPEL 2.0 standard. WS- BPEL which stands for
Web Services Business Process Execution Language, is an executable
language for writing business processes with web services <a class="reference internal" href="#www-bpel-wiki" id="id1">[1]</a>.
It includes control structures like conditions or loops as well as
elements to invoke web services and receive messages from services.
ODE uses WSDL (Web Services Description Language) for interfacing
with web services <a class="reference internal" href="#www-ode-wiki" id="id2">[2]</a>. Naming a few of its features,
It supports two communication layers for&nbsp;interacting with the outside
world, one based on Axis2 (Web Services http transport) and another
one based on the JBI standard. It also supports both long and short
living process executions for orchestrating services for applications <a class="reference internal" href="#www-ode-web" id="id3">[3]</a>.</p>
</li>
<li><p class="first">ActiveBPEL</p>
<p>Business Process Execution Language for Web Services (BPEL4WS or
just BPEL) is an XML-based grammar for describing the logic to
coordinate and control web services that seamlessly integrate
people, processes and systems, increasing the efficiency and
visibility of the business. ActiveBPEL is a robust Java/J2EE
runtime environment that is capable of executing process
definitions created to the Business Process Execution Language for
Web Services. The ActiveBPEL also provides an administration
interface that is accessible via web service invocations;and it can
also be use to administer, to control and to integrate web services
into a larger application. <a class="reference internal" href="#www-bpel" id="id4">[4]</a></p>
</li>
<li><p class="first">Airavata</p>
</li>
<li><p class="first">Pegasus</p>
<p>The Pegasus <a class="reference internal" href="#www-pegasus" id="id5">[5]</a> is workflow management system
that alows to compose and execute a workflow in an application
in different environment without the need  for any
modifications.&nbsp;It allows users to make high level workflow
without thinking about the low level details. It locates
the required input data and computational resources automatically.
Pegasus also maintains information about tasks done and data
produced. In case of errors Pegasus tries to recover by retrying
the whole workflow and providing check pointing at workflow-level.
It cleans up the storage as the workflow gets executed so that
data-intensive workflows can have enough required space to execute
on storage-constrained resources. Some of the other advantages of
Pegasus are:scalability, reliability and high performance. Pegasus
has been used in many scientific domains like astronomy,
bioinformatics, earthquake science , ocean science, gravitational
wave physics and others.</p>
</li>
<li><p class="first">Kepler</p>
<p>Kepler, scientific workflow application, is designed to help scientist,
analyst, and computer programmer create, execute and share models and
analyses across a broad range of scientific and engineering disciplines.
Kepler can operate on data stored in a variety of formats, locally and
over the internet, and is an effective environment for integrating
disparate software components such as merging &#8220;R&#8221; scripts with compiled &#8220;C&#8221;
code, or facilitating remote, distributed execution of models. Using Kepler&#8217;s
GUI, users can simply select and then connect pertinent analytical components
and data sources to create a &#8220;scientific workflow&#8221;. Overall, the Kepler helps
users share and reuse data, workflow, and components developed by the scientific
community to address common needs <a class="reference internal" href="#www-kepler" id="id6">[6]</a>.</p>
</li>
<li><p class="first">Swift</p>
</li>
<li><p class="first">Taverna</p>
<p>Taverna is workflow management system. According to
<a class="reference internal" href="#www-taverna" id="id7">[7]</a>, Taverna is transitioning to Apache Incubator
as of Jan 2017.  Taverna suite includes 2 products:</p>
<p>(1). Taverna Workbench is desktop client where user can define the workflow.
(2). Taverna Server is responsible for executing the remote workflows.</p>
<p>Taverna workflows can also be executed on command-line.  Taverna
supports wide range of services including WSDL-style and RESTful
Web Services, BioMart, SoapLab, R, and Excel. Taverna also support
mechanism to monitor the running workflows using its web browser
interface.  In the <a class="reference internal" href="#taverna-paper" id="id8">[8]</a> paper, the formal syntax and
operational semantics of Taverna is explained.</p>
</li>
<li><p class="first">Triana</p>
<p><a class="reference internal" href="#trianadocumentation-1" id="id9">[9]</a> Triana is an open source problem
solving software that comes with powerful data analysis tools.  Having
been developed at Cardiff University, it has a good and
easy-to-understand User Interface and is typically used for signal,
text and image processing.  Although it has its own set of analysis
tools, it can also easily be integrated with custom tools.  Some of
the already available toolkits include signal-analysis toolkit, an
image-manipulation toolkit, etc.  Besides, it also checks the data
types and reports the usage of any incompatible tools.  It also
reports errors, if any, as well as useful debug messages in order to
resolve them.  It also helps track serious bugs, so that the program
does not crash.  It has two modes of representing the data - a
text-editor window or a graph-display window.  The graph-display
window has the added advantage of being able to zoom in on particular
features.  Triana is specially useful for automating the repetitive
tasks, like finding-and-replacing a character or a string.</p>
</li>
<li><p class="first">Trident</p>
<p>In <a class="reference internal" href="#www-trident-tutorial" id="id10">[10]</a>, it is explained that Apache Trident
is a &#8220;high-level abstraction for doing realtime computing on top of
[Apache] Storm.&#8221; Similarly to Apache Storm, Apache Trident was
developed by Twitter. Furthermore, <a class="reference internal" href="#www-trident-tutorial" id="id11">[10]</a>
introduces Trident as a tool that &#8220;allows you to seamlessly intermix
high throughput (millions of messages per second), stateful stream
processing with low latency distributed querying.&#8221; In
<a class="reference internal" href="#www-trident-overview" id="id12">[11]</a>, the five kinds of operations in
Trident are described as &#8220;Operations that apply locally to each
partition and cause no network transfer&#8221;, &#8220;repartitioning operations
that repartition a stream but otherwise don&#8217;t change the contents
(involves network transfer)&#8221;, &#8220;aggregation operations that do
network transfer as part of the operation&#8221;, &#8220;operations on grouped
streams&#8221; and &#8220;merges and joins.&#8221; In <a class="reference internal" href="#www-trident-tutorial" id="id13">[10]</a>,
these five kinds of operations (i.e. joins, aggregations, grouping,
functions, and filters) and the general concepts of Apache Trident
are described as similar to &#8220;high level batch processing tools like
Pig or Cascading.&#8221;</p>
</li>
<li><p class="first">BioKepler</p>
<p>BioKepler is a Kepler module of scientific workflow components to
execute a set of bioinformatics tools using distributed execution
patterns <a class="reference internal" href="#www-biokepler" id="id14">[12]</a>. It contains a specialized set of
actors called bioActors for running bioinformatic tools,
directors providing distributed data-parallel(DPP) execution on
Big Data platforms such as Hadoop and Spark they are also
configurable and reusable <a class="reference internal" href="#www-biokepler-demos" id="id15">[13]</a>. BioKepler
contains over 40 example workflows that demonstrate the actors and
directors <a class="reference internal" href="#bioactors" id="id16">[14]</a>.</p>
</li>
<li><p class="first">Galaxy</p>
<p>Ansible Galaxy is a website platform and command line tool that
enables users to discover, create, and share community developed
roles. Users&#8217; GitHub accounts are used for authentication,
allowing users to import roles to share with the ansible
community. <a class="reference internal" href="#www-galaxy-ansible" id="id17">[15]</a> describes how Ansible roles
are encapsulated and reusable tools for organizing automation
content. Thus a role contains all tasks, variables, and handlers
that are necessary to complete that
role. <a class="reference internal" href="#ansible-book-2016" id="id18">[16]</a> depicts roles as the most powerful
part of Ansible as they keep playbooks simple and readable. &#8220;They
provide reusable definitions that you can include whenever you
need and customize with any variables that the role exposes.&#8221;
<a class="reference internal" href="#www-github-galaxy" id="id19">[17]</a> provides the project documents for
Ansible Galaxy on github.</p>
</li>
<li><p class="first">IPython</p>
</li>
<li><p class="first">Jupyter</p>
</li>
<li><p class="first">(Dryad)</p>
</li>
<li><p class="first">Naiad</p>
<p>Naiad <a class="reference internal" href="#paper-naiad" id="id20">[18]</a> is a distributed system based on
computational model called &#8220;Timely Dataflow&#8221; developed for
execution of data-parallel, cyclic dataflow programs. It provides
an in-memory distributed dataflow framework which exposes control
over data partitioning and enables features like the high
throughput of batch processors, the low latency of stream
processors, and the ability to perform iterative and incremental
computations. The Naiad architecture consists of two main
components: (1) incremental processing of incoming updates and (2)
low-latency real-time querying of the application state.</p>
<p>Compared to other systems supporting loops or streaming
computation, Naiad provides support for the combination of the
two, nesting loops inside streaming contexts and indeed other
loops, while maintaining a clean separation between the many
reasons new records may flow through the computation
<a class="reference internal" href="#www-naiad" id="id21">[19]</a>.</p>
<p>This model enriches dataflow computation with timestamps that
represent logical points in the computation and provide the basis
for an efficient, lightweight coordination mechanism.  All the
above capabilities in one package allows development of High-level
programming models on Naiad which can perform tasks as streaming
data analysis, iterative machine learning, and interactive graph
mining. On the contrary, it&#8217;s public reusable low-level
programming abstractions leads Naiad to outperforms many other
data parallel systems that enforce a single high-level programming
model.</p>
</li>
<li><p class="first">Oozie</p>
<p>Oozie is a workflow manager and scheduler. Oozie is designed to scale in a
Hadoop cluster. Each job will be launched from a different datanode
<a class="reference internal" href="#paper-oozie" id="id22">[20]</a> <a class="reference internal" href="#www-oozie1" id="id23">[21]</a> .
Oozie <a class="reference internal" href="#www-oozie2" id="id24">[22]</a> is architected from the ground up for large-scale
Hadoop workflow. Scales to meet the demand, provides a multi-tenant service,
is secure to protect data and processing, and can be operated cost effective
ly. As demand for workflow and the sophistication of applications increase,
it must continue to mature in these areas <a class="reference internal" href="#paper-oozie" id="id25">[20]</a>.Is well integr
ated with Hadoop security. Is the only workflow manager with built-in Hadoo
p actions, making workflow development, maintenance and troubleshooting easi
er. Its UI makes it easier to drill down to specific errors in the data
nodes. Proven to scale in some of the worlds largest clusters
<a class="reference internal" href="#paper-oozie" id="id26">[20]</a>. Gets callbacks from MapReduce jobs so it knows when
they finish and whether they hang without expensive polling. Oozie Coordinat
or allows triggering actions when files arrive at HDFS. Also supported by
Hadoop vendors <a class="reference internal" href="#paper-oozie" id="id27">[20]</a>.</p>
</li>
<li><p class="first">Tez</p>
</li>
<li><p class="first">Google FlumeJava</p>
</li>
<li><p class="first">Crunch</p>
</li>
<li><p class="first">Cascading</p>
<p><a class="reference internal" href="#www-cascading" id="id28">[23]</a> Cascading software authored by Chris Wensel
is development platform for building the application in Hadoop.
It basically act as an abstraction for Apache Hadoop used for
creating complex data processing workflow using the scalability of
hadoop however hiding the complexity of mapReduce jobs.  User can
write their program in java without having knowledge of
mapReduce. Applications written on cascading are portable.</p>
<p>Cascading Benefits
1. With Cascading application can be scaled as per the data sets.
2. Easily Portable
3. Single jar file for application deployment.</p>
</li>
<li><p class="first">Scalding</p>
</li>
<li><p class="first">e-Science Central</p>
<p>In <a class="reference internal" href="#e-science-central-paper-2010" id="id29">[24]</a>, it is explained
that e-Science Central is designed to address some of the
pitfalls within current Infrastructure as a Service (e.g.
Amazon EC2) and Platform as a Service (e.g. force.com)
services. For instance, in
<a class="reference internal" href="#e-science-central-paper-2010" id="id30">[24]</a>, the &#8220;majority of
potential scientific users, access to raw hardware is of
little use as they lack the skills and resources needed to
design, develop and maintain the robust, scalable
applications they require&#8221; and furthermore &#8220;current
platforms focus on services required for business
applications, rather than those needed for scientific
data storage and analysis.&#8221; In
<a class="reference internal" href="#www-e-science-central" id="id31">[25]</a>, it is explained that
e-Science Central is a &#8220;cloud based platform for
data analysis&#8221; which is &#8220;portable and can be run on
Amazon AWS, Windows Azure or your own hardware.&#8221; In
<a class="reference internal" href="#e-science-central-paper-2010" id="id32">[24]</a>, e-Science Central
is further described  as a platform, which &#8220;provides
both Software and Platform as a Service for scientific
data management, analysis and collaboration.&#8221; This
collaborative platform is designed to be scalable while
also maintaining ease of use for scientists. In
<a class="reference internal" href="#e-science-central-paper-2010" id="id33">[24]</a>, &#8220;a project
consisting of chemical modeling by cancer researchers&#8221;
demonstrates how e-Science Central &#8220;allows scientists to
upload data, edit and run workflows, and share results in
the cloud.&#8221;</p>
</li>
<li><p class="first">Azure Data Factory</p>
<p>Azure data factory is a cloud based data integration service that
can ingest data from various sources, transform/ process data and
publish the result data to the data stores. A data management
gateway enables access to data on SQL Databases
<a class="reference internal" href="#azure-df" id="id34">[26]</a>. The data processing is done by It works by
creating pipelines to transform the raw data into a format that
can be readily used by BI Tools or applications. The services
comes with rich visualization aids that aid data analysis. Data
Factory supports two types of activities: data movement activities
and data transformation activities. Data Movement <a class="reference internal" href="#azure-ms" id="id35">[27]</a>
is a Copy Activity in Data Factory that copies data from a data
source to a Data sink. Data Factory supports the following data
stores. Data from any source can be written to any sink.  Data
Transformation: Azure Data Factory supports the following
transformation activities such as Map reduce, Hive transformations
and Machine learning activities.  Data factory is a great tool to
analyze web data, sensor data and geo-spatial data.</p>
</li>
<li><p class="first">Google Cloud Dataflow</p>
<p>Google Cloud Dataflow is a unified programming model and a managed
service for developing and executing a wide variety of data processing
patterns (pipelines). Dataflow includes SDKs for defining data
processing workflows and a Cloud platform managed services to run
those workflows on a Google cloud platform resources such as Compute
Engine, BigQuery amongst others <a class="reference internal" href="#www-dataflow" id="id36">[28]</a>. Dataflow
pipelines can operate in both batch and streaming mode. The platform
resources are provided on demand, allowing users to scale to meet
their requirements, its also optimized to help balance lagging work
dynamically.</p>
<p>Being a cloud offering, Dataflow is designed to allow users to focus
on devising proper analysis without worrying about the installation
and maintaining <a class="reference internal" href="#www-googlelivestream" id="id37">[29]</a> the underlying data
piping and process infrastructure.</p>
</li>
<li><p class="first">NiFi (NSA)</p>
<p><a class="reference internal" href="#www-nifi" id="id38">[30]</a> Defines NiFi as &#8220;An Easy to use, powerful and
realiable system to process and distribute data&#8221;.
This tool aims
at automated data flow from sources with different sizes ,
formats and following diffent protocals to the centralized
location or destination. <a class="reference internal" href="#www-hortanworks" id="id39">[31]</a>.</p>
<p>This comes equipped with an easy use UI where the data flow
can be conrolled with a drag and a drop.
NiFi was initiatially developed by NSA ( called Niagarafiles )
using the concepts of flowbased
programming and latter submitted to Apachi Software
foundation. <a class="reference internal" href="#www-forbes" id="id40">[32]</a></p>
</li>
<li><p class="first">Jitterbit</p>
<p>Jitterbit <a class="reference internal" href="#datasheet" id="id41">[33]</a> is an integration tool that delivers a
quick, flexible and simpler approach to design, configure, test,
and deploy integration solutions. It delivers powerful, flexible,
and easy to use integration solutions that connect modern on
premise, cloud, social, and mobile infrastructures. Jitterbit
employs high performance parallel processing algorithms to handle
large data sets commonly found in ETL initiatives
<a class="reference internal" href="#www-jitetl" id="id42">[34]</a>. This allows easy synchronization of disparate
computing platforms quickly. The Data Cleansing and Smart
Reconstruction tools provides complete reliability in data
extraction, transformation and loading.</p>
<p>Jitterbit employs a no-code GUI (graphical user interface) and
work with diverse applications such as : ETL
(extract-transform-load), SaaS (Software as a Service),SOA
(service-oriented architecture).</p>
<p>Thus it provides centralized platform with power to control all
data. It supports many document types and protocols: XML, web
services, database, LDAP, text, FTP, HTTP(S), Flat and Hierarchic
file structures and file shares <a class="reference internal" href="#tech-manual" id="id43">[35]</a>. It is
available for Linux and Windows, and is also offered through
Amazon EC2 (Amazon Elastic Compute Cloud). Jitterbit Data Loader
for Salesforce is a free data migration tool that enables
Salesforce administrators automated import and export of data
between flat files, databases and Salesforce.</p>
</li>
<li><p class="first">Talend</p>
<p>Talend is Apache Software Foundation sponsor Big data integration tool design to
ease the development and integration and management of big data, Talend provides
well optimised auto generated code to load transform, enrich and cleanse data inside
Hadoop, where one dont need to learn write and maintain Hadoop and spark code.
The product has 900+ inbuild components feature data integration</p>
<p>Talend features multiple products that simplify the digital transformation tools
such as Big data integration, Data integration, Data Quality, Data Preparation,
Cloud Integration, Application Integration, Master Data management, Metadata Manager.
Talend Integration cloud is secure and managed integration Platform-as-a-service (iPaas),
for connecting, cleansing and sharing cloud on premise data.</p>
</li>
<li><p class="first">Pentaho</p>
<p>Pentaho is a business intelligence corporation that provides data
mining, reporting, dashboarding and data integration
capabilities. Generally, organizations tend to obtain meaningful
relationships and useful information from the data present with
them. Pentaho addresses the obstacles that obstruct them from
doing so <a class="reference internal" href="#pent1" id="id44">[36]</a>. The platform includes a wide range of
tools that analyze, explore, visualize and predict data easily
which simplifies blending any data. The sole objective of pentaho
is to translate data into value. Being an open and extensible
source, pentaho provides big data tools to extract, prepare and
blend any data <a class="reference internal" href="#pent2" id="id45">[37]</a>. Along with this, the visualizations
and analytics will help in changing the path that the
organizations follow to run their business. From spark and hadoop
to noSQL, pentaho transforms big data into big insights.</p>
</li>
<li><p class="first">Apatar</p>
</li>
<li><p class="first">Docker Compose</p>
<p>Docker is an open-source container based technology.A container
allows a developer to package up an application and all its part
includig the stack it runs on, dependencies it is associated with
and everything the application requirs to run within an isolated
enviorment . Docker seperates Application from the underlying
Operating System in a similar way as Virtual Machines seperates
the Operating System from the underlying Hardware.Dockerizing an
application is very lightweight in comparison with running the
application on the Virtual Machine as all the containers share the
same underlying kernel, the Host OS should be same as the
container OS (eliminating guest OS) and an average machine cannot
have more than few VMs running o them.</p>
<p>:cite:&#8217;docker-book&#8217; Docker Machine is a tool that lets you install
Docker Engine on virtual hosts, and manage the hosts with
docker-machine commands. You can use Machine to create Docker
hosts on your local Mac or Windows box, on your company network,
in your data center, or on cloud providers like AWS or Digital
Ocean. For Docker 1.12 or higher swarm mode is integerated with
the Docker Engine, but on the older versions with Machine&#8217;s swarm
option, we can configure a swarm cluster Docker Swarm provides
native clustering capabilities to turn a group of Docker engines
into a single, virtual Docker Engine. With these pooled resources
,:cite:&#8217;www-docker&#8216;&#8220;you can scale out your application as if it
were running on a single, huge computer&#8221; as swarm can be scaled
upto 1000 Nodes or upto 50,000 containers</p>
</li>
<li><p class="first">KeystoneML</p>
</li>
</ol>
</div>
<div class="section" id="application-and-analytics">
<h2>Application and Analytics<a class="headerlink" href="#application-and-analytics" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="32">
<li><p class="first">Mahout <a class="reference internal" href="#www-mahout" id="id46">[38]</a></p>
<p>&#8220;Apache Mahout software provides three major features:
(1) A simple and extensible programming environment and framework
for building scalable algorithms
(2) A wide variety of premade algorithms for Scala + Apache Spark,
H2O, Apache Flink
(3) Samsara, a vector math experimentation environment with R-like
syntax which works at scale&#8221;</p>
</li>
<li><p class="first">MLlib</p>
</li>
<li><p class="first">Mbase</p>
</li>
<li><p class="first">DataFu</p>
<p>The Apache DataFu project was created out of the need for stable,
well-tested libraries for large scale data processing in Hadoop.
As detailed in <a class="reference internal" href="#www-datafu" id="id47">[39]</a> Apache DatFu consists of two
libraries Apache DataFu Pig and Apache DataFu Hourglass.  Apache
DataFu Pig is a collection of useful user-defined functions for
data analysis in Apache Pig. The functions are in areas of
Statistics, Bag Operations, Set Operations, Sessions, Sampling,
Estimation, Hashing and Link Analysis.  Apache DataFu Hourglass is
a library for incrementally processing data using Hadoop
MapReduce. It is designed to make computations over sliding windows
more efficient. For these types of computations, the input data is
partitioned in some way, usually according to time, and the range
of input data to process is adjusted as new data arrives.
Hourglass works with input data that is partitioned by day, as
this is a common scheme for partitioning temporal data.</p>
</li>
<li><p class="first">R</p>
<p>R, a GNU project, is a successor to S - a statistical programming
language. It offers a range of capabilities  programming
language, high level graphics, interfaces to other languages and
debugging. &#8220;R is an integrated suite of software facilities for
data manipulation, calculation and graphical display&#8221;. The
statistical and graphical techniques provided by R make it popular
in the statistical community. The statistical techniques provided
include linear and nonlinear modelling, classical statistical
tests, time-series analysis, classification and clustering to name
a few <a class="reference internal" href="#www-r" id="id48">[40]</a>. The number of packages available in R has
made it popular for use in machine learning, visualization, and
data operations tasks like data extraction, cleaning, loading,
transformation, analysis, modeling and visualization. It&#8217;s
strength lies in analyzing data using its rich library but falls
short when working with very large datasets <a class="reference internal" href="#book-r" id="id49">[41]</a>.</p>
</li>
<li><p class="first">pbdR</p>
<p>Programming with Big Data in R (pbdR) <a class="reference internal" href="#www-pbdr" id="id50">[42]</a> is an
environment having series of R packages for statistical computing
with Big Data using high-performance statistical computation. It
uses R, a popular language between statisticians and data
miners. &#8220;pbdR&#8221; focuses on distributed memory system, where data is
distributed accross several machines and processed in batch
mode. It uses MPI for inter process communications. R focuses on
single machines for data analysis using a interactive
GUI. Currenly there are two implementation of pbdR, one Rmpi and
another being pdbMpi.  Rmpi uses SPMD parallelism while pbdRMpi
uses manager/worker parallelism.</p>
</li>
<li><p class="first">Bioconductor</p>
<p>Bioconductor is an open source and open development platform used
for analysis and understanding of high throughput genomic
data. Bioconductor is used to analyze DNA microarray, flow,
sequencing, SNP, and other biological data. All contributions to
Bioconductor are under an open source
license. <a class="reference internal" href="#bioconductor-article-2004" id="id51">[43]</a> describes the goals of
Bioconductor &#8220;include fostering collaborative development and
widespread use of innovative software, reducing barriers to entry
into interdisciplinary scientific research, and promoting the
achievement of remote reproducibility of research results&#8221;
<a class="reference internal" href="#www-bioconductor-about" id="id52">[44]</a> described that Bioconductor is
primarily based on R, as most components of Bioconductor are
released in R packages. Extensive documentation is provided for
each Bioconductor package as vignettes, which include
task-oriented descriptions for the functionalities of each
package. Bioconductor has annotation functionality to associate
&#8220;genemoic data in real time with biological metadata from web
databases such as GenBank, Entrez genes and PubMed.&#8221;  Bioconductor
also has tools to process genomic annotation data.</p>
</li>
<li><p class="first">ImageJ</p>
<p>ImageJ is a  Java-based image processing program developed at the National
Institutes of Health (NIH). ImageJ was designed with an open architecture
that provides extensibility via Java plugins and recordable macros.
Using ImageJ&#8217;s built-in editor and a Java compiler, it has enabled to solve
many image processing and analysis problems in scientifif research from
three-dimensional live-cell imaging to radiological image processing.
ImageJ&#8217;s plugin architecture and built-in development environment has made
it a popular platform for teaching image processing. <a class="reference internal" href="#www-imagej" id="id53">[45]</a></p>
</li>
<li><p class="first">OpenCV</p>
<p>OpenCV stands for Open source Computer Vision. It was designed for
computational efficiency and with a strong focus on real-time
applications. It has C++, C, Python and Java interfaces and
supports Windows, Linux, Mac OS, iOS and Android. It can take
advantage of the hardware acceleration of the underlying
heterogeneous compute platform as it is enabled with OpenCL(Open
Computing Language) <a class="reference internal" href="#www-opencv" id="id54">[46]</a>. OpenCV 3.2 is the latest
version of the software that is currently available
<a class="reference internal" href="#opencv-version" id="id55">[47]</a>.</p>
</li>
<li><p class="first">Scalapack</p>
<p>ScaLAPACK is a library of high-performance linear algebra routines for
parallel distributed memory machines. It solves dense and banded linear
systems, least squares problems, eigenvalue problems, and singular
value problems. It is designed for heterogeneous computing and is
portable on any computer that supports Message Passing Interface or
Parallel Virtual Machine. <a class="reference internal" href="#git-scalapack" id="id56">[48]</a></p>
<p>ScaLAPACK is a open source software package and is available from netlib
via anonymous ftp and the World Wide Web. It contains driver routines  for
solving standard types of problems, computational routines  to perform a
distinct computational task, and auxiliary routines  to perform a certain
subtask or common low-level computation. ScaLAPACK routines are based on
block-partitioned algorithms in order to minimize the frequency of data
movement between different levels of the memory hierarchy.</p>
</li>
<li><p class="first">PetSc</p>
</li>
<li><p class="first">PLASMA MAGMA</p>
<p>PLASMA is built to address the performance shortcomings of the LAPACK and
ScaLAPACK libraries on multicore processors and multi-socket systems of
multicore processors and their inability to efficiently utilize accelerators
such as Graphics Processing Units (GPUs). Real arithmetic and complex
arithmetic are supported in both single precision and double precision.
PLASMA has been designed by restructuring the software to achieve much
greater efficiency, where possible, on modern computers based on multicore
processors. PLASMA does not support band matrices and does not solve
eigenvalue and singular value problems. Also, PLASMA does not replace
ScaLAPACK as software for distributed memory computers, since it only
supports shared-memory machines. <a class="reference internal" href="#paper-plasma-magma-1" id="id57">[49]</a> <a class="reference internal" href="#www-plasma-1" id="id58">[50]</a>
Recent activities of major chip manufacturers, such as Intel, AMD, IBM and
NVIDIA, make it more evident than ever that future designs of
microprocessors and large HPC systems will be hybrid/heterogeneous in
nature, relying on the integration (in varying proportions) of two major
types of components: <a class="reference internal" href="#paper-plasma-magma-2" id="id59">[51]</a> <a class="reference internal" href="#paper-plasma-magma-3" id="id60">[52]</a>
1. Many-cores CPU technology, where the number of cores will continue to
escalate because of the desire to pack more and more components on a chip
while avoiding the power wall, instruction level parallelism wall, and the
memory wall;
2. Special purpose hardware and accelerators, especially Graphics Processing
Units (GPUs), which are in commodity production, have outpaced standard CPUs
in floating point performance in recent years, and have become as easy, if
not easier to program than multicore CPUs.
While the relative balance between these component types in future designs
is not clear, and will likely to vary over time, there seems to be no doubt
that future generations of computer systems, ranging from laptops to
supercomputers, will consist of a composition of heterogeneous components.
<a class="reference internal" href="#paper-plasma-magma-4" id="id61">[53]</a><a class="reference internal" href="#paper-plasma-magma-5" id="id62">[54]</a><a class="reference internal" href="#paper-plasma-magma-6" id="id63">[55]</a></p>
</li>
<li><p class="first">Azure Machine Learning</p>
<p>Azure Machine Learning is a cloud based service that can be used
to do predictive analytics, machine learning or data mining. It
has features like in-built algorithm library, machine learning
studio and a webservice <a class="reference internal" href="#www-azuremlsite" id="id64">[56]</a>. In built
algorithm library has implementation of various popular machine
learning algorithms like decision tree, SVM, linear regression,
neural networks etc. Machine learning studio facilitates creation
of predictive models using graphical user interface by dragging,
dropping and connecting of different modules that can be used by
people with minimal knowledge in the machine learning
field. Machine learning studio is a free service for basic version
and comes with a monthly charge for advanced versions. Apart from
building models, studio also has options to do preprocessing like
clean, transform and normalize the data. Webservice provides
option to deploy the machine learning algorithm as ready to
consume APIs that can be reused in future with minimal effort and
can also be published.</p>
</li>
<li><p class="first">Google Prediction API &amp; Translation API</p>
<p>Google Prediction API &amp; Translation API are part of Cloud ML API
family with specific roles. Below is a description of each and
their use.</p>
<p>Google Prediction API provides pattern-matching and machine
learning capabilities. Built on HTTP and JSON, the prediction API
uses training data to learn and consecutively use what has been
learned to predict a numeric value or choose a category that
describes new pieces of data. This makes it easier for any
standard HTTP client to send requests to it and parse the
responses. The API can be used to predict what users might like,
categorize emails as spam or non-spam, assess whether posted
comments sentiments are positive or negative or how much a user
may spend in a day. Prediction API has a 6 month limited free
trial or a paid use for $10 per project which offers up to 10,000
predictions a day <a class="reference internal" href="#www-prediction" id="id65">[57]</a>.</p>
<p>Google Translation API is a simple programmatic interface for
translating an arbitrary string into any supported
language. Google Translation API is highly responsive allowing
websites and applications to integrate for fast dynamic
translation of source text from source language to a target
language. Translation API also automatically identifies and
translate languages with a high accuracy from over a hundred
different languages.  Google Translation API is charged at $20 per
million characters making it an affordable localization
solution. Translation API is also distributed in two editions,
premium edition which is tailored for users with precise long-form
translation services like livestream, high volumes of emails or
detailed articles and documents. Theres also standard edition
which is tailored for short, real-time
conversations <a class="reference internal" href="#www-translation" id="id66">[58]</a>.</p>
</li>
<li><p class="first">mlpy</p>
<p>mlpy is an open source python library made for providing
machine learning functionality.It is built on top of popular
existing python libraries of NumPy, SciPy and GNU scientific
libraries (GSL).It also makes extensive use of Cython
language. These form the prerequisites for mlpy. <a class="reference internal" href="#dblp-journals-corr-abs-1202-6548" id="id67">[59]</a>
explains the significanceq of its components: NumPy, SciPy provide
sophisticated N-dimensional arrays, linear algebra functionality
and a variety of learning methods, GSL, which is written in C,
provides complex numerical calculation functionality.</p>
<p>mlpy provides a wide range of machine learning methods for both
supervised and unsupervised learning problems. mlpy is multiplatform
and works both on Python 2 and 3 and is distributed under GPL3. Mlpy
provides both classic and new learning algorithms for classification,
regression and dimensionality reduction. <a class="reference internal" href="#www-mlpy" id="id68">[60]</a>
provides a detailed list of functionality offered by mlpy. Though
developed for general machine learning applications, mlpy has special
applications in computational biology, particularly in functional
genomics modeling.</p>
</li>
<li><p class="first">scikit-learn</p>
<p>Scikit-learn is an open source library that provides simple and
efficient tools for data analysis and data mining. It is
accessible to everybody and reusable in various contexts. It is
built on numpy, Scipy and matplotlib and is commercially usable as
it is distributed under many linux distributions
<a class="reference internal" href="#scik1" id="id69">[61]</a>. Through a consistent interface, scikit-learn
provides a wide range of learning algorithms. Scikits are the
names given to the modules for SciPy, a fundamental library for
scientific computing and as these modules provide different
learning algorithms, the library is named as sciki-learn
<a class="reference internal" href="#scik2" id="id70">[62]</a>. It provides an in-depth focus on code quality,
performance, collaboration and documentation. Most popular models
provided by scikit-learn include clustering, cross-validation,
dimensionality reduction, parameter tuning, feature selection and
extraction.</p>
</li>
<li><p class="first">PyBrain</p>
</li>
<li><p class="first">CompLearn</p>
<p>Complearn is a system that makes use of data compression
methodologies for mining patterns in a large amount of data. So,
it is basically a compression-based machine learning system. For
identifying and learning different patterns, it provides a set of
utilities which can be used in applying standard compression
mechanisms. The most important characteristic of complearn is its
power in mining patterns even in domains that are unrelated. It
has the ability to identify and classify the language of different
bodies of text <a class="reference internal" href="#comp1" id="id71">[63]</a>. This helps in reducing the work of
providing background knowledge regarding a particular
classification. It provides such generalization through a library
that is written in ANSI C which is portable and works in many
environments <a class="reference internal" href="#comp1" id="id72">[63]</a>. Complearn provides immediate to access
every core functionality in all the major languages as it is
designed to be extensible.</p>
</li>
<li><p class="first">DAAL(Intel)</p>
</li>
<li><p class="first">Caffe</p>
<p>Caffe is a deep learning framework made with three terms namely
expression, speed and modularity <a class="reference internal" href="#www-caffe" id="id73">[64]</a>. Using Expressive
architecture, switching between CPU and GPU by setting a single
flag to train on a GPU machine then deploy to commodity cluster or
mobile devices.Here the concept of configuration file will comes
without hard coding the values . Switching between CPU and GPU can
be done by setting a flag to train on a GPU machine then deploy to
commodity clusters or mobile devices.</p>
<p>It can process over 60 million images per day with a single NVIIA
k40 GPU It is being used bu academic research projects, startup
prototypes, and even large-scale industrial applications in vision,
speech, and multimedia.</p>
</li>
<li><p class="first">Torch</p>
<p>Torch is a open source machine learning library, a scientific
computing framework <a class="reference internal" href="#www-torch" id="id74">[65]</a> .It implements LuaJIT
programming language and implements C/CUDA. It implements
N-dimensional array. It does routines of indexing, slicing,
transposing etc. It has in interface to C language via scripting
language LuaJIT. It supports different artificial intelligence
models like neural network and energy based models. It is
compatible with GPU.  The core package of is torch. It provides
a flexible N dimensional array which supports basic routings. It
has been used to build hardware implementation for data flows like
those found in neural networks.</p>
</li>
<li><p class="first">Theano
Theano is a Python library. It was written at the&nbsp;LISA&nbsp;lab.
Initially it was created with the purpose to support efficient
development of machine learning(ML) algorithms.
Theano uses recent GPUs for higher speed.
It is used to evaluate mathematical expressions and especially
those mathematical expressions that include multi-dimensional arrays.
Theanos working is dependent on combining aspects of a computer algebra
system and an optimizing compiler.
This combination of computer algebra system with optimized compilation
is highly beneficial for the tasks which involves complicated
mathematical expressions and that need to be evaluated repeatedly as
evaluation speed is highly critical in such cases.
It can also be used to generate customized C code for number of
mathematical operations.
For cases where many different expressions are there and each of them
is evaluated just once, Theano can minimize the amount of compilation
and analyses overhead <a class="reference internal" href="#www-theano" id="id75">[66]</a>.</p>
</li>
<li><p class="first">DL4j</p>
<p>DL4j stands for Deeplearning4j. <a class="reference internal" href="#www-dl4j" id="id76">[67]</a> It is a deep
learning programming library written for Java and the Java virtual
machine (JVM) and a computing framework with wide support for deep
learning algorithms. Deeplearning4j includes implementations of
the restricted Boltzmann machine, deep belief net, deep
autoencoder, stacked denoising autoencoder and recursive neural
tensor network, word2vec, doc2vec, and GloVe. These algorithms all
include distributed parallel versions that integrate with Apache
Hadoop and Spark. It is a open-source software released under
Apache License 2.0.</p>
<p>Training with Deeplearning4j occurs in a cluster. Neural nets are
trained in parallel via iterative reduce, which works on
Hadoop-YARN and on Spark. Deeplearning4j also integrates with CUDA
kernels to conduct pure GPU operations, and works with distributed
GPUs.</p>
</li>
<li><p class="first">H2O</p>
<p>It is an open source software for big data analysis. It was launched
by the Start-up H2O in 2011. [www-H2O-website] It provides an
in-memory, distributed, fast and a scalable machine learning and
predictive analytics platform that allows the users to build
machine learning models on big data. It is written in
Java. [www-H20-book] It is currently implemented in 5000
companies. It provides APIs for R(3.0.0 or later), Python(2.7.x,
3.5.x), Scala(1.4-1.6) and JSON. The software also allows online
scoring and modeling on a single platform.  It is scalable and has a
wide range of OS and language support. It works perfectly on the
conventional operating systems, and big data systems such as Hadoop,
Cloudera, MapReduce, HortonWorks. <a class="reference internal" href="#www-h20-wiki" id="id79">[68]</a> It can be used
on cloud computing environments such as Amazon and Microsoft Azure.</p>
</li>
<li><p class="first">IBM Watson</p>
<p>IBM Watson <a class="reference internal" href="#www-ibmwatson-wiki" id="id80">[69]</a> is a super computer built on
cognitive technology that processes information like the way human
brain does by understanding the data in a natural language as well
as analyzing structured and unstructured data. It was initially
developed as a question and answer tool more specifically to
answer questions on the quiz show &#8220;Jeopardy&#8221; but now it has been
seen as helping doctors and nurses in the treatment of cancer. It
was developed by IBM&#8217;s DeepQA research team led by David
Ferrucci. <a class="reference internal" href="#www-ibmwatson" id="id81">[70]</a> illustrates that with Watson you
can create bots that can engage in conversation with you. You can
even provide personalized recommendations to Watson by
understanding a user&#8217;s personality, tone and emotion. Watson uses
the Apache Hadoop framework in order to process the large volume
of data needed to generate an answer by creating in-memory
datasets used at run-time. Watson&#8217;s DeepQA UIMA (Unstructured
Information Management Architecture) annotators were deployed as
mappers in the Hadoop Map-Reduce framework. Watson is written in
multiple programming languages like Java, C++, Prolog and it runs
on the SUSE Linux Enterprise Server. <a class="reference internal" href="#www-ibmwatson" id="id82">[70]</a>
mentions that today Watson is available as a set of open source
APIs and Software As a Service product as well.</p>
</li>
<li><p class="first">Oracle PGX</p>
<p>Numerous information is revealed from graphs. Information like
direct and indirect relations or patterns in the elements of the
data, can be easily seen through graphs. The analysis of graphs
can unveil significant insights. Oracle PGX (Parallel Graph
AnalytiX) is a toolkit for graph analysis.  It is a fast,
parallel, in-memory graph analytic framework that allows users to
load up their graph data, run analytic algorithms on them, and to
browse or store the result <a class="reference internal" href="#www-pgx" id="id83">[71]</a>. Graphs can be loaded
from various sources like SQL and NoSQL databases, Apache Spark
and Hadoop <a class="reference internal" href="#www-ora" id="id84">[72]</a>.</p>
</li>
<li><p class="first">GraphLab</p>
<p>GraphLab <a class="reference internal" href="#www-graphlab" id="id85">[73]</a> is a graph-based, distributed computation,
high performance framework for machine learning written in C++. It
is an open source project started by Prof. Carlos Guestrin of
Carnegie Mellon University in 2009, designed considering the
scale, variety and complexity of real world data. It integrates
various high level algorithms such as Stochastic Gradient Descent,
Gradient Descent &amp; Locking and provides high performance
experience. It includes scalable machine learning toolkits which
has implementation for deep learning, factor machines, topic
modeling, clustering, nearest neighbors and almost everything
required to enhance machine learning models. This framework is
targeted for sparse iterative graph algorithms. It helps data
scientists and developers easily create and install applications
at large scale.</p>
</li>
<li><p class="first">GraphX</p>
<p>GraphX is Apache Spark&#8217;s API for graph and graph-parallel computation.
<a class="reference internal" href="#www-graphx" id="id86">[74]</a></p>
<p>GraphX provides:</p>
<p>Flexibility: It seamlessly works with both graphs and collections. GraphX
unifies ETL, exploratory analysis, and iterative graph computation within a
single system. You can view the same data as both graphs and collections,
transform and join graphs with RDDs efficiently, and write custom iterative
graph algorithms using the Pregel API.</p>
<p>Speed: Its performance is comparable to the fastest specialized graph
processing systems while retaining Apache Spark&#8217;s flexibility, fault
tolerance, and ease of use.</p>
<p>Algorithms: GraphX comes with a variety of algorithms such as PageRank,
Connected Components, Label propagations, SVD++, Strongly connected
components and Triangle Count.</p>
<p>It combines the advantages of both data-parallel and graph-parallel systems
by efficiently expressing graph computataion within the Spark data-parallel
framework. <a class="reference internal" href="#www-graphx1" id="id87">[75]</a></p>
<p>It gets developed as a part of Apache Spark project. It thus gets tested and
updated with each Spark release.</p>
</li>
<li><p class="first">IBM System G</p>
<p><a class="reference internal" href="#ibmsystemgdocumentation-1" id="id88">[76]</a> IBM System G provides a set of
Cloud and Graph computing tools and solutions for Big Data.  In fact,
the G stands for Graph and typically spans a database, visualization,
analytics library, middleware and Network Science Analytics tools.
<a class="reference internal" href="#ibmsystemgdocumentation-2" id="id89">[77]</a> It assists the easy creating of
graph stores and queries and exploring them via interactive
visualizations.  Internally, it uses the property graph model for its
working.  It consists of five individual components - gShell, REST
API, Python interface to gShell, Gremlin and a Visualizer.
<a class="reference internal" href="#ibmsystemgpaper" id="id90">[78]</a> Some of the typical applications wherein it
can be used include Expertise Location, Commerce, Recommendation,
Watson, Cybersecurity, etc.</p>
<p>However, it is to be noted that the current version does not work in a
distributed environment and it is planned that future versions would
support it.</p>
</li>
<li><p class="first">GraphBuilder(Intel)</p>
<p>Intel GraphBuilder for Apache Hadoop V2 is a software that is used
to build graph data models easily enabiling data scientists to
concentrate more on the business solution rather than
preparing/formatting the data. The software automates a)Data
cleaning, b)transforming data and c)creating graph models with
high throughput parallel processing using hadoop, with the help of
prebuilt libraries. Intel Graph Builder helps to speed up the time
to insight for data scientists by automating heavy custom
workflows and also by removing the complexities of cluster
computing for constructing graphs from Big Data. Intel Graph
Building uses Apache Pig scripting language to simplify data
preparation pipeline.  &#8220;Intel Graph Builder also includes a
connector that parallelizes the loading of the graph output into
the Aurelius Titan open source graph databasewhich further speeds
the graph processing pipeline through the final stage&#8221;.  Finally
being an open source there is a possibility of adding a load of
functionalities by various contributors.:cite:<cite>graphbuilder</cite></p>
</li>
<li><p class="first">TinkerPop</p>
<p>ThinkerPop is a graph computing framework from Apache software
foundation. :cite :<cite>www-ApacheTinkerPop</cite> Before coming under the
Apache project, ThinkerPop was a stack of technologies like
Blueprint, Pipes, Frames, Rexters, Furnace and Gremlin where each
part was supporting graph-based application development. Now all
parts are come under single TinkerPop project
repo. <a class="reference internal" href="#www-news" id="id91">[79]</a> It uses Gremlin, a graph traversal machine
and language. It allows user to write complex queries (traversal),
that can use for real-time transactional (OLTP) queries, graph
analytic system (OLAP) or combination of both as in
hybrid. Gremlin is written in
java. <a class="reference internal" href="#www-apachetinkerpophome" id="id92">[80]</a> TinkerPop has an ability to
create a graph in any size or complexity. Gremlin engine allows
user to write graph traversal in Gremlin language, Python,
JavaScript, Scala, Go, SQL and SPARQL. It is capable to adhere
with small graph which requires a single machine or massive graphs
that can only be possible with large cluster of machines, without
changing the code.</p>
</li>
<li><p class="first">Parasol</p>
<p>The parasol laboratory is a multidisciplinary research program
founded at Texas A&amp;M University with a focus on next generation
computing languages.  The core focus is centered around algorithm
and application development to find solutions to data concentrated
problems. <a class="reference internal" href="#www-parasol" id="id93">[81]</a> The developed applications are being
applied in the following areas: computational biology, geophysics,
neuroscience, physics, robotics, virtual reality and computer aided
drug design(CAD).  The program has organized a number of workshops
and conferences in the areas such as software, intelligent systems,
and parallel architecture.</p>
</li>
<li><p class="first">Dream:Lab</p>
<p>DREAM:Lab stands for Distributed Research on Emerging
Applications and Machines Lab. <a class="reference internal" href="#dream" id="id94">[82]</a> DREAM:Lab is centered
around distributed systems research to enable expeditious
utilization of distributed data and computing systems. <a class="reference internal" href="#dream" id="id95">[82]</a>
DREAM:Lab utilizes the capabilities of hundereds of personal
computers to allow access to supercomputing resources to average
individuals. <a class="reference internal" href="#rao" id="id96">[83]</a> The DREAM:Lab pursues this goal by utilizing
distributed computing. <a class="reference internal" href="#rao" id="id97">[83]</a> Distributed computing consists of
independent computing resources that communicate with each other
over a network. <a class="reference internal" href="#denero" id="id98">[84]</a> A large, complex computing problem is
broken down into smaller, more manageable tasks and then these
tasks are distributed to the various components of the distributed
computing system. <a class="reference internal" href="#denero" id="id99">[84]</a></p>
</li>
<li><p class="first">Google Fusion Tables</p>
<p>Fusion Tables is a cloud based services, provided by Google for
data management and integration. Fusion Tables allow users to
upload the data in tabular format using data files like
spreadsheet, CSV, KML, .tsv up to
250MB. <a class="reference internal" href="#www-fusiontablesupport" id="id100">[85]</a> It used for data management,
visualizing data (e.g. pie-charts, bar-charts, lineplot,
scatterplot, timelines) <a class="reference internal" href="#wiki-fusiontable" id="id101">[86]</a> , sharing of
tables, filter and aggregation the data. It allows user to take
the data privately, within controlled collaborative group or in
public. It allows to integrate the data from different tables from
different users or tables.Fusion Table uses two-layer storage,
Bigtable and Magastore. The information rows are stored in bigdata
table called Rows, user can merge the multiple table in to one,
from multiple users. Megastore is a library on top of
bigtable. <a class="reference internal" href="#googlefusiontable2012" id="id102">[87]</a> Data visualization is one
the feature, where user can see the visual representation of their
data as soon as they upload it. User can store the data along with
geospatial information as well.</p>
</li>
<li><p class="first">CINET</p>
<p>A representation of connected entities such as physical, biological and social
phenomena[www-bi.vt.edu] predictive model. Network science has grown its
importance understanding these phenomena Cyberinfrastructure is middleware tool
helps study Network science, [www-portal.futuresystems.org/projects/233]
by providing unparalleled computational and analytic environment for researcher.</p>
<p>Network science involves study of graph a large volume which requires high power
computing which usually cant be achieve by desktop. Cyberinfrastructure provides
cloud based infrastructure (e.g. FutureGrid) as well as use of HPC (e.g. Shadowfax,
Pecos). With use of advance intelligent Job mangers, it select the infrastructure
smartly suitable for submitted job.</p>
<p>It provides structural and dynamic network analysis, has number of algorithms for
network analysis such as shortest path, sub path, motif counting, centrality and
graph traversal. CiNet has number of range of network visualization modules.
CiNet is actively being used by several universities, researchers and analysist.</p>
</li>
<li><p class="first">NWB</p>
<p><a class="reference internal" href="#www-nwb-edu" id="id105">[88]</a> NWB stands for Network workbench is analysis,
modelling and visualization toolkit for the network scientists.
It provides an environment which help scientist researchers and
practitioner to get online access to the shared resource
environment and network datasets for analysis, modelling and
visualization of large scale networking application.  User can
access this network datasets and algorithms previously obtained by
doing lot of research and can also add their own datasets helps in
speeding up the process and saving the time for redoing the same
analysis.</p>
<p>NWB provides advanced tools for users to understand and interact
with different types of networks.  NWB members are largely the
computer scientist, biologist, engineers, social and behavioural
scientist. The platform helps the specialist researchers to
transfer the knowledge within the broader scientific and research
communities.</p>
</li>
<li><p class="first">Elasticsearch</p>
<p>Elasticsearch <a class="reference internal" href="#www-elasticsearch" id="id106">[89]</a> is a real time
distributed, RESTful search and analytics engine which is capable
of performing full text search operations for you. It is not just
limited to full text search operations but it also allows you to
analyze your data, perform CRUD operations on data, do basic text
analysis including tokenization and
filtering. <a class="reference internal" href="#www-elasticsearch-intro" id="id107">[90]</a> For example while
developing an E-commerce website, Elasticsearch can be used to
store the entire product catalog and inventory and can be used to
provide search and autocomplete suggestions for the
products. Elasticsearch is developed in Java and is an open source
search engine which uses standard RESTful APIs and JSON on
top of Apache&#8217;s Lucene - which is a full text search engine
library. Clinton Gormley &amp; Zachary Tong <a class="reference internal" href="#elasticsearch-book" id="id108">[91]</a>
describes elastic search as &#8220;A distributed real time document
store where every field is indexed and searchable&#8221;. They also
mention that &#8220;Elastic search is capable of scaling to hundreds of
servers and petabytes of structured and unstructured
data&#8221;. <a class="reference internal" href="#www-elasticsearch-hadoop" id="id109">[92]</a> mentions that Elastic
search can be used on big data by using the Elasticsearch-Hadoop
(ES-Hadoop) connector. ES-Hadoop connector lets you index the
Hadoop data into the Elastic Stack to take full advantage of the
Elasticsearch engine and returns output through Kibana
visualizations. <a class="reference internal" href="#www-wikipedia-elasticsearch" id="id110">[93]</a> A log parsing
engine &#8220;Logstash&#8221; and analytics and visualization platform
&#8220;Kibana&#8221; are also developed alongside Elasticsearch forming a
single package.</p>
</li>
<li><p class="first">Kibana</p>
</li>
<li><p class="first">Logstash</p>
<p>Logstash is an open source data collection engine with real-time
pipelining capabilities. Logstash can dynamically unify data from
disparate sources and normalize the data into destinations of your
choice. <a class="reference internal" href="#www-logstash" id="id111">[94]</a> Cleanse and democratize all your data
for diverse advanced downstream analytics and visualization use
cases.</p>
<p>While Logstash originally drove innovation in log collection, its
capabilities extend well beyond that use case. Any type of event
can be enriched and transformed with a broad array of input,
filter, and output plugins, with many native codecs further
simplifying the ingestion process. Logstash accelerates your
insights by harnessing a greater volume and variety of data.</p>
</li>
<li><p class="first">Graylog</p>
</li>
<li><p class="first">Splunk</p>
</li>
<li><p class="first">Tableau</p>
<p><a class="reference internal" href="#www-tableau-tutorial" id="id112">[95]</a> Tableau is a family of interactive data visualization products
focused on business intelligence. The different products which
tableau has built are: Tableau Desktop, for individual use;
Tableau Server for collaboration in an organization; Tableau
Online, for Business Intelligence in the Cloud; Tableau Reader,
for reading files saved in Tableau Desktop; Tableau Public, for
journalists or anyone to publish interactive data online.
<a class="reference internal" href="#www-tableau-web" id="id113">[96]</a> Tableau uses VizQL as a  visual query language for translating
drag-and-drop actions into data queries and later expressing the
data visually. Tableau also benefits from an Advanced In-Memory
Technology for handling large amounts of data.
The strengths of Tableau are mainly the ease of use and speed.
However, it has a number of limitations, which the most prominent
are unfitness for broad business and technical user, being
closed-source, no predictive analytical capabilities and no support
for expanded analytics.</p>
</li>
<li><p class="first">D3.js</p>
<p>D3.js is a JavaScript library responsible for manipulating
documents based on data. D3 helps in making data more interactive
using HTML, SVG, and CSS. D3s emphasis on web standards makes it
framework independent utilizing the full capabilities of modern
browsers, combining powerful visualization components and a
data-driven approach to DOM manipulation <a class="reference internal" href="#www-d3" id="id114">[97]</a>.</p>
<p>It assists in binding random data to a Document Object Model
(DOM), followed by applying data-driven transformations to the
document. It is very fast, supports large datasets and dynamic
behaviours involving interaction and animation.</p>
</li>
<li><p class="first">three.js</p>
</li>
<li><p class="first">Potree</p>
<p>Potree <a class="reference internal" href="#www-potree" id="id115">[98]</a> is a opensource tool powered by WebGL
based viewer to visualize data from large point clouds. It started
at the TU Wien, institute of Computer Graphics and Algorithms and
currently begin continued under the Harvest4D project. Potree
relies on reorganizing the point cloud data into an
multi-resolution octree data structure which is time consuming. It
efficiency can be improved by using techiques such as divide and
conquer as disscused in a conference paper Taming the beast: Free
and Open Source massive cloud point cloud web
visualization <a class="reference internal" href="#potree-paper-1" id="id116">[99]</a>. It has also been widely used
in works involving spatio-temporal data where the changes in
geographical features are across time <a class="reference internal" href="#potree-paper-2" id="id117">[100]</a>.</p>
</li>
<li><p class="first">DC.js</p>
<p>According to <a class="reference internal" href="#www-dcjs" id="id118">[101]</a>: DC.js&nbsp;is a javascript charting
library with native&nbsp;crossfilter&nbsp;support, allowing exploration on
large multi-dimensional datasets. It uses d3&nbsp;to render charts in
CSS-friendly SVG format. Charts rendered using dc.js are data
driven and reactive and therefore provide instant feedback to user
interaction. DC.js library can be used to perform data anlysis
on both mobile devices and different browsers. Under the dc
namespace the following chart classes are included: barChart,
boxplot, bubbleChart, bubbleOverlay, compositeChart, dataCount,
dataGrid, dataTable, geoChoroplethChart, heatMap,
legend,lineChart, numberDisplay, pieChart, rowChart, scatterPlot,
selectMenu and seriesChart.</p>
</li>
<li><p class="first">TensorFlow</p>
<p>TensorFlow is a platform that provides a software library for
expressing and executing machine learning
algorithms. <a class="reference internal" href="#tensorflow-paper-2016" id="id119">[102]</a> states TensorFlow has a
flexible architecture allowing it to be executed with minimal
change to many hetegeneous systems such as CPUs and GPUs of mobile
devices, desktop machines, and servers. TensorFlow can &#8220;express a
wide variety of algorithms, including training and inference
algorithms for deep neural netowrk models, and it has been used
for conducting research and for deploying machine learning systems
into production across more than a dozen
areas&#8221;. <a class="reference internal" href="#www-tensorflow" id="id120">[103]</a> describes that TensorFlow utilizes
data flow graphs in which the &#8220;nodes in the graph represent
mathematical operations, while the graph edges represent the
multidimensional data arrays (tensors) communicated between them.&#8221;
TensorFlow was developed by the Google Brain Team and has a
reference implementation that was released on 2015-11-09 under the
Apache 2.0 open source license.</p>
</li>
<li><p class="first">CNTK</p>
<p>The Microsoft Cognitive Toolkit - CNTK - is a unified deep-learning toolkit
by Microsoft Research. It is in essence an implementation of Computational
Network(CN) which supports both CPU and GPU. CNTK supports arbitrary valid
computational networks and makes building DNNs, CNNs, RNNs, LSTMS, and other
complicated networks as simple as describing the operations of the networks.
The toolkit is implemented with efficiency in mind. It removes duplicate
computations in both forward and backward passes, uses minimal memory needed
and reduces memory reallocation by reusing them. It also speeds up the model
training and evaluation by doing batch computation whenever possible
<a class="reference internal" href="#book-cntk" id="id121">[104]</a> . It can be included as a library in your Python or C++ pro
grams, or used as a standalone machine learning tool through its own model
description language (BrainScript). <a class="reference internal" href="#www-cntk" id="id122">[105]</a>
Latest Version:2017-02-10. V 2.0 Beta 11 Release</p>
</li>
</ol>
</div>
<div class="section" id="application-hosting-frameworks">
<h2>Application Hosting Frameworks<a class="headerlink" href="#application-hosting-frameworks" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="80">
<li><p class="first">Google App Engine  <a class="reference internal" href="#www-gae" id="id123">[106]</a></p>
<p>On purpose we put in here a &#8220;good&#8221; example of a bad entry that woudl
receive 10 out of 100 points, e.g. an F:</p>
<p>&#8220;Google App Engine&#8221; provides platform as a service.
There are major advantages from this framework:</p>
<ol class="arabic simple">
<li>Scalable Applications</li>
<li>Easier to maintain</li>
<li>Publishing services easily</li>
</ol>
<p>Reasons: (a) &#8220;major advantages is advertisement&#8221; if you add word
major (b) grammar needs to be improved (c) the three points do not
realy say anything about Google App Engine (d) the reader will
after reading this have not much information about what it is (e)
a refernce is not included. (f) enumeration should be in this page
avoided. We like to see a number of paragraphs with text.</p>
<p><strong>Note: This is an example for a bad entry</strong></p>
</li>
<li><p class="first">AppScale</p>
<p>AppScale is an application hosting platform. This platform helps
to deploy and scale the unmodified Google App Engine application,
which run the application on any cloud infrastructure in public,
private and on premise cluster. <a class="reference internal" href="#www-appscale" id="id124">[107]</a> AppScale
provide rapid, API development platform that can run on any cloud
infrastructure. The platform separates the app logic and its
service part to have control over application deployment, data
storage, resource use, backup and migration.  AppScale is based on
Googles App Engine APIs and has support for Python, Go, PHP and
Java applications. It supports single and multimode deployment,
which will help with large, dataset or CPU. AppScale allows to
deploy app in thee main mode i.e. dev/test, production and
customize deployment.  [www-apscale-deployment]</p>
</li>
<li><p class="first">Red Hat OpenShift</p>
<p>[www-paas] OpenShift was launched as a PaaS (Platform as a
Service) by Red Hat in the Red Hat Summit, 2011.
<a class="reference internal" href="#www-developers-openshift" id="id127">[108]</a> It is a cloud application
development and hosting platform that envisages shifting of the
developer&#8217;s focus to development by automating the management and
scaling of applications.  Thus, <a class="reference internal" href="#www-openshift" id="id128">[109]</a> OpenShift
enables us to write our applications in any one web development
language (using any framework) and it itself takes up the task of
running the application on the web.  This has its advantages and
disadvantages - advantage being the developer doesn&#8217;t have to
worry about how the stuff works internally (as it is abstracted
away) and the disadvantage being that he cannot control how it
works, again because it is abstracted.</p>
<p>[openshift-blog] OpenShift is powered by Origin, which is in
turn built using Docker container packaging and Kubernetes container
cluster.  Due to this, OpenShift offers a lot of options, including
online, on-premise and open source project options.</p>
</li>
<li><p class="first">Heroku</p>
<p>Heroku <a class="reference internal" href="#www-heroku" id="id130">[110]</a> is a platform as a service that is used
for building, delivering monitoring and scaling applications. It
lets you  develop and deploy application quickly without thinking
about irrelevant problems such as infrastructure. Heroku also
provides a secure and scalable database as a service with number of
developers tools like database followers, forking, data clips and
automated health checks. It works by deploying to cedar stack
<a class="reference internal" href="#www-cedar" id="id131">[111]</a>, an online runtime environment that supports apps
buit in Java, Node.js, Scala, Clojure, Python and PHP. It uses Git
for version controlling. It is also tightly intergrated with
Salesforce, providing seamless and smooth Heroku and Salesforce
data synchronization enabling companies to develop and design creative
apps that uses both platforms.</p>
</li>
<li><p class="first">Aerobatic</p>
<p>According to <a class="reference internal" href="#www-aero" id="id132">[112]</a>: Aerobatic is a platform that allows
hosting static websites. It used to be an ad-on for Bitbucket but
now Aerobatic is transitioning to standalone CLI(command Line
Tool) and web dashboard . Aerobatic allows automatic builds to
different branches. New changes to websites can be deployed using
aero deploy command which can be executed from local desktop or
any of CD tools and services like Jenkins, Codeship,Travis and so
on.  It also allows users to configure custom error pages and
offers authentication which can also be customized. Aerobatic is
backed by AWS cloud. Aerobatic has free plan and pro plan options
for customers.</p>
</li>
<li><p class="first">AWS Elastic Beanstalk</p>
</li>
<li><p class="first">Azure</p>
<p>Microsoft Corporation (MSFT) markets its cloud products under the
<em>Azure</em> brand name. At its most basic, Azure acts as an
<em>infrastructure- as-a-service</em> (IaaS) provider.  IaaS virtualizes
hardware components, a key differentiation from other
<em>-as-a-service</em> products. IaaS &#8220;abstract[s] the user from the
details of infrasctructure like physical computing resources,
location, data partitioning, scaling, security, backup, etc.&#8221;
<a class="reference internal" href="#www-wikipedia-cloud" id="id133">[113]</a></p>
<p>However, Azure offers a host of closely-related tool and products
to enhance and improve the core product, such as raw block
storage, load balancers, and IP addresses
<a class="reference internal" href="#www-azure-msft" id="id134">[114]</a>. For instance, Azure users can access
predictive analytics, Bots and Blockchain-as-a-Service
<a class="reference internal" href="#www-azure-msft" id="id135">[114]</a> as well as more-basic computing,
networking, storage, database and management components
<a class="reference internal" href="#www-sec-edgar-msft" id="id136">[115]</a>.  The Azure website shows twelve major
categories under <em>Products</em> and twenty <em>Solution</em> categories,
e.g., e-commerce or Business SaaS apps.</p>
<p>Azure competes against Amazon&#8217;s <em>Amazon Web Service</em>,
<a class="reference internal" href="#www-aws-amzn" id="id137">[116]</a> even though IBM (<em>SoftLayer</em>
<a class="reference internal" href="#www-softlayer-ibm" id="id138">[117]</a> and <em>Bluemix</em> <a class="reference internal" href="#www-bluemix-ibm" id="id139">[118]</a>)
and Google (<em>Google Cloud Platform</em>) <a class="reference internal" href="#www-cloud-google" id="id140">[119]</a>
offer IaaS to the market.  As of January 2017, Azure&#8217;s datacenters
span 32 Microsoft-defined <em>regions</em>, or 38 <em>declared regions</em>,
throughout the world. <a class="reference internal" href="#www-azure-msft" id="id141">[114]</a></p>
</li>
<li><p class="first">Cloud Foundry</p>
<p>It is an open source software with multi cloud application .It is
a platform for running applications and
services. <a class="reference internal" href="#www-cloudfoundry-book" id="id142">[120]</a> It was originally
developed by VMware and currently owned by Pivotal . It is written
in Ruby and Go .It has a commercial version called Pivotal Cloud
Foundry (PFC). Cloud Foundry is available as a stand alone
software package, we can also deploy it to Amazon AWS as well as
host it on OpenStack server , HPs Helion or VMwares vSphere as
given in the blog <a class="reference internal" href="#www-cloudfoundry-blog" id="id143">[121]</a> , it delivers
quick application from development to deployment and is highly
scalable. It has a DevOps friendly workflow.  Cloud Foundry
changes the way application and services are deployed and reduces
the develop to deployment cycle time.</p>
</li>
<li><p class="first">Pivotal</p>
<p>Pivotal Software, Inc. (Pivotal) is a software and services
company. It offeres multiple consulting and technology services,
which includes Pivotal Web Services, which is an agile application
hosting service. It has a single step upload feature &#8220;cf push&#8221;,
another feature called Buildpacks lets us push applications
written for any language like Java, Grails, Play, Spring, Node.js,
Ruby on Rails, Sinatra or Go. Pivotal Web Services also allows
developers to connect to 3rd party databases, email services,
monitoring and more from the Marketplace. It also offers
performance monitoring, active health monitoring, unified log
streaming, web console built for team-based agile development
<a class="reference internal" href="#pivotal-www" id="id144">[122]</a>.</p>
</li>
<li><p class="first">IBM BlueMix</p>
</li>
<li><p class="first">(Ninefold)</p>
<p>The Australian based cloud computing platform has shut down their
services since January 30, 2016. Refer <a class="reference internal" href="#www-ninefoldsite" id="id145">[123]</a></p>
</li>
<li><p class="first">Jelastic</p>
<p>Jelastic (acronym for Java Elastic) is an unlimited PaaS and Container based
IaaS within a single platform that provides high availability of
applications, automatic vertical and horizontal scaling via containerization
to software development clients, enterprise businesses, DevOps, System
Admins, Developers, OEMs and web hosting providers. <a class="reference internal" href="#www-jelastic-2" id="id146">[124]</a>
Jelastic is a Platform-as-Infrastructure provider of Java and PHP hosting.
It has international hosting partners and data centers. The company can add
memory, CPU and disk space to meet customer needs. The main competitors of
Jelastic are Google App Engine, Amazon Elastic Beanstalk, Heroku, and Cloud
Foundry.Jelastic is unique in that it does not have limitations or code
change requirements, and it offers automated vertical scaling, application
lifecycle management, and availability from multiple hosting providers
around the world. <a class="reference internal" href="#www-jelastic-1" id="id147">[125]</a></p>
</li>
<li><p class="first">Stackato</p>
<p>Hewlett Packard Enterprise or HPE Helion Stackato is a platform as a
service(PaaS) cloud computing solution.  The platform facilitates
deployment of the users application in the cloud and will function
on top of an Infrastructure as a service(IaaS). <a class="reference internal" href="#www-hpe" id="id148">[126]</a> Multiple
cloud development is supported across AWS, vSphere, and Helion Openstack.
The platform supports the following programming languages: native
.NET support, java, Node.js, python, and ruby.  This flexibility is
advantageous compared to early PaaS solutions which would force the
customer into utilizing a single stack.  Additionally, this solution
has the capacity to support private, public and hybrid clouds.
<a class="reference internal" href="#www-virt" id="id149">[127]</a> This capability user has to not have to make choices
of flexibility over security of sensitive data when choosing a
cloud computing platform.</p>
</li>
<li><p class="first">appfog</p>
<p>According to <a class="reference internal" href="#wee" id="id150">[128]</a>, AppFog is a platform as a service (PaaS)
provider. Platform as a service provides a platform for the
development of web applications without the necessity of
purchasing the software and infrastructure that supports
it. <a class="reference internal" href="#kepes" id="id151">[129]</a> PaaS provides an environment for the creation of
software. <a class="reference internal" href="#kepes" id="id152">[129]</a> The underlying support infrastructure that AppFog
provides includes things such as runtime, middleware, o/s,
virtualization, servers, storage, and networking. <a class="reference internal" href="#appfog" id="id153">[130]</a> AppFog
is based on VMWares CloudFoundry project. <a class="reference internal" href="#wee" id="id154">[128]</a> It gets things
such as MySQL, Mongo, Reddis, memCache, etc. running and then
manages them. <a class="reference internal" href="#tweney" id="id155">[131]</a></p>
</li>
<li><p class="first">CloudBees</p>
<p><a class="reference internal" href="#www-cloudbees-wiki" id="id156">[132]</a> Cloudbees provides Platform as a
Service (PaaS) solution, which is a cloud service for Java
applications. It is used to build, run and manage the web
applications. It was created in 2010 by Jenkins. It has a
continuous delivery platform for DevOps, and adds a
enterprise-grade functionality with an expert level
support. Cloudbees is better than the traditional Java platform as
it requires no provision of the nodes, clusters, load balancers
and databases. In cloudbees the environment is constantly managed
and monitored where a metering and scale updating is done on a
real time basis. <a class="reference internal" href="#www-cloudbees-webpage" id="id157">[133]</a> The platform ships
with verified security and enhancements assuring less risk for
sharing sensitive information. It simplies the task of getting the
platform accessed by every user using the feature Jenkins
Sprawl.</p>
</li>
<li><p class="first">Engine Yard</p>
</li>
<li><p class="first">(CloudControl)</p>
<p>No Longer active as of Feb. 2016 <a class="reference internal" href="#www-wiki" id="id158">[134]</a></p>
</li>
<li><p class="first">dotCloud <a class="reference internal" href="#www-dotcloud" id="id159">[135]</a></p>
<p>dotCloud services were shutdown on February 29,2016.</p>
</li>
<li><p class="first">Dokku</p>
</li>
<li><p class="first">OSGi</p>
</li>
<li><p class="first">HUBzero</p>
<p>HUBzero is a collaborative framework which allows creation of
dynamic websites for scientific research as well as educational
activities.  HUBzero lets scientific researchers work together
online to develop simulation and modeling tools.  These tools can
help you connect with powerful Grid computing resources as well
as rendering farms.:cite:<cite>hubzerowebsite</cite> Thus allowing other
researchers to access the resulting tools online using a normal
web browser and launch simulation runs on the Grid infrastructure
without having to download or compile any code. It is a unique
framework with simulation and social networking
capabilities.:cite:<cite>hubzeropaper2010</cite></p>
</li>
<li><p class="first">OODT</p>
<p>The Apache Object Oriented Data Technology (OODT) is an open source data
management system framework. OODT was originally developed at NASA Jet
Propulsion Laboratory to support capturing, processing and sharing of data
for NASA&#8217;s scientific archives. OODT focuses on two canonical use cases:
Big Data processing and on Information integration. It facilitates the
integration of highly distributed and heterogeneous data intensive systems
enabling the integration of different, distributed software systems,
metadata and data. OODT is written in the Java, and through its REST API
used in other languages including Python. [git-OOTD]</p>
</li>
<li><p class="first">Agave</p>
<p>Agave is an open source, application hosting framework and
provides a platform-as-a-service solution for hybrid
computing. <a class="reference internal" href="#agave-paper" id="id161">[136]</a> It provides everything ranging
from authentication and authorization to computational, data and
collaborative services. Agave manages end to end lifecycle of an
applications execution.  Agave provides an execution platform,
data management platform, or an application platform through
which users can execute applications, perform operations on their
data or simple build their web and mobile
applications. <a class="reference internal" href="#www-agaveapi-features" id="id162">[137]</a></p>
<p>Agaves APIs provide a catalog with existing technologies and
hence no additional appliances, servers or other software needs
to be installed. To deploy an application from the catalog, the
user needs to host it on a storage system registered with Agave,
and submit to agave, a JSON file that shall contain the path to
the executable file, the input parameters, and specify the
desired output location. <a class="reference internal" href="#agave-paper" id="id163">[136]</a> Agave shall read the
JSON file, formalize the parameters, execute the user program and
dump the output to the requested destination.</p>
</li>
<li><p class="first">Atmosphere</p>
<p>Atmosphere is developed by CyVerse (previously named as iPlant
Collaborative).
It is a cloud-computing platform. It allows one to launch his own
isolated virtual machine (VM) image <a class="reference internal" href="#www-at1" id="id164">[138]</a>.
It does not require any machine specification. It can be run on any device
(tablet/desktop/laptop) and any machine(Linux/Windows/Max/Unix).
User should have a CyVerse account and be granted permission to access to
Atmosphere before he can begin using Atmosphere. No subscription is needed.
Atmosphere is designed to execute data-intense bioinformatics tasks that
may include a)Infrastructure as a Service (IaaS) with advanced APIs;
b)Platform as a Service (PaaS), and c)Software as a Service (SaaS).
On Atmosphere one has several images of virtual machine and user can launch
any image or instance according to his requirements.
The images launched by users can be shared among different members as and
when required <a class="reference internal" href="#www-at2" id="id165">[139]</a>.</p>
</li>
</ol>
</div>
<div class="section" id="high-level-programming">
<h2>High level Programming<a class="headerlink" href="#high-level-programming" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="104">
<li><p class="first">Kite</p>
</li>
<li><p class="first">Hive</p>
</li>
<li><p class="first">HCatalog</p>
</li>
<li><p class="first">Tajo</p>
<p>Apache Tajo <a class="reference internal" href="#www-apache-tajo" id="id166">[140]</a> is a big data relational and
distributed data warehouse system for Apache&#8217;s Hadoop
framework. It uses the Hadoop Distributed File System (HDFS) as a
storage layer and has its own query execution engine instead of
the MapReduce framework. Tajo is designed to provide low-latency
and scalable ad-hoc queries, online aggregation, and ETL
(extraction-transformation-loading process) on large-data sets
which are stored on HDFS (Hadoop Distributed File System) and on
other data sources. <a class="reference internal" href="#www-tutorialspoint-tajo" id="id167">[141]</a> Apart from HDFS,
it also supports other storage formats as Amazon S3, Apache
HBase, Elasticsearch etc. It provides distributed SQL query
processing engine and even has query optimization techniques and
provides interactive anaysis on large-data sets. Tajo is
compatible with ANSI/ISO SQL standard, JDBC standard. Tajo can
also store data from various file formats such as CSV,
JSON,RCFile, SequenceFile, ORC and Parquet. It provides a SQL
shell which allows users to submit the SQL queries. It also
offers user defined functions to work with it which can be
created in python. A Tajo cluster has one master node and a
number of worker nodes. <a class="reference internal" href="#www-tutorialspoint-tajo" id="id168">[141]</a> The master
node is responsible for performing the query planning and
maintaining a coordination among the worker nodes. It does this
by dividing a query in small task which are assigned to the
workers who have a local query engine for executing the queries
assigned to them.</p>
</li>
<li><p class="first">Shark</p>
<p>Data Scientists when working on huge data sets try to extract
meaning and interpret the data to enhance insight about the
various patterns, oppurtunities and possiblities that the dataset
has to offer. :cite: &#8216;shark-paper-2012&#8217; At a traditional
EDW(Enterprrise Data Warehouse) a simple data manipulation can be
perfpormed using SQL queries but we have to rely on other systems
to apply the machine learning on thoese data.Apache Shark is a
distributed query engine developed by the open source community
whoese goal is to provide a a unified system for easy data
manipulation using SQL and pushing sophisticated analysis towards
the data.</p>
<p>:cite:&#8217;shark-paper-2012&#8217; Shark is a data Warehouse system built
on top of Apache Spark which does the parallel data execution and
is capable of deep data analysis using the Resilient Distributed
Datasets(RDD) memory abstraction which unifies the SQL query
processing engine with analytical algorithms based on this common
abstraction allowing the two to run in the same set of workers
and share intermediate data. Since RDDs are designed to scale
horizontally, it is easy to add or remove nodes to accommodate
more data or faster query processing thus it can be scaled to
thoushands o nodes in a fault-toleranat manner</p>
<p>:cite:&#8217;shark-paper-2012&#8217; &#8220;Shark is built on Hive Codebase and it
has the ability to execute HIVE QL queries up to 100 times faster
than Hive without making any change in the existing
queries&#8221;. Shark can run both on the StandAlone Mode and Cluster
Mode.:cite:&#8217;shark-paper-2012&#8217; Shark can answer the queries 40X
faster than Apache Hive and can machine learning programs 25X
faster than MapReduce programmes. in Apache hadoop on large data
sets.Thus, this new data analysis system performs query
processing and complex analytics(iterative Machine learning) at
scale and efficiently recovers form the failures midway</p>
</li>
<li><p class="first">Phoenix</p>
<p>In the first quarter of 2013, Salesforce.com released its
proprietary SQL-like interface and query engine for HBase,
<em>Phoenix</em>, to the open source community.  The company appears to
have been motivated to develop Phoenix as a way to 1) increase
accessiblity to HBase by using the industry-standard query
language (SQL); 2) save users time by abstracting away the
complexities of coding native HBase queries; and, 3) implementing
query best practices by implementing them automatically via
Phoenix. <a class="reference internal" href="#www-phoenix-cloudera" id="id169">[142]</a> Although Salesforce.com
initially <em>open-sourced</em> it via Github, by May of 2014 it had
become a top-level Apache project. [www-phoenix-wikipedia]</p>
<p>Phoenix, written in Java, &#8220;compiles [SQL queries] into a series
of HBase scans, and orchestrates the running of those scans to
produce regular JDBC result sets.&#8221; <a class="reference internal" href="#www-apachephoenix-org" id="id171">[143]</a>
In addition, the program directs compute intense portions of the
calls to the server.  For instance, if a user queried for the top
ten records across numerous regions from an HBase database
consisting of a billion records, the program would first select
the top ten records for each region using server-side compute
resources.  After that, the client would be tasked with selecting
the overall top ten. <a class="reference internal" href="#www-phoenix-salesforcedev" id="id172">[144]</a></p>
<p>Despite adding an abstraction layer, Phoenix can actually speed
up queries because it optimizes the query during the translation
process. <a class="reference internal" href="#www-phoenix-cloudera" id="id173">[142]</a> For example, &#8220;Phoenix
beats Hive for a simple query spanning 10M-100M rows.&#8221;
<a class="reference internal" href="#www-phoenix-infoq" id="id174">[145]</a></p>
<p>Finally, another program can enhance HBase&#8217;s accessibility for
those inclined towards graphical interfaces.  SQuirell only
requires the user to set up the JDBC driver and specify the
appropriate connection string. <a class="reference internal" href="#www-phoenix-bighadoop" id="id175">[146]</a></p>
</li>
<li><p class="first">Impala</p>
</li>
<li><p class="first">MRQL</p>
<p>MapReduce Query Language (MRQL, pronounced miracle) &#8220;is a query
processing and optimization system for large-scale, distributed
data analysis&#8221; <a class="reference internal" href="#www-apachemrql" id="id176">[147]</a>. MRQL provides a SQL
like language for use on Apache Hadoop, Hama, Spark, and Flink.
MRQL allows users to perform complex data analysis using only SQL
like queries, which are translated by MRQL to efficient Java
code. MRQL can evaluate queries in Map-Reduce (using Hadoop), Bulk
Synchronous Parallel (using Hama), Spark, and Flink modes
<a class="reference internal" href="#www-apachemrql" id="id177">[147]</a>.</p>
<p>MRQL was created in 2011 by Leaonids
Fegaras <a class="reference internal" href="#www-mrqlhadoop" id="id178">[148]</a> and is currently in the Apache
Incubator.  All projects accepted by the Apache Software
Foundation (ASF) undergo an incubation period until a review
indicates that the project meets the standards of other ASF
projects <a class="reference internal" href="#www-apacheincubator" id="id179">[149]</a>.</p>
</li>
<li><p class="first">SAP HANA</p>
<p>As noted in <a class="reference internal" href="#www-sap-hana" id="id180">[150]</a>, SAP HANA is in-memory massively
distributed platform that consists of three components:
analytics, relational ACID compliant database and
application. Predictive analytics and machine learning
capabilities are dynamically allocated for searching and
processing of spatial, graphical, and text data.
SAP HANA accommodates flexible development and deployment of
data on premises, cloud and hybrid configurations.  In a
nutshell, SAP HANA acts as a warehouse that integrates live
transactional data from various data sources on a single
platform <a class="reference internal" href="#olofson-2014" id="id181">[151]</a>. It provides extensive
administrative, security features and data access that ensures
high data availability, data protection and data quality.</p>
</li>
<li><p class="first">HadoopDB</p>
<p>HadoopDB is a hybrid of parallel database and MapReduce technologies. It
approaches parallel databases in performance and efficiency, yet still
yields the scalability, fault tolerance, and flexibility of MapReduce
systems. It is a free and open source parallel DBMS. The basic idea behind
it is to give Hadoop access to multiple single-node DBMS servers
(eg. PostgreSQL or MySQL) deployed across the cluster. It pushes as much
as possible data processing into the database engine by issuing SQL
queries which results in resembling a shared-nothing cluster of
machines. <a class="reference internal" href="#git-hadoopdb" id="id182">[152]</a></p>
<p>HadoopDB is more scalable than currently available parallel database
systems and DBMS/MapReduce hybrid systems. It has been demonstrated on
clusters with 100 nodes and should scale as long as Hadoop scales, while
achieving superior performance on structured data analysis workloads.</p>
</li>
<li><p class="first">PolyBase</p>
</li>
<li><p class="first">Pivotal HD/Hawq</p>
<p>Pivotal HDB is the Apache Hadoop native SQL database powered by
Apache HAWQ <a class="reference internal" href="#www-apache-hqwq" id="id183">[153]</a> for data science and machine
learning workloads. It can be used to gain deeper and actionable
insights into data with out the need from moving data to another
platform to perfrom advanced analytics. Few important problems
that Pivot HDB address are as follows Quickly unlock business
insights with exceptional performance, Integrate SQL BI tools
with confidence and Iterate advanced analytics and machine
learning in database support. Pivotal HDB comes with an elastic
SQL query engine which combines MPP-based analytical performance,
roboust ANSI SQL compliance and integrated Apache MADlib for
machine learning <a class="reference internal" href="#www-pivotalhdb" id="id184">[154]</a>.</p>
</li>
<li><p class="first">Presto</p>
<p>Presto <a class="reference internal" href="#www-presto" id="id185">[155]</a> is an open-source distributed SQL query
engine that supports interactive analytics on large datasets. It
allows interfacing with a variety of data sources such as Hive,
Cassandra, RDBMSs and proprietary data source. Presto is used at a
number of big-data companies such as Facebook, Airbnb and
Dropbox. Presto&#8217;s performance compares favorably to similar systems
such as Hive and Stinger <a class="reference internal" href="#presto-paper-2014" id="id186">[156]</a>.</p>
</li>
<li><p class="first">Google Dremel</p>
<p>Dremel is a scalable, interactive ad-hoc query system for analysis of
read-only nested data. By combining multi-level execution trees and
columnar data layout, Google Dremel is capable of running aggregation
queries over trillion-row tables in seconds. <a class="reference internal" href="#paper-dremel" id="id187">[157]</a>
With Dremel, you can write a declarative SQL-like query against data stored
in a read-only columnar format efficiently for analysis or data exploration.
It&#8217;s also possible to write queries that analyze billions of rows, terabytes of
data, and trillions of records in seconds. Dremel can be use for a variety
of jobs including analyzing web-crawled documents, detecting e-mail spam,
working through application crash reports.</p>
</li>
<li><p class="first">Google BigQuery</p>
</li>
<li><p class="first">Amazon Redshift</p>
<p>Amazon Redshift is a fully managed, petabyte-scale data werehouse service
in the cloud. Redshift service manages all of the workof setting up, operating
and scalling a data werehouse. AWS Redshift can perform these tasks including
provisioning capacity, monitoring and backing up the cluster, and applying
patches as well as upgrades to the Redshift&#8217;s engine <a class="reference internal" href="#www-redshift" id="id188">[158]</a>.
Redshift is built on thet top of technology from the Massive Paraller Processing
(MPP) data-werehouse company ParAccel which based on PostgresSQL 8.0.2
to PostgresSQL 9.x with capabilty to handle analytics workloads on large-
scale dataset stored by a column-oriented DBMS principle <a class="reference internal" href="#www-wiki-red" id="id189">[159]</a>.</p>
</li>
<li><p class="first">Drill</p>
<p>Apache Drill <a class="reference internal" href="#www-apachedrill" id="id190">[160]</a> is an open source framework
that provides schema free SQL query engine for distributed
large-scale datasets. Drill has an extensible architecture at
its different layers. It does not require any centralized
metadata and does not have any requirement for schema
specification. Drill is highly useful for short and interactive
ad-hoc queries on very large scale data sets. It is scalable to
several thousands of nodes. Drill is also capable to query
nested data in various formats like JSON and Parquet. It can
query large amount of data at very high speed. It is also
capable of performing discovery of dynamic schema.
A service called Drillbit  is at the core of Apache Drill
responsible for accepting requests from the client, processing
the required queries, and returning all the results to the client.
Drill is primarily focused on non-relational datastores,
including Hadoop and NoSQL</p>
</li>
<li><p class="first">Kyoto Cabinet</p>
<p>Kyoto Cabinet as specified in <a class="reference internal" href="#www-kyotocabinet" id="id191">[161]</a> is a
library of routines for managing a database which is a simple
data file containing records. Each record in the database is a
pair of a key and a value. Every key and value is serial bytes
with variable length. Both binary data and character string can
be used as a key and a value. Each key must be unique within a
database.  There is neither concept of data tables nor data
types. Records are organized in hash table or B+ tree. Kyoto
Cabinet runs very fast. The elapsed time to store one million
records is 0.9 seconds for hash database, and 1.1 seconds for B+
tree database. Moreover, the size of database is very small. The,
overhead for a record is 16 bytes for hash database, and 4 bytes
for B+ tree database. Furthermore, scalability of Kyoto Cabinet
is great. The database size can be up to 8EB (9.22e18 bytes).</p>
</li>
<li><p class="first">Pig</p>
</li>
<li><p class="first">Sawzall</p>
<p>Google engineers created the domain-specific programming language
(DSL) <em>Sawzall</em> as a productivity enhancement tool for Google
employees.  They targeted the analysis of large data sets with
flat, but regular, structures spread across numerous servers.
The authors designed it to handle &#8220;simple, easily distributed
computations: filtering, aggregation, extraction of statistics,&#8221;
etc. from the aforementioned data sets.
[google-sawzall]</p>
<p>In general terms, a Sawzall job works as follows: multiple
computers each create a Sawzall instance, perform some operation
on a single record out of (potentially) petabytes of data, return
the result to an aggregator function on a different computer and
then shut down the Sawzall instance.</p>
<p>The engineer&#8217;s focus on simplicity and parallelization led to
unconventional design choices.  For instance, in contrast to most
programming languages Sawzall operates on one data record at a
time; it does not even preserve state between records.
<a class="reference internal" href="#www-bytemining-sawzall" id="id193">[162]</a> Addtionally, the language provides
just a single primitive result function, the <em>emit</em> statement.
The emitter returns a value from the Sawzall program to a
designated virtual receptacle, generally some type of aggregator.
In another example of pursuing language simplicity and
parallelization, the aggregators remain separate from the formal
Sawzall language (they are written in C++) because &#8220;some of the
aggregation algorithms are sophisticated and best implemented in
a native language [and] [m]ore important[ly] drawing an explicit
line between filtering and aggregation enables a high degree of
parallelism, even though it hides the parallelism from the
language itself&#8221;.  [google-sawzall]</p>
<p>Important components of the Sawzall language include: <em>szl</em>, the
binary containing the code compiler and byte-code interpreter
that executes the program; the <em>libszl</em> library, which compiles
and executes Sawzall programs &#8220;[w]hen szl is used as part of
another program, e.g. in a [map-reduce] program&#8221;; the Sawzall
language plugin, designated <em>protoc_gen_szl</em>, which generates
Sawzall code when run in conjunction with Google&#8217;s own <em>protoc</em>
protocol compiler; and libraries for intrinsic functions as well
as Sawzall&#8217;s associated aggregation functionality.
<a class="reference internal" href="#www-google-code-wiki-sawzall" id="id195">[163]</a></p>
</li>
<li><p class="first">Google Cloud DataFlow</p>
<p>Google Cloud DataFlow <a class="reference internal" href="#data-flow1" id="id196">[164]</a> is a unified programming
model that manages the deployment, maintenance and optimization
of data processes such as batch processing, ETL etc. It creates a
pipeline of tasks and dynamically allocates resources thereby
maintaining high efficiency and low latency. According to
<a class="reference internal" href="#data-flow1" id="id197">[164]</a>, these capabilities make it suitable for
solving challenging big data problems. Also, google DataFlow
overcomes the performance issues faced by Hadoops Mapreduce while
building pipelines. As stated in <a class="reference internal" href="#dataconomy" id="id198">[165]</a> the
performance of MapReduce started deteriorating while facing
multiplepetabytes of data whereas Google Cloud Dataflow is
apparently better at handling enormous
datasets. <a class="reference internal" href="#data-flow1" id="id199">[164]</a> Additionally Google Dataflow can be
integrated with Cloud Storage, Cloud Pub/Sub, Cloud Datastore,
Cloud Bigtable, and BigQuery. The unified programming ability is
another noteworthy feature which uses Apache Beam SDKs to support
powerful operations like windowing and allows correctness control
to be applied to batch and stream  data processes.</p>
</li>
<li><p class="first">Summingbird</p>
</li>
<li><p class="first">Lumberyard</p>
</li>
</ol>
</div>
<div class="section" id="streams">
<h2>Streams<a class="headerlink" href="#streams" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="127">
<li><p class="first">Storm</p>
<p>Apache Storm is an open source distributed computing framework for
analyzing big data in real time. <a class="reference internal" href="#storm-paper-ijctt" id="id200">[166]</a> refers
storm as the Hadoop of real time data. Storm operates by reading real
time input data from one end and passes it through a sequence of
processing units delivering output at the other end. The basic element
of Storm is called topology. A topology consists of many other
elements interconnected in a sequential fashion. Storm allows us to
define and submit topologies written in any programming language.</p>
<p>Once under execution, a storm topology runs indefinitely unless killed
explicitly. The key elements in a topology are the spout and the
bolt. A spout is a source of input which can read data from various
datasources and passes it to a bolt. A bolt is the actual processing
unit that processes data and produces a new output stream. An output
stream from a bolt can be given as an input to another
bolt. <a class="reference internal" href="#www-storm-home-concepts" id="id201">[167]</a></p>
</li>
<li><p class="first">S4</p>
</li>
<li><p class="first">Samza</p>
<p>Apache Samza is an open-source near-realtime, asynchronous computational
framework for stream processing developed by the Apache Software
Foundation in Scala and Java. <a class="reference internal" href="#www-samza-3" id="id202">[168]</a>
Apache Samza is a distributed stream processing framework. It uses Apache
Kafka for messaging, and Apache Hadoop YARN to provide fault tolerance,
processor isolation, security, and resource management. Samza processes
streams. A stream is composed of immutable messages of a similar type or
category. Messages can be appended to a stream or read from a stream.
Samza supports pluggable systems that implement the stream abstraction:
in Kafka a stream is a topic, in a database we might read a stream by
consuming updates from a table, in Hadoop we might tail a directory of
files in HDFS. Samza is a stream processing framework. Samza provides a
very simple callback-based process message API comparable to MapReduce.
Samza manages snapshotting and restoration of a stream processors state.
Samza is built to handle large amounts of state (many gigabytes per
partition). <a class="reference internal" href="#www-samza-1" id="id203">[169]</a> Whenever a machine in the cluster fails,
Samza works with YARN to transparently migrate your tasks to another
machine. Samza uses Kafka to guarantee that messages are processed in the
order they were written to a partition, and that no messages are ever lost.
Samza is partitioned and distributed at every level. Kafka provides
ordered, partitioned, replayable, fault-tolerant streams. YARN provides a
distributed environment for Samza containers to run in. Samza works with
Apache YARN, which supports Hadoops security model, and resource isolation
through Linux CGroups <a class="reference internal" href="#www-samza-4" id="id204">[170]</a> <a class="reference internal" href="#www-samza-3" id="id205">[168]</a>.</p>
</li>
<li><p class="first">Granules</p>
<p>Granules in used for execution or processing of data streams in
distributed environment.
When applications are running concurrently on multiple computational
resources, granules manage their parallel execution.
The MapReduce implementation in Granules is responsible for providing
better performance.It has the capability of expressing computations like
graphs.
Computations can be scheduled based on periodicity or other activity.
Computations can be developed in C, C++, Java, Python, C#, R
It also provides support for extending basic Map reduce framework.
Its application domains include hand writing recognition, bio informatics
and computer brain interface <a class="reference internal" href="#www-granules" id="id206">[171]</a>.</p>
</li>
<li><p class="first">Neptune</p>
</li>
<li><p class="first">Google MillWheel</p>
<p>MillWheel is a framework for building low-latency data-processing
applications. Users specify a directed computation graph and
application code for individual nodes, and the system manages
persistent state and the continuous flow of records, all within
the envelope of the frameworks fault-tolerance guarantees. Other
streaming systems do not provide this combination of fault
tolerance, versatility, and scalability. MillWHeel allows for
complex streaming systems to be created without distributed
systems expertise. MillWheels programming model provides a
notion of logical time, making it simple to write time-based
aggregations. MillWheel was designed from the outset with fault
tolerance and scalability in mind. In practice, we find that
MillWheels unique combination of scalability, fault tolerance,
and a versatile programming model <a class="reference internal" href="#millwheel-paper" id="id207">[172]</a>.</p>
</li>
<li><p class="first">Amazon Kinesis</p>
<p>Kinesis is Amazons <a class="reference internal" href="#www-kinesis" id="id208">[173]</a> real time data processing
engine. It is designed to provide scalable, durable and reliable
data processing platform with low latency. The data to Kinesis
can be ingested from multiple sources in different format. This
data is further made available by Kinesis to multiple
applications or consumers interested in the data. Kinesis
provides robust and fault tolerant system to handle this high
volume of data. Data sharding mechanism is Kinesis makes it
horizontally scalable. Each of these shards in Kinesis process a
group of records which are partitioned by the shard key. Each
record processed by Kinesis is identified by sequence number,
partition key and data blob. Sequence number to records is
assigned by the stream. Partition keys are used by partitioner(a
hash function) to map the records to the shards i.e. which
records should go to which shard. Producers like web servers,
client applications, logs push the data to Kinesis whereas
Kinesis applications act as consumers of the data from Kinesis
engine. It also provides data retention for certain time for
example 24 hours default. This data retention window is a sliding
window. Kinesis collects lot of metrics which can used to
understand the amount of data being processed by Kinesis.  User
can use this metrics to do some analytics and visualize the
metrics data.  Kinesis is one of the tools part of AWS
infrastructure and provides its users a complete
software-as-a-service. Kinesis <a class="reference internal" href="#big-data-analytics-book" id="id209">[174]</a> in
the area of real-time processing provides following key benefits:
ease of use, parellel processing, scalable, cost effective, fault
tolerant and highly available.</p>
</li>
<li><p class="first">LinkedIn</p>
</li>
<li><p class="first">Twitter Heron</p>
<p>Heron is a real-time analytics platform that was developed at
Twitter for distributed streaming processing. Heron was
introduced at SIGMOD 2015 to overcome the shortcomings of Twitter
Storm as the scale and diversity of Twitter data increased. As
mentioned in <a class="reference internal" href="#twitterheronopen" id="id210">[175]</a> The primary advantages of
Heron were: API compatible with Storm: Back compatibility with
Twitter Storm reduced migration time. Task-Isolation: Every task
runs in process-level isolation, making it easy to debug/
profile. Use of main stream languages: C++, Java, Python for
efficiency, maintainability, and easier community
adoption. Support for backpressure: dynamically adjusts the rate
of data flow in a topology during run-time, to ensure data
accuracy. Batching of tuples: Amortizing the cost of transferring
tuples. Efficiency: Reduce resource consumption by 2-5x and Heron
latency is 5-15x lower than Storms latency. The architecture of
Heron (as shown in <a class="reference internal" href="#twitterheron" id="id211">[176]</a>)uses the Storm API to
submit topologies to a scheduler. The scheduler runs each
topology as a job consisting of several containers. The
containers run the topology master, stream manager, metrics
manager and Heron instances. These containers are managed by the
scheduler depending on resource availability.</p>
</li>
<li><p class="first">Databus</p>
</li>
<li><p class="first">Facebook Puma/Ptail/Scribe/ODS</p>
<p>The real time data Processing at Fcabook is carried out using the
technologies like Scibe,PTail, Puma and ODS. While designing the
system, facebook primarily focused on the five key decissions
that the system should incorporate and that included Ease of Use,
Performance , Fault-tolerance , Scalability and
Correctness.:cite: &#8216;www-facebook&#8217; &#8220;The real time data analytics
ecosystem at facebook is designed to handle hundreds of Gigabytes
of data per second via hundreds of data pipelines and this system
handles over 200,000 events per second with a maximum latency of
30 seconds&#8221;. :cite:&#8217;www-facebook&#8217;Fcabook focused on the Seconds
of latency while designing the system and not milliseconds as
seconds are fast enough to for all the use case that needs to be
supported, and it allowed facebook to use persistent message bus
for data transport and this made the system more fault toleranat
and scalable. :cite:&#8217;facebook-paper-2017&#8217; The large
infrastructure of facebook comprises of hundreds of systems
distributed across multiple data centers that needs a continious
monitoring to track their health and performance.Which is done by
Operational Data Store(ODS).ODS comprises of a time series
database (TSDB),which is a query service, and a detection and
alerting system. ODSs TSDB is built atop the HBase storage
system.Time series data from services running on Facebook hosts
is collected by the ODS write service and written to HBase.</p>
<p>When the data is generated by the user from their devices, an
AJAX request is fired to facebook,and these requests are then
written to a log file using Scribe(distributed data transport
system), this messaging system collect, aggregate and delivers
high volume of log data with few seconds of latency and high
throughput.Scribe stores the data in the HDFS(Hadoop Distributed
File System) in a tailing fashion, where the new events are
stored in log files and the files are tailed below the current
events.The events are then written into the storage HBase on
distributed machines. This makes the data avalible for both batch
and real-time processing. Ptail is an internal tool built to
aggregate data from multiple Scribe stores and It then tails the
log files and pulls data out for processing. Puma is a stream
processing system which is the real-time aggregation/storage of
data. Puma provides filtering and processing of Scribe streams
(with a few seconds delay), usually Puma batches the storage per
1.5 seconds on average and when the last flush completes, then
only a new batch starts to avoid the contention issues, which
makes i fairly real time</p>
</li>
<li><p class="first">Azure Stream Analytics</p>
<p>Azure Stream Analytics is a platform that manages data streaming
from devices, web sites, infrastructure systems, social media,
internet of things analytics, and other sources usings real-time
event processing engine. <a class="reference internal" href="#www-azurestreamanalytics" id="id212">[177]</a> Jobs
are authored by &#8220;specifying the input source of the streaming
data, the output sink for the results of your job, and a data
tranformation expressed in a SQL-like language.&#8221;  Some key
capabilities and benefits include ease of use, scalability,
reliability, repeatability, quick recovery, low cost, reference
data use, user defined functions capability, and
connectivity. <a class="reference internal" href="#www-docs-microsoft" id="id213">[178]</a> Available documentation
to get started with Azure Stream
Analytics. <a class="reference internal" href="#www-github-azure" id="id214">[179]</a> Azure Stream Analytics has a
development project available on github.</p>
</li>
<li><p class="first">Floe</p>
</li>
<li><p class="first">Spark Streaming</p>
</li>
<li><p class="first">Flink Streaming</p>
</li>
<li><p class="first">DataTurbine</p>
</li>
</ol>
</div>
<div class="section" id="basic-programming-model-and-runtime-spmd-mapreduce">
<h2>Basic Programming model and runtime, SPMD, MapReduce<a class="headerlink" href="#basic-programming-model-and-runtime-spmd-mapreduce" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="143">
<li><p class="first">Hadoop</p>
</li>
<li><p class="first">Spark <a class="reference internal" href="#www-spark" id="id215">[180]</a></p>
<p>Apache Spark which is an open source cluster computing framework
has emerged as the next generation big data processing engine
surpassing Hadoop MapReduce. &#8220;Spark engine is developed for
in-memory processing as well a disk based processing. This system
also provides large number of impressive high level tools such as
machine learning tool M Lib, structured data processing, Spark
SQL, graph processing took Graph X, stream processing engine
called Spark Streaming, and Shark for fast interactive question
device.&#8221; The ability of spark to join datasets across various
heterogeneous data sources is one of its prized
attributes. Apache Spark is not the most suitable data analysis
engine when it comes to processing (1) data streams where latency
is the most crucial aspect and (2) when the available memory for
processing is restricted. &#8220;When available memory is very limited,
Apache Hadoop Map Reduce may help better, considering huge
performance gap.&#8221; In cases where latency is the most crucial
aspect we can get better results using Apache Storm.</p>
</li>
<li><p class="first">Twister</p>
<p>Twister is a new software tool released by Indiana University, which is an
extension to MapReduce architectures currently used in the academia and
industry <a class="reference internal" href="#www-twister1" id="id216">[181]</a>. It supports faster execution of many data mining applications
implemented as MapReduce programs. Applications that currently use Twister
include: K-means clustering, Google&#8217;s page rank, Breadth first graph search
, Matrix multiplication, and Multidimensional scaling. Twister also builds
on the SALSA team&#8217;s work related to commercial MapReduce runtimes,
including Microsoft Dryad software and open source Hadoop software. SALSA
project work is funded in part by an award from Microsoft, Inc. The archite
cture is based on pub/sub messaging that enables it to perform faster data
transfers, minimizing the overhead of the runtime. Also, the support for
long running processes improves the efficiency of the runtime for many
iterative MapReduce computations. <a class="reference internal" href="#www-twister2" id="id217">[182]</a> <a class="reference internal" href="#www-twister3" id="id218">[183]</a>
<a class="reference internal" href="#paper-twister" id="id219">[184]</a>.</p>
</li>
<li><p class="first">MR-MPI</p>
<p><a class="reference internal" href="#www-mapreducempi" id="id220">[185]</a> MR-MPI stands for Map Reduce-Message
Passing Interface is open source library build on top of standard
MPI. It basically implements mapReduce operation providing a
interface for user to simplify writing mapReduce program.  It is
written in C++ and needs to be linked to MPI library in order to
make the basic map reduce functionality to be executed in
parallel on distributed memory architecture.  It provides
interface for c, c++ and python. Using C interface the library
can also be called from Fortrain.</p>
</li>
<li><p class="first">Stratosphere (Apache Flink)</p>
<p>Apache Flink is an open-source stream processing framework for
distributed, high-performing, always-available, and accurate data
streaming applications. Apache Flink is used in big data application
primarily involving analysis of data stored in Hadoop clusters.
It also supports a combination of in-memory and disk-based processing
as well as handles both batch and stream processing jobs, with data
streaming the default implementation and batch jobs running as
special-case versions of streaming application <a class="reference internal" href="#www-flink" id="id221">[186]</a>.</p>
</li>
<li><p class="first">Reef</p>
<p>REEF (Retainable Evaluator Execution Framework) <a class="reference internal" href="#www-reef" id="id222">[187]</a>
is a scale-out computing fabric that eases the development of Big
Data applications on top of resource managers such as Apache YARN
and Mesos. It is a Big Data system that makes it easy to
implement scalable, fault-tolerant runtime environments for a
range of data processing models on top of resource managers. REEF
provides capabilities to run multiple heterogeneous frameworks
and workflows of those efficiently. REEF contains two libraries,
Wake and Tang where Wake is an event-based-programming framework
inspired by Rx and SEDA and Tang is a dependency injection
framework inspired by Google Guice, but designed specifically for
configuring distributed systems.</p>
</li>
<li><p class="first">Disco</p>
</li>
<li><p class="first">Hama</p>
<p>Apache Hama is a framework for Big Data analytics which uses the
Bulk Synchronous Parallel (BSP) computing model, which was
established in 2012 as a Top-Level Project of The Apache Software
Foundation.It provides not only pure BSP programming model but
also vertex and neuron centric programming models, inspired by
Google&#8217;s Pregel and DistBelief <a class="reference internal" href="#apache-hama" id="id223">[188]</a>. It avoids the
processing overhead of MapReduce approach such as sorting,
shuffling, reducing the vertices etc. Hama provides a message
passing interface and each superstep in BSP is faster than a full
job execution in MApReduce framework, such as Hadoop
<a class="reference internal" href="#book-hama" id="id224">[189]</a>.</p>
</li>
<li><p class="first">Giraph</p>
</li>
<li><p class="first">Pregel</p>
</li>
<li><p class="first">Pegasus</p>
</li>
<li><p class="first">Ligra</p>
<p>Ligra is a Light Weight Graph Processing Framework for the graph
manipulation and analysis in shared memory system. It is
particularly suited for implementing on parallel graph traversal
algorithms where only a subset of the vertices are processed in an
iteration The interface is lightweight in that it supplies only a
few functions. The Ligra framework has two very simple routines,
one for mapping over edges and one for mapping over vertices.</p>
<p>:cite:&#8217;ligra-paper-2013 &#8216;The implementations of several graph
algorithms like BFS, breadth-first search, betweenness centrality,
graph radii estimation, graph-connectivity, PageRank and
Bellman-Ford single-source shortest paths efficient and scalable,
and often achieve better running times than ones reported by other
graph libraries/systems</p>
<p>:cite:&#8217;ligra-paper-2&#8217; Although the shared memory machines cannot
be scaled to the same size as distributed memory clusters but the
current commodity single unit servers can easily fit graphs with
well over a hundred billion edges in the shared memory systems
that is large enough for any of the graphs reported in the papers
mentioned above.</p>
</li>
<li><p class="first">GraphChi</p>
</li>
<li><p class="first">Galois</p>
<p>Galois system was built by intelligent software systems team at
University of Texas, Austin. As explained in
<a class="reference internal" href="#www-galoissite" id="id225">[190]</a>, Galois is a system that automatically
executes &#8216;Galoized&#8217; serial C++ or Java code&nbsp;in parallel&nbsp;on
shared-memory machines. It works by exploiting amorphous
data-parallelism, which is present even in irregular codes that
are organized around pointer-based data structures such as graphs
and trees. By using Galois provided data structures programmers
can write serial programs that gives the performance of parallel
execution. Galois employs annotations at loop levels to
understand correct context during concurrent execution and
executes the code that could be run in parallel. The key idea
behind Galois is Tao-analysis, in which parallelism is exploited
at compile time rather than at run time by creating operators
equivalent of the code by employing data driven local computation
algorithm <a class="reference internal" href="#taoparallelismpaper" id="id226">[191]</a>. Galois currently supports
C++ and Java.</p>
</li>
<li><p class="first">Medusa-GPU</p>
</li>
<li><p class="first">MapGraph</p>
</li>
<li><p class="first">Totem</p>
<p>Totem is a project to overcome the current challenges in graph
algorithms.  The project is research the Networked Systems
Laboratory (NetSysLab) The issue resides in the scale of real
world graphs and the inability to process them on platforms
other than a supercomputer.  Totem is based on a bulk synchronous
parallel(BSP) model that can enable hybrid CPU/GPU systems to process
graph based applications in a cost effective manner.
<a class="reference internal" href="#www-netsyslab" id="id227">[192]</a></p>
</li>
</ol>
</div>
<div class="section" id="inter-process-communication-collectives">
<h2>Inter process communication Collectives<a class="headerlink" href="#inter-process-communication-collectives" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="160">
<li><p class="first">point-to-point</p>
</li>
<li><ol class="first loweralpha simple">
<li>publish-subscribe: MPI</li>
</ol>
<p>see <a class="reference external" href="http://www.slideshare.net/Foxsden/high-performance-processing-of-streaming-data">http://www.slideshare.net/Foxsden/high-performance-processing-of-streaming-data</a></p>
</li>
</ol>
<ol class="arabic" start="161">
<li><ol class="first loweralpha simple" start="2">
<li>publish-subscribe: Big Data</li>
</ol>
<p>Publish/Subscribe (Pub/Sub) <a class="reference internal" href="#thesis-pub-sub" id="id228">[193]</a> is a
communication paradigm in which subscribers register their
interest as a pattern of events or topics and then asynchronously
receive events matching their interest. On the other hand,
publishers generate events that are delivered to subscribers with
matching interests.  In Pub/sub systems, publishers and
subscribers need not know each other. Pub/sub technology is
widely used for a loosely coupled interaction between disparate
publishing data-sources and numerous subscribing data-sinks. The
two most widely used pub/sub schemes are - Topic-Based
Publish/Subscribe (TBPS) and Content-Based Publish/Subscribe
(CBPS) <a class="reference internal" href="#paper-pub-sub" id="id229">[194]</a>.</p>
<p>Big Data analytics architecture are being built on top of a
publish/subscribe service stratum, serving as the communication
facility used to exchange data among the involved components
<a class="reference internal" href="#paper-pub-sub-bigdata" id="id230">[195]</a>. Such a publish/subscribe service
stratum brilliantly solves several interoperability issues due to
the heterogeneity of the data to be handled in typical Big Data
scenarios.</p>
<p>Pub/Sub systems are being widely deployed in Centralized
datacenters, P2P environments, RSS feed notifications, financial
data dissemination, business process management, Social
interaction message notifications- Facebook, Twitter, Spotify,
etc.</p>
</li>
<li><p class="first">HPX-5</p>
<p>Based on <a class="reference internal" href="#www-hpx-5" id="id231">[196]</a>, High Performance ParallelX (HPX-5)
is an open source, distributed model that provides opportunity
for operations to run unmodified on one-to-many nodes. The
dynamic nature of the model accommodates effective computing
resource management and task scheduling. It is portable and
performance-oriented. HPX-5 was developed by IU Center for
Research in Extreme Scale Technologies (CREST). Concurrency is
provided by lightweight control object (LCO) synchronization and
asynchronous remote procedure calls. ParallelX component allows
for termination detection and supplies per-process
collectives. It addresses the challenges of starvation, latency,
overhead, waiting, energy and reliability. Finally, it supports
OpenCL to use distributed GPU and coprocessors. HPX-5 could be
compiled on various OS platforms , however it was only tested on
several Linux and Darwin (10.11) platforms. Required
configurations and environments could be accessed via
<a class="reference internal" href="#www-hpx-5-user-guide" id="id232">[197]</a>.</p>
</li>
<li><p class="first">Argo BEAST HPX-5 BEAST PULSAR</p>
<p>Search on the internet was not successsful.</p>
</li>
<li><p class="first">Harp</p>
<p>Harp <a class="reference internal" href="#www-harp" id="id233">[198]</a> is a simple, easy to maintain, low risk and
easy to scale static web server that also serves Jade, Markdown,
EJS, Less, Stylus, Sass, and CoffeeScript as HTML, CSS, and
JavaScript without any configuration and requires low cognitive
overhead. It supports the beloved layout/partial paradigm and it
has flexible metadata and global objects for traversing the file
system and injecting custom data into templates. It acts like a
lightweight web server that was powerful enough for me to abandon
web frameworks for dead simple front-end publishing. Harp can
also compile your project down to static assets for hosting
behind any valid HTTP server.</p>
</li>
<li><p class="first">Netty</p>
<p>Netty <a class="reference internal" href="#www-netty" id="id234">[199]</a> &#8220;is an asynchronous event-driven network
application framework for rapid development of maintainable high
performance protocol servers &amp; clients&#8221;. Netty <a class="reference internal" href="#netty-book" id="id235">[200]</a>
&#8220;is more than a collection of interfaces and classes; it also
defines an architectural model and a rich set of design
patterns&#8221;. It is protocol agnostic, supports both connection
oriented protocols using TCP and connection less protocols built
using UDP. Netty offers performance superior to standard Java NIO
API thanks to optimized resource management, pooling and reuse
and low memory copying.</p>
</li>
<li><p class="first">ZeroMQ</p>
<p>In <a class="reference internal" href="#www-zeromq" id="id236">[201]</a>, ZeroMQ is introduced as a software product
that can &#8220;connect your code in any language, on any platform&#8221; by
leveraging &#8220;smart patterns like pub-sub, push-pull, and
router-dealer&#8221; to carry &#8220;messages across inproc, IPC, TCP, TIPC,
[and] multicast.&#8221; In <a class="reference internal" href="#www-zeromq2" id="id237">[202]</a>, it is explained that
ZeroMQ&#8217;s &#8220;asynchronous I/O model&#8221; causes this &#8220;tiny library&#8221; to
be &#8220;fast enough to be the fabric for clustered products.&#8221; In
<a class="reference internal" href="#www-zeromq" id="id238">[201]</a>, it is made clear that ZeroMQ is &#8220;backed by a
large and open source community&#8221; with &#8220;full commercial support.&#8221;
In contrast to Message Passing Interface (i.e. MPI), which is
popular among parallel scientific applications, ZeroMQ is
designed as a fault tolerant method to communicate across highly
distributed systems.</p>
</li>
<li><p class="first">ActiveMQ</p>
</li>
<li><p class="first">RabbitMQ</p>
<p>RabbitMQ is a message broker <a class="reference internal" href="#www-rabbitmq" id="id239">[203]</a> which allows
services to exchange messages in a fault tolerant manner. It
provides variety of features which enables software applications
to connect and scale. Features are: reliability, flexible
routing, clustering, federation, highly available queues,
multi-protocol, many clients, management UI, tracing, plugin
system, commercial support, large community and user
base. RabbitMQ can work in multiple scenarios:</p>
<ol class="arabic">
<li><p class="first">Simple messaging: producers write messages to the queue and
consumers read messages from the the queue. This is synonymous
to a simple message queue.</p>
</li>
<li><p class="first">Producer-consumer: Producers produce messages and consumers
receive messages from the queue. The messages are delivered to
multiple consumers in round robin manner.</p>
</li>
<li><p class="first">Publish-subscribe: Producers publish messages to exchanges
and consumers subscribe to these exchanges. Consumers receive
those messages when the messages are available in those
exchanges.</p>
</li>
<li><p class="first">Routing: In this mode consumers can subscribe to a subset
of messages instead of receiving all messages from the queue.</p>
</li>
<li><p class="first">Topics: Producers can produce messages to a topic multiple
consumers registered to receive messages from those topics get
those messages. These topics can be handled by a single
exchange or multiple exchanges.</p>
</li>
<li><p class="first">RPC:In this mode the client sends messages as well as
registers a callback message queue. The consumers consume the
message and post the response message to the callback queue.</p>
<p>RabbitMQ is based on AMPQ <a class="reference internal" href="#ampq-article" id="id240">[204]</a> (Advanced
Message Queuing Protocol) messaging model. AMPQ is described
as follows messages are published to exchanges, which are
often compared to post offices or mailboxes. Exchanges then
distribute message copies to queues using rules called
bindings. Then AMQP brokers either deliver messages to
consumers subscribed to queues, or consumers fetch/pull
messages from queues on demand</p>
</li>
</ol>
</li>
<li><p class="first">NaradaBrokering</p>
</li>
<li><p class="first">QPid</p>
</li>
<li><p class="first">Kafka</p>
<p>Apache Kafka is a streaming platform, which works based on
publish-subscribe messaging system and supports distributed environment.</p>
<p><em>Kafka lets you publish and subscribe to the messages.</em> Kafka maintains
message feeds based on topic. A topic is a category or feed name to
which records are published. Kafkas Connector APIs are used to publish
the messages to one or more topics, whereas, Consumer APIs are used to
subscribe to the topics.</p>
<p><em>Kafka lets you process the stream of data at real time.</em> Kafkas stream
processor takes continual stream of data from input topics, processes the
data in real time and produces streams of data to output topics. Kafkas
Streams API are used for data transformation.</p>
<p><em>Kafka lets you store the stream of data in distributed clusters.</em> Kafka
acts as a storage system for incoming data stream. As Kafka is a distributed
system, data streams are partitioned and replicated across nodes.</p>
<p>Thus, a combination of messaging, storage and processing data stream makes
Kafka a streaming platform. It can be used for building data pipelines
where data is transferred between systems or applications. Kafka can also be
used by applications that transform real time incoming data. :cite:&#8217;www-kafka&#8217;</p>
</li>
<li><p class="first">Kestrel</p>
<p>Kestrel is a distributed message queue, with added features and
bulletproofing, as well as the scalability offered by actors and
the Java virtual machine. It supports multiple protocols: memcache:
the memcache protocol; thrift: Apache Thrift-based RPC; text: a simple
text-based protocol. Each queue is strictly ordered following the FIFO
(first in, first out) principle. To keep up with performance items are
cached in system memory. Kestrel is more durable as queues are stored
in memory for speed, but logged into a journal on disk so that servers
can be shutdown or moved without losing any data. When kestrel starts
up, it scans the journal folder and creates queues based on any journal
files it finds there, to restore state to the way it was when it last
shutdown (or was killed or died).</p>
<p>Kestrel uses a pull-based data aggregator system that convey data without
prior definition on its destination. So the destination can be defined
later on either storage system, like HDFS or NoSQL, or processing system,
like storm and sppark streaming. Each server handles a set of reliable,
ordered message queues. When you put a cluster of these servers together,
with no cross communication, and pick a server at random whenever you do
a set or get, you end up with a reliable, loosely ordered message
queue <a class="reference internal" href="#git-kestrel" id="id241">[205]</a>.</p>
</li>
<li><p class="first">JMS</p>
<p>JMS (Java Messaging Service) is a java oriented messaging standard
that defines a set of interfaces and semantics which allows
applications to send, receive, create, and read messages.  It allows
the communication between different components of a distributed
application to be loosely coupled, reliable, and
asynchronous. <a class="reference internal" href="#www-jms-wiki" id="id242">[206]</a> JMS overcomes the drawbacks of RMI
(Remote Method Invocation) where the sender needs to know the method
signature of the remote object to invoke it and RPC(Remote Procedure
Call), which is tightly coupled i.e it cannot function unless the
sender has important information about the receiver.</p>
<p>JMS establishes a standard that provides loosely coupled communication
i.e the sender and receiver need not be present at the same time or
know anything about each other before initiating the communication.
JMS provides two communication domains.A point-to-point messaging
domain where there is one producer and one consumer. On generating
message, a producer simple pushes the message to a message queue which
is known to the consumer. The other communication domain is
publish/subscribe model, where one message can have multiple
receivers. <a class="reference internal" href="#www-jms-oracle-docs" id="id243">[207]</a></p>
</li>
<li><p class="first">AMQP</p>
<p><a class="reference internal" href="#www-amqp" id="id244">[208]</a> AMQP stands for Advanced Message Queueing
Protocol. AMQP is open interenet protocol that allows secure and
reliable communication between applications in different
orginization and different applications which are on diffferent
platforms. AMQP allows businesses to implement middleware
applications interoperability by allowing secure message transfer
bewteen the applications on timly manner. AMQP is mainly used by
financial and banking business. Other sectors that aslo use AMQP
are Defence, Telecommunication, cloud Computing and so on.
Apache Qpid, StormMQ, RabbitMQ, MQlight, Microsoft&#8217;s Windows
Azure Service Bus, IIT Software&#8217;s SwiftMQ and JORAM are some of
the products that implement AMQP protocol.</p>
</li>
<li><p class="first">Stomp</p>
</li>
<li><p class="first">MQTT</p>
<p>According to <a class="reference internal" href="#www-mqtt" id="id245">[209]</a>, Message Queueing Telemetry
Transport (MQTT) protocol is an Interprocess communication
protocol that could serve as better alternative to HTTP in
certain cases. It is based on a publish-subscribe messaging
pattern. Any sensor or remote machine can publish it&#8217;s data and
any registered client can subscribe the data. A broker takes care
of the message being published by the remote machine and updates
the subscriber in case of new message from the remote
machine. The data is sent in binary format which makes it use
less bandwidth. It is designed mainly to cater to the needs to
devices that has access to minimal network bandwidth and device
resources without affecting reliability and quality assurance of
delivery. MQTT protocol has been in use since 1999. One of the
notable work is project Floodnet <a class="reference internal" href="#www-floodnet" id="id246">[210]</a>, which
monitors river and floodplains through a set of sensors.</p>
</li>
<li><p class="first">Marionette Collective</p>
</li>
<li><p class="first">Public Cloud: Amazon SNS</p>
<p>Amazon SNS is an Inter process communication service which gives
the user simple, end-to-end push messaging service allowing them
to send messages, alerts, or notifications. According to
<a class="reference internal" href="#www-sns" id="id247">[211]</a>, it can be used to send a directed message
intended for an entity or to broadcast messages to list of
selected entities. It is an easy to use and cost effective
mechanism to send push messages. Amazon SNS is compatible to send
push notifications to iOS, Windows, Fire OS and Android OS
devices.</p>
<p>According to <a class="reference internal" href="#sns-blog" id="id248">[212]</a> SNS system architecture consists
of four elements: (1) Topics, (2) Owners, (3) Publishers, and
(4) Subscribers. Topics are events or access points that identifies
the subject of the event and can be accessed by an unique
identifier(URI). Owners create topics and control all access to
the topic and define the corressponding permission for each
topic. Subscribers are clients (applications, end-users,
servers, or other devices) that want to receive messages or
notifications on specific topics of interest to them.Publishers
send messages to topics. SNS matches the topic with the list of
subscribers interested in the topic, and delivers the message to
them.</p>
<p>According to <a class="reference internal" href="#sns-faq" id="id249">[213]</a>, Amazon SNS follows pay as per usage. In
general it is $0.50 per 1 million Amazon SNS Requests.Amazon SNS
supports notifications over multiple transport protocols such as
HTTP/HTTPS, Email/Email-JSON, SQS(Message queue) and SMS.Amazon SNS
can be used with other AWS services such as Amazon SQS, Amazon EC2 and
Amazon S3.</p>
</li>
<li><p class="first">Lambda</p>
<p>AWS Lambda is a product from amazon which facilitates serverless
computing <a class="reference internal" href="#www-awslambda" id="id250">[214]</a>.AWS Lambda allows for running the code
without the need for provisioning or managing servers, all server management
is taken care by AWS.The code to be run on AWS Lambda is called a server
function which can be written in Node.js,Python,Java,C#.Each Lambda
function is to be stateless and any persistent data needs are to be handled
through storage devices.AWS Lambda function can be setup using the AWS Lambda
console where one can setup the function code and specify the event that
triggers the functional call.AWS Lamda service supports multiple event sources
as identified in <a class="reference internal" href="#www-awslambdaevent" id="id251">[215]</a>.AWS Lambda is designed to use
replication and redundancy to provide for high availability both for the service
itself and the function it runs.AWS Lambda automatically scales your application
by running the code in response to each trigger. The code runs in parallel and
processes each trigger individually, scaling precisely with the size of the
workload.Billing for AWS Lambda is based on the number of times the code executes
and in 100 ms increments of the duration of the processing.</p>
</li>
<li><p class="first">Google Pub Sub</p>
<p><a class="reference internal" href="#www-google-pub-sub" id="id252">[216]</a> Google Pub/Sub provides an asynchronous
messaging facility which assists the communication between independent
applications. It works in real time and helps keep the two interacting
systems independent. It is the same technology used by many of the
Google apps like GMail, Ads, etc. and so integration with them becomes
very easy. <a class="reference internal" href="#www-google-pub-sub-features" id="id253">[217]</a> Some of the typical
features it provides are: (1) Push and Pull - Google Pub/Sub integrates
quickly and easily with the systems hosted on the Google Cloud Platform
thereby supporting one-to-many, one-to-one and many-to-many
communication, using the push and pull requests. (2) Scalability - It
provides high scalability and availability even under heavy load without
any degradation of latency. This is done by using a global and highly
scalable design. (3) Encryption - It provides security by encryption of
the stored data as well as that in transit. Other than these important
features, it provides some others as well, like the usage of RESTful
APIs, end-to-end acknowledgement, replicated storage, etc.</p>
</li>
<li><p class="first">Azure Queues</p>
<p>Azure Queues storage is a Microsoft Azure service, providing inter
-process communication by message passing <a class="reference internal" href="#silberschatz1998operating" id="id254">[218]</a>.
A sender sends the message and a client receives and processes them.
The messages are stored in a queue which can contain millions of
messages, up to the total capacity limit of a storage account <a class="reference internal" href="#www-azurequeue-web" id="id255">[219]</a>.
Each message can be up to 64 KB in size. These messages can then be
accessed from anywhere in the world via authenticated calls using HTTP or
HTTPS. Similar to the other message queue services, Azure Queues enables
decoupling of the components <a class="reference internal" href="#www-tutorialspoint" id="id256">[220]</a>. It runs in an
asynchronous environment where messages can be sent among the different
components of an application. Thus, it provides an efficient solution for
managing workflows and tasks. The messages can remain in the queue up to 7
days, and afterwards, they will be deleted automatically.</p>
</li>
<li><p class="first">Event Hubs</p>
</li>
</ol>
</div>
<div class="section" id="in-memory-databases-caches">
<h2>In-memory databases/caches<a class="headerlink" href="#in-memory-databases-caches" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="183">
<li><p class="first">Gora (general object from NoSQL)</p>
<p>Gora is a in-memory data model <a class="reference internal" href="#www-gora" id="id257">[221]</a> which also
provides persistence to the big data. Gora provides persistence
to different types of data stores. Primary goals of Gora are:</p>
<ol class="arabic simple">
<li>data persistence</li>
<li>indexing</li>
<li>data access</li>
<li>analysis</li>
<li>map reduce support</li>
</ol>
<p>Unlike ORM models which mostly work with relational databases for
example hibernate gora works for most type of data stores like
documents, columnar, key value as well as relational. Gora uses
beans to maintain the data in-memory and persist it on
disk. Beans are defined using apache avro schema. Gora provides
modules for each type of data store it supports.  The mapping
between bean definition and datastore is done in a mapping file
which is specific to a data store.  Type Gora workflow will be:</p>
<ol class="arabic simple">
<li>define  the bean used as model for persistence</li>
<li>use gora compiler to compile the bean</li>
<li>create a mapping file to map bean definition to datastore</li>
<li>update gora.properties to specify the datastore to use</li>
<li>get an instance of corresponding data store using datastore factory.</li>
</ol>
<p>Gora has a query interface to query the underlying data
store. Its configuration is stored in gora.properties which
should be present in classpath. In the file you can specify
default data store used by Gora engine. Gora also has a CI/CD
library call GoraCI which is used to write integration tests.</p>
</li>
<li><p class="first">Memcached</p>
<p>Memcached is a free and open-source, high performance, distributed memory
object caching system. <a class="reference internal" href="#www-memcached" id="id258">[222]</a> Although, generic in nature,it
is intended for se in speeding up dynamic web applications by reducing
the database load.</p>
<p>It can be thought of as a short term memory for your applications.
Memcached is an in-memory key-value store for small chunks of arbitrary
data from the results of database calls, API calls and page rendering. Its
API is available in most of the popular languages. In simple terms, it
allows you to take memory from parts of your system where you have more
memory than you need and allocate it to parts of your system where you
have less memory than you need.</p>
</li>
<li><p class="first">Redis</p>
<p>Redis (Remote Dictionary Server) is an open source ,in-memory,
key-value database which is commonly referred as a data structure
server.  :cite:&#8217;redis-book-2011&#8217; &#8220;It is called a data structure
server and not simply a key-value store because Redis implements
datastructure which allows keys to contain binary safe strings
,hashes,sets and sortedsets, as well as lists&#8221; .Rediss
exceptional performance, simplicity to use and implement, and
atomic manipulation of data structures lends itself to solving
problems that are difficult or perform poorly when implemented
with traditional relational databases.  :cite:&#8217;redis-book-2016&#8217;
&#8220;Salivator Sanfilippo(Creator of open-sorce database Redis) makes
a strong case that Redis does not need to replace the existing
database but is an excellent addition to an enterprise for new
functionalities or to solve sometimes intyractable problems.&#8221;</p>
<p>:cite:&#8217;redis-book-2016&#8217; A very popular use pattern for Redis is
an in-memory cache for web-applications. The second popular use
pattern for REDIS is for metric storage of such quantitative data
such as web page usage and user behaviour on gamer leaderboards
where using a bit operations on strings, Redis very efficently
stores binary information on a particular characteristics.The
third popular Redis use pattern is a communication layer between
different systems through a publish/subscribe(pub/sub for short),
where one can post message to one or more channels that can be
acted upon by other systems that are subscribed to or listening
to that channel for incoming message. The Comapnies using REDIS
includes Twitter to store the timelines of all the user ,
Pinterest stores the user follower graph, Github, popular web
frameworks like Node.js ,Django,Ruby-on-Rails etc.</p>
</li>
<li><p class="first">LMDB (key value)</p>
<p>LMDB (Lighting memory-mapped Database) is a high performance embedded
transactional database in form of a key-value store
<a class="reference internal" href="#www-keyvalue" id="id259">[223]</a>. LMDB is designed around
virtual memory facilities found in modern operating
systems, multi-version concurrency control (MVCC)
and single-level store (SLS) concepts. LMDB stores
arbitrary key/data pairs as byte arrays, provides a
range-based search capability, supports multiple
data items for a single key and has a special mode
for appending records at the end of the database
(MDB_APPEND) which significantly increases its write
performance compared to other similar databases.</p>
<p>LMDB is not a relational database <a class="reference internal" href="#www-relationaldb" id="id260">[224]</a> and
strictly uses key-value store. Key-value databases
allows one write at a time, the difference that LMDB
highlights is that write transactions do not block
readers nor do readers block writes. Also, it does
allow multiple applications on the same system to
open and use the store simultaneously which helps in
scaling up performance <a class="reference internal" href="#www-lmdb" id="id261">[225]</a>.</p>
</li>
<li><p class="first">Hazelcast</p>
<p>Hazelcast is a java based, in memory data grid <a class="reference internal" href="#www-wikihazel" id="id262">[226]</a>.
It is open source software, released under the Apache 2.0 License
<a class="reference internal" href="#www-githubhazel" id="id263">[227]</a>. Hazelcast enables predictable scaling for
applications by providing in memory access to data.
Hazelcast uses a grid to distribute data evenly across
a cluster. Clusters allow processing and storage to scale
horizontally. Hazelcast can run locally, in the cloud, in virtual
machines, or in Docker containers. Hazelcast can be utilized for
a wide variety of applications. It has APIs for many programing
languages including Python, Java, Scala, C++, .NET and Node.js and
supports any binary languages through an Open Binary Client Protocol
<a class="reference internal" href="#www-wikihazel" id="id264">[226]</a>.</p>
</li>
<li><p class="first">Ehcache</p>
<p>EHCACHE is an open-source Java-based cache. It supports distributed
caching and could scale to hundred of caches. It comes with REST APIs
and could be integrated with popular frameworks like Hibernate
<a class="reference internal" href="#www-ehcache-features" id="id265">[228]</a>. It offers storage tires such that less
frequently data could be moved to slower tires
<a class="reference internal" href="#www-ehcache-documentation" id="id266">[229]</a>. It&#8217;s XA compliant and supports two-
phase commit and recovery for transactions. It&#8217;s developed and
maintained by Terracotta and is available under Apache 2.0 license.
It conforms to Java caching standard JSR 107.</p>
</li>
<li><p class="first">Infinispan</p>
</li>
<li><p class="first">VoltDB</p>
<p>VoltDB is an in-memory database. It is an ACID-compliant RDBMS
which uses a shared nothing architecture to achieve database
parallelism. It includes both enterprise and community
editions. VoltDB is a scale-out NewSQL relational database that
supports SQL access from within pre-compiled Java stored
procedures.  VoltDB relies on horizontal partitioning down to the
individual hardware thread to scale, k-safety (synchronous
replication) to provide high availability, and a combination of
continuous snapshots and command logging for durability (crash
recovery) <a class="reference internal" href="#voltdb-www" id="id267">[230]</a>. The in-memory, scale-out
architecture couples the speed of traditional streaming solutions
with the consistency of an operational database. This gives a
simplified technology stack that delivers low-latency response
times (1ms) and hundreds of thousands of transactions per
second. VoltDB allows users to ingest data, analyze data, and act
on data in milliseconds, allowing users to create per-person,
real-time experiences <a class="reference internal" href="#voltdb-wiki" id="id268">[231]</a>.</p>
</li>
<li><p class="first">H-Store</p>
<p>H-Store is an in memory and parallel database management system
for on-line transaction processing (OLTP). Specifically ,
<a class="reference internal" href="#www-hstore" id="id269">[232]</a> illustrates that H-Store is a highly
distributed, row-store-based relational database that runs on a
cluster on shared-nothing, main memory executor nodes.As Noted in
<a class="reference internal" href="#kallman2008" id="id270">[233]</a> &#8220;the architectural and application shifts
have resulted in modern OLTP databases increasingly falling short
of optimal performance.In particular, the availability of
multiple-cores, the abundance of main memory, the lack of user
stalls, and the dominant use of stored procedures are factors
that portend a clean-slate redesign of RDBMSs&#8221;.The H-store which
is a complete redesign has the potential to outperform legacy
OLTP databases by a significant factor.  As detailed in
<a class="reference internal" href="#www-hstorewiki" id="id271">[234]</a> H-Store is the first implementation of a
new class of parallel DBMS, called NewSQL, that provides the
high-throughput and high-availability of NoSQL systems, but
without giving up the transactional guarantees of a traditional
DBMS.  The H-Store system is able to scale out horizontally
across multiple machines to improve throughput, as opposed to
moving to a more powerful , more expensive machine for a
single-node system.</p>
</li>
</ol>
</div>
<div class="section" id="object-relational-mapping">
<h2>Object-relational mapping<a class="headerlink" href="#object-relational-mapping" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="192">
<li><p class="first">Hibernate</p>
</li>
<li><p class="first">OpenJPA</p>
</li>
<li><p class="first">EclipseLink</p>
<p>EclipseLink is an open source persistence Services project from Eclipse
foundation. It is a framework which provide developers to
interact with data services including database and web services,
Object XML mapping etc. <a class="reference internal" href="#www-eclipselink" id="id272">[235]</a>. This is the project
which was developed out of Oracle&#8217;s Toplink product. The main
difference is EclipseLink does not have some key enterprise
feature. Eclipselink support a number of persistence standard
model like JPA, JAXB, JCA and Service Data Object. Like Toplink,
the ORM (Object relational model) is the technique to convert
incompatible type system in Object Oriented programming
language. It is a framework for storing java object into
relational database.</p>
</li>
<li><p class="first">DataNucleus</p>
<p>DataNucleus (available under Apache 2 open source license) is a
data management framework in Java. Formerly known as Java
Persistent Objects (JPOX) this was relaunched in 2008 as
DataNucleus. According to <a class="reference internal" href="#datanucleuswiki" id="id273">[236]</a> DataNucleus
Access Platform is a fully compliant implementation of the Java
Persistent API (JPA) and Java Data Objects (JDO)
specifications. It provides persistence and retrieval of data to
a number of datastores using a number of APIs, with a number of
query languages. In addition to object-relational mapping (ORM)
it can also map and manage data from sources other than RDBMS
(PostgreSQL, MySQL, Oracle, SQLServer, DB2, H2 etc.) such as
Map-based (Cassandra, HBase), Graph-based (Neo4j), Documents
(XLS, OOXML, XML, ODF), Web-based (Amazon S3, Google Storage,
JSON), Doc-based (MongoDB) and Others (NeoDatis, LDAP). It
supports the JPA (Uses JPQL Query language), JDO (Uses JDOQL
Query language) and REST APIs <a class="reference internal" href="#datanucleus" id="id274">[237]</a>.DataNucleus
products are built from a sequence of plugins where each of it is
an OSGi bundle and can be used in an OSGi environment. Google App
Engine uses DataNucleus as the Java persistence layer
<a class="reference internal" href="#datanucleusperformance" id="id275">[238]</a>.</p>
</li>
<li><p class="first">ODBC/JDBC</p>
<p>Open Database Connectivity (ODBC) is an open standard application
programming interface (API) for accessing database management
systems (DBMS) <a class="reference internal" href="#www-odbc" id="id276">[239]</a>. ODBC was developed by the SQL
Access Group and released in September, 1992. Microsoft Windows
was the first to provide an ODBC product. Later the versions for
UNIX, OS/2, and Macintosh platforms were developed. ODBC is
independent of the programming language, database system and
platform.</p>
<p>Java Database Connectivity (JDBC) is a API developed specific to
the Java programming language. JDBC was released as part of Java
Development Kit (JDK) 1.1 on February 19, 1997 by Sun
Microsystems <a class="reference internal" href="#www-jdbc" id="id277">[240]</a>. The java.sql and javax.sql
packages contain the JDBC classes. JDBC is more suitable for
object oriented databases. JDBC can be used for ODBC compliant
databases by using a JDBC-to-ODBC bridge.</p>
</li>
</ol>
</div>
<div class="section" id="extraction-tools">
<h2>Extraction Tools<a class="headerlink" href="#extraction-tools" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="197">
<li><p class="first">UIMA</p>
<p>Unstructured Information Management applications (UIMA) provides
a framework for content analytics. It searches unstructured data
to retrieve specific targets for the user. For example, when a
text document is given as input to the system, it identifies
targets such as persons, places, objects and even
associations. According to , <a class="reference internal" href="#uima-wiki" id="id278">[241]</a> theUIMA
architecture can be thought of as four dimensions: 1. Specifies
component interfaces in analytics pipeline.  2. Describes a set
of Design patterns. 3. Suggests two data representations: an
in-memory representation of annotations for high-performance
analytics and an XML representation of annotations for
integration with remote web services. 4. Suggests development
roles allowing tools to be used by users with diverse skills.</p>
<p>UIMA uses different, possibly mixed, approaches which include
Natural Language Processing, Machine Learning, IR. UIMA supports
multimodal analytics <a class="reference internal" href="#uima-ss" id="id279">[242]</a> which enables the system to
process the resource fro various points of view. UIMA is used in
several software projects such as the IBM Research&#8217;s Watson uses
UIMA for analyzing unstructured data and Clinical Text Analysis
and Knowledge Extraction System (Apache cTAKES) which is a
UIMA-based system for information extraction from medical
records.</p>
</li>
</ol>
<ol class="arabic" start="381">
<li><p class="first">Tika</p>
<p>&#8220;The Apache Tika toolkit detects and extracts metadata and text
from over a thousand different file types (such as PPT, XLS, and
PDF). All of these file types can be parsed through a single
interface, making Tika useful for search engine indexing, content
analysis, translation, and much more. <a class="reference internal" href="#www-tika" id="id280">[243]</a>&#8220;</p>
</li>
</ol>
</div>
<div class="section" id="sql-newsql">
<h2>SQL(NewSQL)<a class="headerlink" href="#sql-newsql" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="198">
<li><p class="first">Oracle</p>
<p>Oracle database is an object-relational database management system by
Oracle. Following are some of the key features of Oracle <a class="reference internal" href="#www-oracle" id="id281">[244]</a>
1. ANSI SQL Compliance
2. Multi-version read consistency
3. Procedural extensions: PL/SQL and Java.
Apart from above they are performance related features, including but not
limited to: indexes, in-memory, partitioning, optimization.
As of today the latest release of Oracle is <a class="reference internal" href="#www-oracle" id="id282">[244]</a>
Oracle Database 12c Release 1: 12.1 (Patch set as of June 2013 )</p>
</li>
<li><p class="first">DB2</p>
</li>
<li><p class="first">SQL Server</p>
<p>SQL Server <a class="reference internal" href="#www-sqlserver-wiki" id="id283">[245]</a> is a relational database
management system from Microsoft. As of Jan 2017, SQL Server is
available in below editions</p>
<ol class="arabic simple">
<li>Standard - consists of core database engine</li>
<li>Web - low cost edition for web hosting</li>
<li>Business Intelligence - includes standard edition and business
intelligence tools like PowerPivot, PowerBI, Master Data Services</li>
<li>Enterprise - consists of core database engine and enterprise services
like cluster manager</li>
<li>SQL Server Azure - <a class="reference internal" href="#www-azuresql" id="id284">[246]</a> core database engine
integrated with Microsoft Azure cloud platform and available in
platform-as-a-service mode.</li>
</ol>
<p>In the book <a class="reference internal" href="#book-sqlserver" id="id285">[247]</a>, the technical architecture of SQL Server in
OLTP(online transaction processing), hybrid cloud and business
intelligence modes is explained in detail.</p>
</li>
<li><p class="first">SQLite</p>
</li>
<li><p class="first">MySQL</p>
<p>MySQL is a relational database management system. <a class="reference internal" href="#devmysql" id="id286">[248]</a> SQL
is an acronym for Structured Query Language and is a standardized
language used to interact with the databases. <a class="reference internal" href="#devmysql" id="id287">[248]</a>
Databases provide structure to a collection of data
while. <a class="reference internal" href="#devmysql" id="id288">[248]</a> A database management system allows for the
addition, accessing, and processing of the data stored in a
database. <a class="reference internal" href="#devmysql" id="id289">[248]</a> Relational databases utilize tables that are
broken down into columns, representing the various fields of the
table, and rows, which correspond to individual entries in the
table. <a class="reference internal" href="#howmysql" id="id290">[249]</a></p>
</li>
<li><p class="first">PostgreSQL</p>
</li>
<li><p class="first">CUBRID</p>
<p>CUBRID name is deduced from the combination of word CUBE(security
within box) and BRIDGE(data bridge).  It is an open source
Relational DataBase Management System designed in C programming
language with high performance, scalability and availability
features. During its development by NCL, korean IT service
provider the goal was to optimize database performance for
web-applications. <a class="reference internal" href="#www-cubrid" id="id291">[250]</a> Importantly most of the SQL
syntax from MYSQL and ORACLE can work on cubrid.CUBRID also
provides manager tool for database administration and migration
tool for migrating the data from DBMS to CUBRID bridging the dbs.
CUBRID enterprise version and all the tools are free and suitable
database candidate for web-application development.</p>
</li>
<li><p class="first">Galera Cluster</p>
<p>Galera cluster <a class="reference internal" href="#www-galera-cluster" id="id292">[251]</a> is a type of database
clustering which has all multiple masters and works on
synchronous replication. At a deeper level, it was created by
extending MySql replication API to provide all support for true
multi master synchronous replication.  This extended api is
called as Write-Set Replication API and is the core of the
clustering logic.  Each transaction of wsrep API not only
contains the record but also other meta-info to requires to
commit each node separately or asynchronously. So though it seems
synchronous logically but works independently on each node.  The
approach is also called virtually synchronous replication. This
helps in directly read-write on a specific node and can lose a
node without handling any complex failover scenarios (zero
downtime).</p>
</li>
<li><p class="first">SciDB</p>
</li>
<li><p class="first">Rasdaman</p>
</li>
<li><p class="first">Apache Derby</p>
<p><a class="reference internal" href="#www-apachederby" id="id293">[252]</a>: Apache Derby is java based relational
database system. Apache Derby has JDBC driver which can be used
by Java based applications. Apache derby is part of the Apache DB
subproject and licensed under Apache version 2.0.</p>
<p><a class="reference internal" href="#www-apachederbycharter" id="id294">[253]</a>: Derby Embedded Database Engine is
the database engine with JDBC and SQL as programming APIs.
Client/Server functionality is achieved by Derby network server,
it allows connection through TCP/IP using DRDA protocol. ij,
database utility makes it possible for SQL scripts to be run on
JDBC database. The dblook utility is the schema extraction
tool. The sysinfo utility is used for displaying version of Java
environment and Derby.</p>
<p>There are two deployement options for Apache Derby , embedded and
Derby network server option. In embedded framework, Derby is
started and stopped by the single user java application without
any adiministration required. In the case of Derby network server
configuration, Derby is started by multi user java application
over TCP/IP. Since Apache Derby is written in Java, it runs on
any certified JVM(Java Virtual Machine). <a class="reference internal" href="#www-derbymanual" id="id295">[254]</a>:</p>
</li>
<li><p class="first">Pivotal Greenplum</p>
</li>
<li><p class="first">Google Cloud SQL</p>
</li>
<li><p class="first">Azure SQL</p>
</li>
<li><p class="first">Amazon RDS</p>
<p>According to Amazon Web Services, Amazon Relation Database
Service (Amazon RDS) is a web service which makes it easy to
setup, operate and scale relational databases in the cloud. As
mentioned in <a class="reference internal" href="#amazonrds" id="id296">[255]</a> It allows to create and use
MySQL, Oracle, SQL Server, and PostgreSQL databases in the
cloud. Thus, codes, applications and tools used with existing
databases can be used with Amazon RDS. The basic components of
Amazon(As listed in <a class="reference internal" href="#amazonrdscomponents" id="id297">[256]</a>) RDS include: DB
Instances: DB instance is an isolated database environment in the
cloud. Regions and availability zones: Region is a data center
location which contains Availability Zones. Availability Zone is
isolated from failures in other Availability Zones. Security
groups: controls access to DB instance by allowing access to IP
address ranges or Amazon EC2 instances that is specified. DB
parameter groups: manage configuration of DB engine by specifying
engine configuration values that are applied to one or more DB
instances of the same instance type. DB option groups: Simplifies
data management through Oracle Application Express (APEX), SQL
Server Transparent Data Encryption, and MySQL memcached support.</p>
</li>
<li><p class="first">Google F1</p>
</li>
<li><p class="first">IBM dashDB</p>
<p>IBM dashDB is a data warehousing service hosted in cloud ,
This aims at integrating the data from various sources into a
cloud data base. Since the data base is hosted in cloud it
would have the benifits of a cloud like scalability and less
maintainance. This data base can be configured as &#8216;transaction
based&#8217; or &#8216;Analytics based&#8217; depending on the work load
<a class="reference internal" href="#www-ibm-dash-db-com" id="id298">[257]</a> .This is available through ibm blue mix
cloud platform.</p>
<p>dash DB has build in analytics based on IBM Netezza Analytics
in the PureData System for Analytics. Because of the build in
analytics and support of
in memory optimization promises better performance efficieny.
This can be run alone as a standalone or can be connected to
variousBI or analytic tools. <a class="reference internal" href="#www-ibm-analytics-com" id="id299">[258]</a></p>
</li>
<li><p class="first">N1QL</p>
</li>
<li><p class="first">BlinkDB</p>
</li>
<li><p class="first">Spark SQL</p>
</li>
</ol>
</div>
<div class="section" id="nosql">
<h2>NoSQL<a class="headerlink" href="#nosql" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="218">
<li><p class="first">Lucene</p>
<p>Apache Lucene <a class="reference internal" href="#www-lucene" id="id300">[259]</a> is a high-performance,
full-featured text search engine library.  It is originally
written in pure Java but also has been ported to few other
languages chiefly python.  It is suitable for applications that
requires full-text search.  One of the key implementation of
Lucene is Internet search engines and local, single-site
searching.  Another important implementation usage is its
recomendation system. The core idea of Lucene is to extract text
from any document that contains text (not image) field, making it
format idependent.</p>
</li>
<li><p class="first">Solr</p>
</li>
<li><p class="first">Solandra</p>
<p>Solandra is a highly scalable real-time search engine built on
Apache Solr and Apache Cassandra. Solandra simplifies maintaining
a large scale search engine, something that more and more
applications need. At its core, Solandra is a tight integration
of Solr and Cassandra, meaning within a single JVM both Solr and
Cassandra are running, and documents are stored and disributed
using Cassandra&#8217;s data model. <a class="reference internal" href="#www-solandra" id="id301">[260]</a></p>
<p>Solandra supports most out-of-the-box Solr functionality (search,
faceting, highlights), multi-master (read/write to any node). It
features replication, sharding, caching, and compaction managed
by Cassandra. <a class="reference internal" href="#www-solandra2" id="id302">[261]</a></p>
</li>
<li><p class="first">Voldemort</p>
<p>According to <a class="reference internal" href="#www-voldemort" id="id303">[262]</a>, project Voldemort, developed
by LinkedIn, is a non-relational database of key-value type that
supports eventual consistency. The distributed nature of the
system allows pluggable data placement and provides horizontal
scalability and high consistency. Replication and partitioning of
data is automatic and performed on multiple servers. Independent
nodes that comprise the server support transparent handling of
server failure and ensure absence of a central point of
failure. Essentially, Voldemort is a hashtable. It uses APIs for
data replication. In memory caching allows for faster
operations. It allows cluster expansion with no data rebalancing.
When Voldemort performance was benchmarked with the other
key-value databases such as Cassandra, Redis and HBase as well as
MySQL relational database <a class="reference internal" href="#rabl-sadoghi-jacobsen-2012" id="id304">[263]</a>, the
Voldemart&#8217;s throughput was twice lower than MySQL and Cassandra
and six times higher than HBase. Voldemort was slightly
underperforming in comparison with Redis. At the same time, it
demonstrated consistent linear performance in maximum throughput
that supports high scalability. The read latency for Voldemort
was fairly consistent and only slightly underperformed
Redis. Similar tendency was observed with the read latency that
puts Voldermort in the cluster of databases that require good
read-write speed for workload operations. However, the same
authors noted that Voldemort required creation of the node
specific configuration and optimization in order to successfully
run a high throughput tests. The default options were not
sufficient and were quickly saturated that stall the database.</p>
</li>
<li><p class="first">Riak</p>
<p>Riak is a set of scalable distributed NoSQL databases developed by
Basho Technologies. Riak KV is a key-value <a class="reference internal" href="#www-riak-kv" id="id305">[264]</a> database
with time-to-live feature so that older data is deleted automatically.
It can be queried through secondary indexes, search via Apache Solr,
and MapReduce. Riak TS is designed for time-series data. It co-
locates related data on the same physical cluster for faster access
<a class="reference internal" href="#www-riak-ts" id="id306">[265]</a>. Riak S2 is designed to store large objects like media
files and software binaries <a class="reference internal" href="#www-riak-s2" id="id307">[266]</a>. The databases are available
in both open source and commercial versions with multicluster
replication provided only in later. REST APIs are available for these
databases.</p>
</li>
<li><p class="first">ZHT</p>
<p>According to <a class="reference internal" href="#datasys" id="id308">[267]</a>, ZHT is a zero-hop distributed hash
table. Distributed hash tables effectively break a hash table up
and assign different nodes responsibility for managing different
pieces of the larger hash table. <a class="reference internal" href="#wiley" id="id309">[268]</a> To retrieve a value in a
distributed hash table, one needs to find the node that is
responsible for the managing the key value pair of
interest. <a class="reference internal" href="#wiley" id="id310">[268]</a> In general, every node that is a part of the
distributed hash table has a reference to the closest two nodes
in the node list. <a class="reference internal" href="#wiley" id="id311">[268]</a> In a ZHT, however, every node contains
information concerning the location of every other node. <a class="reference internal" href="#li" id="id312">[269]</a>
Through this approach, ZHT aims to provide high availability,
good fault tolerance, high throughput, and low latencies, at
extreme scales of millions of nodes. <a class="reference internal" href="#li" id="id313">[269]</a> Some of the defining
characteristics of ZHT are that it is light-weight, allows nodes
to join and leave dynamically, and utilizes replication to obtain
fault tolerance among others. <a class="reference internal" href="#li" id="id314">[269]</a></p>
</li>
<li><p class="first">Berkeley DB</p>
<p>Berkeley DB is a family of open source, NoSQL key-value database libraries.
<a class="reference internal" href="#www-bdb-wiki" id="id315">[270]</a> It provides a simple function-call API for data access
and management over a number of programming languages, including C, C++,
Java, Perl, Tcl, Python, and PHP. Berkeley DB is embedded because it links
directly into the application and runs in the same address space as the
application. <a class="reference internal" href="#www-bdb-stanford" id="id316">[271]</a> As a result, no inter-process
communication, either over the network or between processes on the same
machine, is required for database operations. It is also extremely portable
and scalable, it can manage databases up to 256 terabytes in size.</p>
<p><a class="reference internal" href="#www-bdb" id="id317">[272]</a> For data management, Berkeley DB offers advanced services,
such as concurrency for many users, ACID transactions, and recovery.</p>
<p>Berkeley DB is used in a wide variety of products and a large number of
projects, including gateways from Cisco, Web applications at Amazon.com
and open-source projects such as Apache and Linux.</p>
</li>
<li><p class="first">Kyoto/Tokyo Cabinet</p>
<p>Tokyo Cabinet <a class="reference internal" href="#www-tokyo-cabinet" id="id318">[273]</a> and Kyoto Cabinet
<a class="reference internal" href="#www-kyoto-cabinet" id="id319">[274]</a> are libraries of routines for managing a
database. The database normally is a simple data file containing
records having a key value pair structure. Every key and value is
serial bytes with variable length. Both binary data and character
string can be used as a key and a value. There is no concept of
data tables nor data types like RDBMS or DBMS. Records are
organized in hash table, B+ tree, or fixed-length array.Tokyo and
Kyoto cabinets both are developed as a successor of GDBM and QDBM
which are library routines for managing database as well. Tokyo
Cabinet is written in the C language, and is provided as API of
C, Perl, Ruby, Java, and Lua. Tokyo Cabinet is available on
platforms which have API conforming to C99 and POSIX. Whereas
Kyoto Cabinet is written in the C++ language, and is provided as
API of C++, C, Java, Python, Ruby, Perl, and Lua. Kyoto Cabinet
is available on platforms which have API conforming to C++03 with
the TR1 library extensions. Both are free software licenced under
GNU (General Public Licence). <a class="reference internal" href="#www-tokyo-cabinet" id="id320">[273]</a> actually mentions
that Kyoto Cabinet is more powerful and has convenient library
structure than Tokyo and recommends people to use Kyoto. Since
they use key-value pair concept, you can store a record with a
key and a value, delete a record using the key and even retrive a
record using the key. Both have smaller size of database file,
faster processing speed and provide effective backup procedures.</p>
</li>
<li><p class="first">Tycoon</p>
<p>Tycoon/ Kyoto Tycoon <a class="reference internal" href="#tycoon-fl" id="id321">[275]</a> is a lightweight database
server developed by FLL labs and is a distributed Key-value store
<a class="reference internal" href="#tycoon-cf" id="id322">[276]</a>. It is very useful in handling cache data
persistent data of various applications. Kyoto Tycoon is also a
package of network interface to the DBM called Kyoto Cabinet
<a class="reference internal" href="#tycoon-fl2" id="id323">[277]</a> which contains a library of routines for
managing a database. Tycoon is composed of a sever process that
manger multiple databases. This renders high concurrency enabling
it to handle more than 10 thousand connections at the same time.</p>
</li>
<li><p class="first">Tyrant</p>
<p>Tyrant provides network interfaces to the database management
system called Tokyo Cabinet. Tyrant is also called as Tokyo
Tyrant. Tyrant is implemented in C and it provides APIs for Perl,
Ruby and C. Tyrant provides high performance and concurrent
access to Tokyo Cabinet. The blog <a class="reference internal" href="#www-tyrant-blog" id="id324">[278]</a>
explains the results of performance experiments between Tyrant and
Memcached + MySQL.</p>
<p>Tyrant was written and maintained by FAL Labs
<a class="reference internal" href="#www-tyrant-fal-labs" id="id325">[279]</a>.  However, according to FAL Labs,
their latest product <a class="reference internal" href="#www-kyoto-tycoon" id="id326">[280]</a> Kyoto Tycoon is
more powerful and convenient server than Tokyo Tyrant.</p>
</li>
<li><p class="first">MongoDB</p>
<p>MongoDB is a NoSQL database which uses collections and documents
to store data as opposed to the relational database where data is
stored in tables and rows. In MongoDB a collection is a container
for documents, whereas a document contains key-value pairs for storing
data. As MongoDB is a NoSQL database, it supports dynamic schema design
allowing documents to have different fields. The database uses a document
storage and data interchange format called BSON, which provides a binary
representation of JSON-like documents.</p>
<p>MongoDB provides high data availability by way of replication and
sharding. High cost involved in data replication can be reduced by
horizontal data
scaling by way of shards where data is scattered across multiple
servers. It reduces query cost as the query load is distributed
across servers. This means that both read and write performance
can be increased by adding more shards to a cluster. Which document
resides on which shard is determined by the shard key of each collection.</p>
<p>As far as data backup and restore is concerned the default MongoDB
storage engines natively support backup of complete data. For incremental
backups one can use MongoRocks that is a third party tool developed by Facebook.</p>
</li>
<li><p class="first">Espresso</p>
</li>
<li><p class="first">CouchDB</p>
</li>
<li><p class="first">Couchbase</p>
<p>Couchbase, Inc. offers Couchbase Server (CBS) to the marketplace
as a NoSQL, document-oriented database alternative to traditional
relationship- oriented database managgement systems as well as
other NoSQL competitors.  The basic storage unit, a <em>document</em>,
is a &#8220;data structure defined as a collection of named fields&#8221;.
The document utilizes JSON, thereby allowing each document to
have its own individual schema. <a class="reference internal" href="#www-infoworld-cbs" id="id327">[281]</a></p>
<p>CBS combines the in-memory capabilities of Membase with CouchDB&#8217;s
inherent data store reliability and data persistency.  Membase
functions in RAM only, providing the highest-possible speed
capabilities to end users.  However, Membase&#8217;s in-ram existence
limits the amount of data it can use.  More importantly, it
provides no mechanism for data recovery if the server crashes.
Combining Membase with CouchDB provides a persistent data source,
mitigating the disadvantages of either product.  In addition,
CouchDB + membase allows the data size &#8220;to grow beyond the size
of RAM&#8221;.  <a class="reference internal" href="#www-safaribooks-cbs" id="id328">[282]</a></p>
<p>CBS is written in Erlang/OTP, but generally shortened to just
Erlang.  In actuality, t is written in &#8220;Erlang using components
of OTP alongside some C/C++&#8221;<a class="reference internal" href="#www-erlangcentral-cbs" id="id329">[283]</a>, It
runs on an Erlang virtual machine known as
BEAM. <a class="reference internal" href="#www-wikipedia-erlang-cbs" id="id330">[284]</a></p>
<p>Out-of-the-box benefits of Erlang/OTP include dynamic type
setting, pattern matching and, most importantly, actor-model
concurrency.  As a result, Erlang code virtually eliminates the
possibility of inadvertent deadlock scenarios.  In addition,
Erlang/OTP processes are lightweight, spawning new processes does
not consume many resources and message passing between processes
is fast since they run in the same memory space.  Finally, OTP&#8217;s
process supervision tree makes Erlang/OTP extremely
fault-tolerant.  Error handling is indistinguishable from a
process startup, easing testing and bug detection.
[www-couchbase-blog-cbs]</p>
<p>CouchDB&#8217;s design adds another layer of reliability to CBS.
CouchDB operates in <em>append-only</em> mode, so it adds user changes
to the tail of database.  This setup resists data corruption
while taking a snapshot, even if the server continues to run
during the procedure.  <a class="reference internal" href="#www-hightower-cbs" id="id332">[285]</a></p>
<p>Finally, CB uses the Apache 2.0 License, one of several
open-source license alternatives. <a class="reference internal" href="#www-quora-cbs" id="id333">[286]</a></p>
</li>
<li><p class="first">IBM Cloudant</p>
<p>Cloudant is based on both Apache-backed CouchDB project and the
open source BigCouch project. IBM Cloudant is an open source
non-relational, distributed database service as service (DBaaS)
that provides integrated data management, search and analytics
engine designed for web applications. Cloudant&#8217;s distributed
service is used the same way as standalone CouchDB, with the
added advantage of data being redundantly distributed over
multiple machines <a class="reference internal" href="#www-ibm-cloudant" id="id334">[287]</a>.</p>
</li>
<li><p class="first">Pivotal Gemfire <a class="reference internal" href="#www-gemfire" id="id335">[288]</a></p>
<p>A real-time, consistent access to data-intensive applications is
provided by a open source, data management platform named Pivotal
Gemfire. &#8220;GemFire pools memory, CPU, network resources, and
optionally local disk across multiple processes to manage
application objects and behavior&#8221;. The main features of Gemfire
are high scalability, continuous availability, shared nothing
disk persistence, heterogeneous data sharing and parallelized
application behavior on data stores to name a few.  In Gemfire,
clients can subscribe to receive notifications to execute their
task based on a specific change in data. This is achieved through
the continuous querying feature which enables event-driven
architecture. The shared nothing architecture of Gemfire suggests
that each node is self-sufficient and independent, which means
that if the disk or caches in one node fail the remaining nodes
remaining untouched. Additionally, the support for multi-site
configurations enable the user to scale horizontally between
different distributed systems spread over a wide geographical
network.</p>
</li>
<li><p class="first">HBase</p>
<p>Apache Hbase is a distributed column-oriented database
which is built on top of HDFS (Hadoop Distributed File
System).According to <a class="reference internal" href="#www-hbase" id="id336">[289]</a>, It is a open source,
versioned, distributed, non-relational database modelled after
Googles Bigtable. Similar to Bigtable providing harnessing
distributed file storage system offered by Google file system,
Apache Hbase provides similar capabilities on top of Hadoop and
HDFS. Moreover, Hbase supports random, real-time CRUD
(Create/Read/Update/Delete) operations.</p>
<p>Hbase is a type of NoSQL database and is classified as a key value
store.In HBase, value is identied with a key where both of them are
stored as byte arrays. Values are stored in the order of keys. HBase
is a database system where the tables have no schema. Some of the
companies that use HBase as their core program are Facebook, Twitter,
Adobe, Netflix etc.</p>
</li>
<li><p class="first">Google Bigtable</p>
<p>Google Bigtable is a NoSQL database service, built upon several Google
technologies, including Google File System, Chubby Lock Service, and
SSTable <a class="reference internal" href="#www-cloudbigtable" id="id337">[290]</a>.  Designed for Big Data, Bigtable
provides high performance and low latency and scales to hundreds of
petabytes <a class="reference internal" href="#www-cloudbigtable" id="id338">[290]</a>. Bigtable powers many core
Google products, such as Search, Analytics, Maps, Earth, Gmail,
and YouTube. Bigtable also drives Google Cloud Datastore and
Spanner, a distributed NewSQL database also developed by
Google <a class="reference internal" href="#www-wikispanner" id="id339">[291]</a> <a class="reference internal" href="#www-wikibigtable" id="id340">[292]</a>.
Since May 6, 2015, a version of Bigtable has been available to the
public <a class="reference internal" href="#www-wikibigtable" id="id341">[292]</a>.</p>
</li>
<li><p class="first">LevelDB</p>
</li>
<li><p class="first">Megastore and Spanner</p>
<p>Spanner <a class="reference internal" href="#corbett-spanner" id="id342">[293]</a> is Google&#8217;s distributed database
which is used for managing all google services like play, gmail,
photos, picasa, app engine etc Spanner is distributed database
which spans across multiple clusters, datacenters and geo
locations.  Spanner is structured in such a way so as to provide
non blocking reads, lock free transactions and atomic schema
modification. This is unlike other noSql databases which follow
the CAP theory i.e. you can choose any two of the three:
Consistency, Availability and Partition-tolerance. However,
spanner gives an edge by satisfying all three of these. It gives
you atomicity and consistency along with availability, partition
tolerance and synchronized replication.  Megastore bridges the
gaps found in google&#8217;s bigtable. As google realized that it is
difficult to use bigtable where the application requires
constantly changing schema. Megastore offers a solution in terms
of semi-relational data model.  Megastore
<a class="reference internal" href="#www-magastore-spanner" id="id343">[294]</a> also provides a transactional
database which can scale unlike relational data stores and
synchronous replication.  Replication in megastore is supported
using Paxos. Megastore also provides versioning. However,
megastore has a poor write performance and lack of a SQL like
query language. Spanners basically adds what was missing in
Bigtable and megastore. As a global distributed database spanner
provides replication and globally consistent reads and
writes. Spanner deployment is called universe which is a
collections of zones. These zones are managed by singleton
universe master and placement driver. Replication in spanner is
supported by Paxos state machine. Spanner was put into evaluation
in early 2011 as F1 backend(F1 is Google&#8217;s advertisement system)
which was replacement to mysql. Overall spanner fulfils the needs
of relational database along with scaling of noSQL database.  All
these features make google run all their apps seamlessly on
spanner infrastructure.</p>
</li>
<li><p class="first">Accumulo</p>
</li>
<li><p class="first">Cassandra</p>
<p>Apache Cassandra <a class="reference internal" href="#www-cassandra" id="id344">[295]</a> is an open-source
distributed database managemment for handling large volume of
data accross comodity servers. It works on asynchronous
masterless replication technique leading to low latency and high
availability. It is a hybrid between a key-value and column
oriented database. A table in cassandra can be viewed as a multi
dimensional map indexed by a key. It has its own &#8220;Cassandra Query
language (CQL)&#8221; query language for data extraction and
mining. One of the demerits of such structure is it does not
support joins or subqueries. It is a java based system which can
be administered by any JMX compliant tools.</p>
</li>
<li><p class="first">RYA</p>
<p>Rya is a scalable system for storing and retrieving RDF data in
a cluster of nodes. <a class="reference internal" href="#punnoose" id="id345">[296]</a> RDF stands for Resource
Description Framework. <a class="reference internal" href="#punnoose" id="id346">[296]</a> RDF is a model that facilitates
the exchange of data on a network. <a class="reference internal" href="#w3" id="id347">[297]</a> RDF utilizes a form
commonly referred to as a triple, an object that consists of a
subject, predicate, and object. <a class="reference internal" href="#punnoose" id="id348">[296]</a> These triples are used
to describe resources on the Internet. <a class="reference internal" href="#punnoose" id="id349">[296]</a> Through new
storage and querying techniques, Rya aims to make accessing RDF
data fast and easy. <a class="reference internal" href="#apacherya" id="id350">[298]</a></p>
</li>
<li><p class="first">Sqrrl</p>
</li>
<li><p class="first">Neo4J</p>
</li>
<li><p class="first">graphdb</p>
<p>A Graph Database is a database that uses graph structures for semantic
queries with nodes, edges and properties to represent and store data.
<a class="reference internal" href="#www-graphdb" id="id351">[299]</a>
The Graph is a concept which directly relates the data items in the store.
The data which is present in the store is linked together directly with the
help of relationships. It can be retrieved with a single operation.
Graph database allow simple and rapid retrieval of complex hierarchical
structures that are difficult to model in relational systems.</p>
<p>There are different underlying storage mechanisms used by graph databases.
Some graphdb depend on a relational engine and store the graph data in a
table, while others use a key-value store or document-oriented database for
storage. Thus, they are inherently caled as NoSQL structures.
Data retrieval in a graph database requires a different query language
other than SQL. Some of the query languages used to retrieve data from a
graph database are Gremlin, SPARQL, and Cypher.
Graph databases are based on graph theory. They employ the concepts of
nodes, edges and properties.</p>
</li>
<li><p class="first">Yarcdata</p>
</li>
<li><p class="first">AllegroGraph</p>
<p>AllegroGraph is a database technology that enables businesses to
extract sophisticated decision insights and predictive analytics from
their&nbsp;highly complex, distributed data&nbsp;that cant be answered with
conventional databases, i.e., it turns complex data into actionable
business insights. <a class="reference internal" href="#www-allegro" id="id352">[300]</a>
It can be viewed as a closed source database that is used for storage
and retrieval of data in the form of triples (triple is a data entity
composed of subject-predicate-object like Professor teaches students).
Information in a triplestore is retrieved using a query language. Query
languages can be classified into database query languages or information
retrieval query languages. The difference is that a database query language
gives exact answers to exact questions, while an information retrieval
query language finds documents containing requested information.
Triple format&nbsp;represents information&nbsp;in a machine-readable format.
Every part of the triple is individually addressable via unique&nbsp;URLs&nbsp;
for example, the statement Professor teaches students might be
represented in RDF(Resource Description Framework&nbsp;) as
<a class="reference external" href="http://example.name#Professor12">http://example.name#Professor12</a> <a class="reference external" href="http://xmlns.com/foaf/0.1/teacheshttp">http://xmlns.com/foaf/0.1/teacheshttp</a>:
//example.name#students. Using this representation, semantic data can
be queried.  <a class="reference internal" href="#www-allegrow" id="id353">[301]</a></p>
</li>
<li><p class="first">Blazegraph</p>
</li>
<li><p class="first">Facebook Tao</p>
<p>In the paper published in USENIX annual technical conference,
Facebook Inc describes TAO (The Association and Objects) as
:cite book-tao a geographically distributed data store that
provides timely access to the social graph for Facebooks demanding
workload using a fixed set of queries. It is deployed at Facebook for
many data types that fit its model. The system runs on thousands of
machines, is widely distributed, and provides access to many petabytes
of data. TAO represents social data items as Objects (user) and
relationship between them as Associations (liked by, friend of).
TAO cleanly separates the caching tiers from the persistent data
store allowing each of them to be scaled independently. To any user
of the system it presents a single unified API that makes the entire
system appear like 1 giant graph database. :cite:&#8217;www-tao&#8217;.</p>
</li>
<li><p class="first">Titan:db</p>
<p>Titan:db <a class="reference internal" href="#www-titan" id="id354">[302]</a> is a distributed graph database that
can support of thousands of concurrent users interacting with a
single massive graph database that is distributed over the
clusters. It is open source with liberal Apache 2 license.
Its main components are storage backend, search backend, and
TinkerPop graph stack. Titan provides support for various
storage backends and also linear scalability for a growing data
and user base. It inherits features such as Gremlin query
language  and Rexter graph server from TinkerPop <a class="reference internal" href="#www-tinkerpop" id="id355">[303]</a>.
For huge graphs, Titan uses a component called Titan-hadoop which
compiles Gremlin queries to Hadoop MapReduce jobs and runs them
on the clusters. Titan is basically optimal for smaller graphs.</p>
</li>
<li><p class="first">Jena</p>
<p>Jena is an open source Java Framework provided by Apache for
semantic web applications. (<a class="reference internal" href="#jena-wiki" id="id356">[304]</a>) It provides a
programmatic environment for RDF, RDFS and OWL, SPARQL, GRDDL,
and includes a rule-based inference engine. Semantic web data
differs from conventional web applications in that it supports a
web of data instead of the classic web of documents format. The
presence of a rule based inference engine enable Jena to perform
a reasoning based on OWL and RDFS ontologies.  <a class="reference internal" href="#jena-blog" id="id357">[305]</a>
The architecture of Jena contains three layers : Graph layer,
model layer and Ontology layer. The graph layer forms the base
for the architecture. It does not have an extensive RDF
implementation and serves more as a Service provider
Interface. According to <a class="reference internal" href="#jena-blog" id="id358">[305]</a> It provides
classes/methods that could be further extended. The model layer
extends the graph layer and provides objects of type resource
instead of node to work with.  The ontology layer enables one
to work with triples.</p>
</li>
<li><p class="first">Sesame</p>
<p>Sesame is framework which can be used for the analysis of RDF
(Resource Description Framework) data.  Resource Description
Framework (RDF) <a class="reference internal" href="#www-rdf" id="id359">[306]</a> is a model that facilitates the
interchange of data on the Web.  Using RFD enables us to merge
data even if the underlying schemas differ.  <a class="reference internal" href="#www-sesame" id="id360">[307]</a>
Sesame has now officially been integrated into RDF4J Eclipse
project.  Sesame takes in the natively written code as the input
and then performs a series of transformations, generating kernels
for various platforms.  <a class="reference internal" href="#sesame-paper-2013" id="id361">[308]</a> In order to
achieve this, it makes use of the feature identifier, impact
predictor, source-to-source translator and the auto-tuner.  The
feature identifier is concerned with the extraction and detection
of the architectural features that are important for application
performance.  The impact predictor determines the performance
impact of the core features extracted above.  A source-to-source
translator transforms the input code into a parametrized one;
while the auto-tuner helps find the optimal solution for the
processor.</p>
</li>
<li><p class="first">Public Cloud: Azure Table</p>
<p>Microsoft offers its NoSQL Azure Table product to the market as a
low-cost, fast and scalable data storage
option. <a class="reference internal" href="#www-what-to-use" id="id362">[309]</a> Table stores data as collections
of key-value combinations, which it terms <em>properties</em>.  Table
refers to a collection of properties as an <em>entity</em>.  Each entity
can contain a mix of properties.  The mix of properties can vary
between each entity, although each entity may consist of no more
than 255 properties. <a class="reference internal" href="#www-blobqueuetable" id="id363">[310]</a></p>
<p>Although data in Azure Table will be structured via key-value
pairs, Table provides just one mechanism for the user to define
relationships between entities: the entity&#8217;s <em>primary key</em>.  The
primary key, which Microsoft sometimes calls a <em>clustered index</em>,
consists of a PartitionKey and a RowKey.  The PartitionKey
indicates the group, a.k.a partition, to which the user assigned
the entity.  The RowKey indicates the entity&#8217;s relative position
in the group.  Table sorts in ascending order by the PartitionKey
first, then by the RowKey using lexical comparisons.  As a
result, numeric sorting requires fixed-length, zero-padded
strings.  For instance, Table sorts <em>111</em> before <em>2</em>, but will
sort <em>111</em> after <em>002</em>. <a class="reference internal" href="#www-scalable-partitioning" id="id364">[311]</a></p>
<p>Azure Table is considered best-suited for infrequently accessed
data storage.</p>
</li>
<li><p class="first">Amazon Dynamo</p>
<p>Amazon explains DynamoDB as :cite:&#8217;www.dyndb&#8217; a fast and flexible
NoSQL database service for all applications that need consistent,
single-digit millisecond latency at any scale. It is a fully managed
cloud database and supports both document and key-value store models.
Its flexible data model and reliable performance make it a great fit
for mobile, web, gaming, ad tech, IoT, and many other applications.
DynamoDB can be easily integrated with big-data processing tools like
Hadoop. It can also be integrated with AWS Lambda, an event driven platform,
which enables creating applications that can automatically react to data
changes. At present there are certain limits to DynamoDB. Amazon has listed
all the limits in a web page titled <a class="reference external" href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html">Limits in DynamoDB</a></p>
</li>
</ol>
<p></p>
<ol class="arabic simple" start="253">
<li>Google DataStore</li>
</ol>
</div>
<div class="section" id="file-management">
<h2>File management<a class="headerlink" href="#file-management" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="254">
<li><p class="first">iRODS</p>
<p>The Integrated Rule-Oriented Data System (iRODS) is open source
data management software. iRODS is released as a production-level
distribution aimed at deployment in mission critical
environments. It virtualizes data storage resources, so users can
take control of their data, regardless of where and on what
device the data is stored. The development infrastructure
supports exhaustive testing on supported platforms. The plugin
architecture supports microservices, storage systems,
authentication, networking, databases, rule engines, and an
extensible API <a class="reference internal" href="#irods-www" id="id365">[312]</a>.  iRODS implements data
virtualization, allowing access to distributed storage assets
under a unified namespace, and freeing organizations from getting
locked in to single-vendor storage solutions. iRODS enables data
discovery using a metadata catalog that describes every file,
every directory, and every storage resource in the iRODS
Zone. iRODS automates data workflows, with a rule engine that
permits any action to be initiated by any trigger on any server
or client in the Zone. iRODS enables secure collaboration, so
users only need to log in to their home Zone to access data
hosted on a remote Zone. <a class="reference internal" href="#github-irods-www" id="id366">[313]</a></p>
</li>
<li><p class="first">NetCDF</p>
<p>NetCDF is a set of software libraries and self-describing, machine-indepen
dent data formats that support the creation, access, and sharing of array
oriented scientific data. NetCDF was developed and is maintained at Unidata
, part of the University Corporation for Atmospheric Research (UCAR) Commun
ity Programs (UCP). Unidata is funded primarily by the National Science F
oundation <a class="reference internal" href="#paper-netcdf" id="id367">[314]</a> <a class="reference internal" href="#www-netcdf" id="id368">[315]</a> . The purpose of the Netwo
rk Common Data Form(netCDF) interface is to support the creation, efficient
access, and sharing of data in a form that is self-describing, portable, co
mpact, extendible, and archivable Version 3 of netCDF is widely used in
atmospheric and ocean sciences due to its simplicity. NetCDF version 4 has
been designed to address limitations of netCDF version 3 while preserving
useful forms of compatibility with existing application software and data
archives <a class="reference internal" href="#paper-netcdf" id="id369">[314]</a>.
NetCDF consists of: a) A conceptual data model b) A set of binary data
formats c) A set of APIs for C/Fortran/Java</p>
</li>
<li><p class="first">CDF</p>
<p>Common Data Format <a class="reference internal" href="#www-cdf" id="id370">[316]</a> is a conceptual data
abstraction for storing, manipulating, and accessing
multidimensional data sets. CDF differs from traditional physical
file formats by defining form and function as opposed to a
specification of the bits and bytes in an actual physical format.</p>
<p>CDF&#8217;s integrated dataset is composed by following two categories
:(a)Data Objects - scalars, vectors, and n-dimensional
arrays.(b)Metadata - set of attributes describing the CDF in
global terms or specifically for a single variable
<a class="reference internal" href="#user-guide-cdf" id="id371">[317]</a>.</p>
<p>The self-describing property (metadata) allows CDF to be a
generic, data-independent format that can store data from a wide
variety of disciplines. Hence, the application developer remains
insulated from the actual physical file format for reasons of
conceptual simplicity, device independence, and future
expandability.CDF data sets are portable on any of the
CDF-supported platforms and accessible with CDF applications or
layered tools. To ensure the data integrity in a CDF file,
checksum method using MD5 algorithm is employed
<a class="reference internal" href="#www-digitalpreserve" id="id372">[318]</a>.</p>
<p>Compared to HDF format <a class="reference internal" href="#www-wiki-hdf" id="id373">[319]</a>, CDF permitted
cross-linking data from different instruments and spacecraft in
ISTP with one development effort. CDF is widely supported by
commercial and open source data analysis/visualization software
such as IDL, MATLAB, and IBMs Data Explorer (XP).</p>
</li>
<li><p class="first">HDF</p>
</li>
<li><p class="first">OPeNDAP</p>
</li>
<li><p class="first">FITS</p>
<p>FITS stand for &#8216;Flexible Image Trasnport System&#8217;. It is a
standard data format used in astronomy. FITS data format is
endorsed by NASA and International Astronomical Union. According
to <a class="reference internal" href="#www-fits-nasa" id="id374">[320]</a>, FITS can be used for transport,
analysis and archival storage of scientific datasets and support
multi-dimensional arrays, tables and headers sections.  FITS is
actively used and developed - according to
<a class="reference internal" href="#www-news-fits-2016" id="id375">[321]</a> newer version of FITS standard
document was released in July 2016. FITS can be used for
digitization of contents like books and
magzines. Vatican Library <a class="reference internal" href="#www-fits-vatican-library" id="id376">[322]</a> used FITS
for long term preservation of their book, manuscripts and other
collection. Matlab, a language used for technical computing
supports fits <a class="reference internal" href="#www-fits-matlab" id="id377">[323]</a>. The 2011 paper
<a class="reference internal" href="#paper-fits-2011" id="id378">[324]</a> explains how to perform
processing of astronomical images on Hadoop using FITS.</p>
</li>
<li><p class="first">RCFile</p>
<p>RCFile (Record Columnar File) <a class="reference internal" href="#www-rcfile" id="id379">[325]</a> is a big
data placement data structure that supports fast data loading and
query processing coupled with efficient storage space utilization
and adaptive to dynamic workload environments. It is designed for
data warehousing systems that uses map-reduce. The data is stored
as a flat file comprising of binary key/value pairs. The rows are
partitioned first and then the columns are partitioned in each
row and the respective meta-data for each row is stored in the
key part for that row and the values comprises of the data part
of the row. Storing the data in this format enables RCFile to
accomplish fast loading and query processing.A shell utility is
available for reading RCFile data and metadata
<a class="reference internal" href="#www-rcfile" id="id380">[325]</a>. According to <a class="reference internal" href="#he2011rcfile" id="id381">[326]</a>, RCFile has
been chosen in Facebook data warehouse system as the default
option. It has also been adopted by Hive and Pig, the two most
widely used data analysis systems developed in Facebook and
Yahoo!</p>
</li>
<li><p class="first">ORC</p>
<p>ORC files were created as part of the initiative to massively
speed up Apache Hive and improve the storage efficiency of data
stored in Apache Hadoop. ORC is a self-describing type-aware
columnar file format designed for Hadoop workloads. It is
optimized for large streaming reads, but with integrated support
for finding required rows quickly. Storing data in a columnar
format lets the reader read, decompress, and process only the
values that are required for the current query. Because ORC files
are type-aware, the writer chooses the most appropriate encoding
for the type and builds an internal index as the file is
written.ORC files are divided in to stripes that are roughly 64MB
by default. The stripes in a file are independent of each other
and form the natural unit of distributed work. Within each
stripe, the columns are separated from each other so the reader
can read just the columns that are required <a class="reference internal" href="#www-orc-docs" id="id382">[327]</a>.</p>
</li>
<li><p class="first">Parquet</p>
<p>Apache parquet is the column Oriented data store for Apache
Hadoop ecosystem and available in any data processing framework,
data model or programming language <a class="reference internal" href="#www-parquet" id="id383">[328]</a>. It
stores data such that the values in each column are physically
stored in contiguous memory locations. As it has the columnar
storage, it provides efficient data compression and encoding
schemes which saves storage space as the queries that fetch
specific column values need not read the entire row data and thus
improving performance.It can be implemented using the Apache
Thrift framework which increases its flexibility to work with a
number of programming languages like C++, Java, Python, PHP, etc.</p>
</li>
</ol>
</div>
<div class="section" id="data-transport">
<h2>Data Transport<a class="headerlink" href="#data-transport" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="263">
<li><p class="first">BitTorrent</p>
<p>Bittorrent is P2P communication protocol commonly used for
sending and receiving the large digital files like movies and
audioclips.In order to upload and download file, user have to
download bittorrent client which implement the bittorrent
protocol. Bittorrent uses the principle of swarning and
tracking. <a class="reference internal" href="#www-bittorrent" id="id384">[329]</a> It divides the files in large
number of chunck and as soon as file is received it can be server
to the other users for downloading.  So rather than downloading
one entire large file from one source, user can download small
chunk from the different sources of linked users in
swarn. Bittorrent trackers keeps list of files available for
transfer and helps the swarn user find each other.</p>
<p>Using the protocol, machine with less configuration can serve as
server for distributing the files. It result in increase in the
downloading speed and reduction in origin server configuration.</p>
<p>Few popular bittorrent client in Torrent, qBittorrent.</p>
</li>
<li><p class="first">HTTP</p>
</li>
<li><p class="first">FTP</p>
<p>According to <a class="reference internal" href="#ftp-wiki" id="id385">[330]</a> FTP is an acronym for File Transfer
Protocol. It is network protocol standard used for transferring
files between two computer systems or between a client and a
server. It is part of the Application layer of the Internet
Protocol Suite and works along with HTTP/SSH. It follows a
client-server model architecture. Secure systems asks the client
to authenticate themselves using a Username and Password
registered with the server to access the files via FTP. The
specification for FTP was first written by Abhay Bhushan
<a class="reference internal" href="#www-rfc114" id="id386">[331]</a> in 1971 and is termed as RFC114. The current
specification, RFC959 in use was written in 1985. Several other
versions of the specification are available which provides
firewall friendly FTP access, additional security extensions,
support for IPV6 and passive mode file access respectively. FTP
can be used in command line in most of the operating systems to
transfer files. There are FTP clients such as WinSCP, FileZilla
etc. which provides a graphical user interface to the clients to
authenticate themselves (sign on) and access the files from the
server.</p>
</li>
<li><p class="first">SSH</p>
<p>SSH is a cryptographic network protocol <a class="reference internal" href="#www-ssh-wiki" id="id387">[332]</a> to
provide a secure channel between two clients over an unsecured
network. It uses public-key cryptography for authenticating the
remote machine and the user. The public-private key pairs could
be generated automatically to encrypt the network connection.
ssh-keygen utility could be used to generate the keys manually.
The public key then could be placed on the all the computers to
which the access is required by the owner of the private key.
SSH runs on the client-server model where a server listens for
incoming ssh connection requests. It&#8217;s generally used for remote
login and command execution. It&#8217;s other important uses include
tunneling(required in cloud computing) and file transfer(SFTP).
OpenSSH is an open source implementation of network utilities
based on SSH <a class="reference internal" href="#www-openssh-wiki" id="id388">[333]</a>.</p>
</li>
<li><p class="first">Globus Online (GridFTP)</p>
<p>GridFTP is a enhancement on the File Tranfer Protocol (FTP) which
provides high-performance , secure and reliable data transfer for
high-bandwidth wide-area networks. As noted in
<a class="reference internal" href="#www-globusonline" id="id389">[334]</a> the most widely used implementation of
GridFTP is Globus Online. GridFTP achieves efficient use of
bandwidth by using multiple simultaneous TCP streams.  Files can
be downloaded in pieces simultaneously from multiple sources; or
even in separate parallel streams from the same source. GridFTP
allows transfers to be restarted automatically and handles
network unavailability with a fault tolerant implementation of
FTP.The underlying TCP connection in FTP has numerous settings
such as window size and buffer size. GridFTP allows automatic (or
manual) negotiation of these settings to provide optimal transfer
speeds and reliability .</p>
</li>
<li><p class="first">Flume</p>
<p>Flume is distributed, reliable and available service for
efficiently collecting, aggregating and moving large amounts of
log data [apche-flume]. Flume was created to allow you to
flow data from a source into your Hadoop environment.  In Flume,
the entities you work with are called sources, decorators, and
sinks. A source can be any data source, and Flume has many
predefined source adapters. A sink is the target of a specific
operation. A decorator is an operation on the stream that can
transform the stream in some manner, which could be to compress
or uncompress data, modify data by adding or removing pieces of
information, and more <a class="reference internal" href="#ibm-flume" id="id391">[335]</a>.</p>
</li>
<li><p class="first">Sqoop</p>
<p>Apache Sqoop is a tool to transfer large amounts of data between Apache Hadoop
and sql databases <a class="reference internal" href="#www-sqoop" id="id392">[336]</a>. The name is a Portmanteau of
SQL + Hadoop. It is a command line interface application which
supports incremental loads of complete tables, free form (custom)
SQL Queries and allows the use of saved and scheduled jobs to import
latest updates made since the last import. The imports can also be
used to populate tables in Hive or Hbase. Sqoop has the option of
export, which allows data to be transferred from Hadoop into a
relational database. Sqoop is supported in many different business
integration suits like Informatica Big Data Management, Pentaho
Data Integration, Microsoft BI Suite and Couchbase <a class="reference internal" href="#sqoop-wiki" id="id393">[337]</a>.</p>
</li>
<li><p class="first">Pivotal GPLOAD/GPFDIST</p>
</li>
</ol>
</div>
<div class="section" id="cluster-resource-management">
<h2>Cluster Resource Management<a class="headerlink" href="#cluster-resource-management" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="271">
<li><p class="first">Mesos</p>
<p>Apache Mesos <a class="reference internal" href="#www-mesos" id="id394">[338]</a> abstracts CPU, memory,
storage, and other compute resources away from machines (physical
or virtual), enabling fault-tolerant and elastic distributed
systems to easily be built and run effectively. The Mesos kernel
runs on every machine and provides applications (e.g., Hadoop,
Spark, Kafka, Elasticsearch) with APIs for resource management
and scheduling across entire datacenter and cloud environments.</p>
<p>The resource scheduler of Mesos supports a generalization of
max-min fairness <a class="reference internal" href="#paper-mesos-abu-dbai-2016" id="id395">[339]</a>, termed Dominant
Resource Fairness (DRF) <a class="reference internal" href="#paper-mesos-ghodsi2011dominant" id="id396">[340]</a>
scheduling discipline, which allows to harmonize execution of
heterogeneous workloads (in terms of resource demand) by
maximizing the share of any resource allocated to a specific
framework.</p>
<p>Mesos uses containers for resource isolation between
processes. In the context of Mesos, the two most important
resource-isolation methods to know about are the control groups
(cgroups) built into the Linux kernel,and Docker. The difference
between using hyper-V, Docker containers, cgroup is described in
detail in the book &#8220;Mesos in action&#8221; <a class="reference internal" href="#book-mesos-ignazio-2016" id="id397">[341]</a></p>
</li>
<li><p class="first">Yarn</p>
<p>Yarn (Yet Another Resource Negotiator) is Apache Hadoops cluster
management project <a class="reference internal" href="#www-cloudera" id="id398">[342]</a> . Its a resource
management technology which make a pace between, the way
applications use Hadoop system resources &amp; node manager
agents. Yarn, split up the functionalities of resource
management and job scheduling/monitoring. The NodeManager watch
the resource (cpu, memory, disk,network) usage the container and
report the same to ResourceManager. Resource manager will take a
decision on allocation of resources to the
applications. ApplicationMaster is a library specific to
application, which requests/negotiate resources from
ResourceManager and launch and monitoring the task with
NodeManager(s) <a class="reference internal" href="#www-architecture" id="id399">[343]</a>.  ResourceManager have
two majors: Scheduler and ApplicationManager. Scheduler have a
task to schedule the resources required by the
application. ApplicationManger holds the record of application
who require resource. It validates (whether to allocate the
resource or not) the applications resource requirement and
ensure that no other application already have register for the
same resource requirement. Also it keeps the track of release of
resource. <a class="reference internal" href="#www-hadoopapache" id="id400">[344]</a></p>
</li>
<li><p class="first">Helix</p>
<p>Helix is a data management system getting developed by IBM which
helps the users to do explitory analysis of the data received
from various sources following different formats. This system
would help orgnaize the data by providing links between data
collected across various sources dispite of the knowledge of the
data sources schemas.It also aims at providing  the data really
required for the user by extracting the important information
from the data. This would plan to target the issue by
mainataining the &#8220;knowledge base of schemas&#8221; and
&#8220;context-dependent dynamic linkage&#8221;, The system can get the
schema details either from the  knowledge base being maintained
or can even get the schema from the data being received. As the
number of users for helix increases the linkages gets stronger
and would provide better data
quality. <a class="reference internal" href="#www-ibm-helix-paper" id="id401">[345]</a></p>
</li>
<li><p class="first">Llama</p>
</li>
<li><p class="first">Google Omega</p>
</li>
<li><p class="first">Facebook Corona</p>
</li>
<li><p class="first">Celery</p>
<p>&#8220;Celery is an asynchronous task queue/job queue based on
distributed message passing.  The focus of celery is mostly on
real-time operation, but it equally scheduling.  In celery there
are execution units, called tasks, are executed concurrently on a
single or more worker servers using multiprocessing, Eventlet,or
gevent.  Tasks can execute asynchronously (in the background) or
synchronously (wait until ready).  Celery is easy to integrate
with web framework. Celery is written in python whereas the
protocol can be implemented in any language&#8221;<a class="reference internal" href="#celery" id="id402">[346]</a>.Celery
is a simple, flexible, and reliable distributed system to process
vast amounts of messages,while providing operations with the
tools required to maintain such a system&#8221;<a class="reference internal" href="#celerydocs" id="id403">[347]</a></p>
</li>
<li><p class="first">HTCondor</p>
<p>HTCondor is a specialized workload management system for
compute-intensive jobs.  HTCondor provides various features like
a)job queuing mechanism, b)scheduling policy, c)resource
monitoring, d)priority scheme and e)resource management just as
other full-featured batch systems.  &#8220;Users submit their serial or
parallel jobs to HTCondor,HTCondor places them into a queue,
chooses when and where to run the jobs based upon a policy,
carefully monitors their progress, and ultimately informs the
user upon completion&#8221;.  HTCondor can be used to manage a cluster
of dedicated compute nodes. HTCondor uses unique mechanisms to
harness wasted CPU power from idle deskto workstations.  &#8220;The
ClassAd mechanism in HTCondor provides an extremely flexible and
expressive framework for matching resource requests (jobs) with
resource offers (machines).  Jobs can easily state both job
requirements and job preferences&#8221;.  &#8220;HTCondor incorporates many
of the emerging Grid and Cloud-based computing methodologies and
protocols&#8221;<a class="reference internal" href="#htcondor" id="id404">[348]</a></p>
</li>
<li><p class="first">SGE</p>
</li>
<li><p class="first">OpenPBS</p>
</li>
<li><p class="first">Moab</p>
</li>
<li><p class="first">Slurm <a class="reference internal" href="#www-slurm" id="id405">[349]</a></p>
<p>Simple Linux Utility for Resource Management (SLURM) workload
manager is an open source, scalable cluster resource management
tool used for job scheduling in small to large Linux cluster
using multi-core architecture. As per,
<a class="reference internal" href="#www-slurmschedmdsite" id="id406">[350]</a> SLURM has three key
functions. First, it allocates resources to users for some
duration with exclusive and/or non-exclusive access. Second, it
enables users to start, execute and monitor jobs on the resources
allocated to them. Finally, it intermediates to resolve conflicts
on resources for pending work by maintaining them in a queue. The
slurm architecture has following components: a centralized
manager to monitor resources and work, may have a backup manager,
daemon on each server to provide fault-tolerant communications,
an optional daemon for clusters with multiple mangers and tools
to initiate, terminate and report about jobs in a graphical view
with network topology. It also provides around twenty additional
plugins that could be used for functionalities like accounting,
advanced reservation, gang scheduling, back fill scheduling and
multifactor job prioritization. Though originally developed for
Linux, SLURM also provides full support on platforms like AIX,
FreeBSD, NetBSD and Solaris <a class="reference internal" href="#www-slurmplatformssite" id="id407">[351]</a>.</p>
</li>
<li><p class="first">Torque</p>
</li>
<li><p class="first">Globus Tools</p>
</li>
<li><p class="first">Pilot Jobs</p>
<p>In pilot job, an application acquires a resource so that it can
be delegated some work directly by the application; instead of
requiring some job scheduler. The issue of using a job scheduler
is that a waiting queue is required. Few examples of Pilot Jobs
are the <a class="reference internal" href="#pilot-job-falkon-paper-2007" id="id408">[352]</a> Falkon lightweight
framework and <a class="reference internal" href="#pilot-job-htcaas-paper-2007" id="id409">[353]</a> HTCaaS. Pilot
jobs are typically associated with both Parallel computing as
well as Distributed computing. Their main aim is to reduce the
dependency on queues and the associated multiple wait times.</p>
<p><a class="reference internal" href="#www-pilot-job-paper-2016" id="id410">[354]</a> Using pilot jobs enables us to have a
multilevel technique for the execution of various workloads. This is so
because the jobs are typically acquired by a placeholder job and they
relayed to the workloads.</p>
</li>
</ol>
</div>
<div class="section" id="file-systems">
<h2>File systems<a class="headerlink" href="#file-systems" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="286">
<li><p class="first">HDFS</p>
<p>Hadoop provides distributed file system framework that uses Map
reduce (Distributed computation framework) for transformation and
analyses of large dataset.  Its main work is to partition the
data and other computational tasks to be performed on that data
across several clusters.  HDFS is the component for distributed
file system in Hadoop.An HDFS cluster primarily consists of a
Name Node and Data Nodes. Name Node manages the file system
metadata such as access permission, modification time, location
of data and Data Nodes store the actual data.&nbsp;  When user
applications or Hadoop frameworks request access to a file in
HDFS, Name Node service responds with the Data Node locations for
the respective individual data blocks that constitute the whole
of the requested <a class="reference external" href="file:cite">file:cite</a>:<cite>www-hdfs</cite>.</p>
</li>
<li><p class="first">Swift</p>
</li>
<li><p class="first">Haystack</p>
<p>Haystack is an open source project working with data from internet of Things, aim to
standardise the semantic data model generated from smart devices, homes, factories etc.
It include automation, control, energy, HVAC, lighting and other environmental systems.
[www-project-haystack.org]</p>
<p>Building block of Project haystack is on TagModel tagging of metadata stored in key/value
pair applied to entity such id, dis, sites, geoAddr, tz. Structure the primary structure
of haystack is based on three entities, Site location of single unit, equip physical or
logical piece of equipment within site, point sensor, actuator or setpoint value for equip,
it also includes weather outside weather condition. TimeZone time series data is most
important factor it is foundation for sensor and operational data. Captured data not  always
associated with measurable unit, however it provides facility to associate the data points.
Commonly Supported units like Misc, Area, Currency, Energy, Power, Temperature, Temperature
differential, Time, Volumetric Flow. The data often represented in 2D tabular form for tagged
entities. It supports the query language for filtering over the data, data exposed through
REST API in JSON format.</p>
</li>
<li><p class="first">f4</p>
<p>As the amount of data Facebook stores continues to increase, the
need for quick access and efficient storage of data continues to
rise.  Facebook stores a class of data in Binary Large OBjects
(BLOBs), which can be created once, read many times, never
modified, and sometimes deleted. Haystack, Facebooks traditional
BLOB storage system is becoming increasingly inefficient. The
storage efficiency is measured in the
effective-replication-factor of BLOBs.</p>
<p>f4 BLOB storage system provides an effective-replication-factor
lower than that of Haystack. f4 is simple, modular, scalable, and
fault tolerant. f4 currently stores over 65PBs of logical BLOBs,
with a reduced effective-replication-factor from 3.6 to either
2.8 or 2.1 <a class="reference internal" href="#paper-f4" id="id412">[355]</a>.</p>
</li>
<li><p class="first">Cinder</p>
<p>&#8220;Cinder is a block storage service for Openstack&#8221;
<a class="reference internal" href="#wiki-cinder" id="id413">[356]</a>. Openstack Compute uses ephemeral disks
meaning that they exist only for the life of the Openstack
instance i.e. when the instance is terminated the disks
disappear. Block storage system is a type of persistent storage
that can be used to persist data beyond the life of the
instance. Cinder provides users with access to persistent
block-level storage devices. It is designed such that users can
create block storage devices on demand and attach them to any
running instances of OpenStack Compute <a class="reference internal" href="#book-cinder" id="id414">[357]</a>. This
is achieved through the use of either a reference
implementation(LVM) or plugin drivers for other storage. Cinder
virtualizes the management of block storage devices and provides
end users with a self-service API to request and consume those
resources without requiring any knowledge of where their storage
is actually deployed or on what type of device
<a class="reference internal" href="#wiki-cinder" id="id415">[356]</a>.</p>
</li>
<li><p class="first">Ceph</p>
<p>Ceph is open-source storage platform providing highly scalable
object, block as well as file-based storage. Ceph is a unified,
distributed storage system designed for excellent performance,
reliability and scalability <a class="reference internal" href="#www-ceph" id="id416">[358]</a>. Ceph Storage
clusters are designed to run using an algorithm called CRUSH
(Controlled Replication Under Scalable Hashing) which replicates
and re-balance data within the cluster dynamically to ensure even
data distribution across cluster and quick data retrieval without
any centralized bottlenecks.</p>
<p>Cephs foundation is the Reliable Autonomic Distributed Object
Store (RADOS) <a class="reference internal" href="#www-cepharch" id="id417">[359]</a>, which provides applications
with object, block, and file system storage in a single unified
storage clustermaking Ceph flexible, highly reliable and easy to
manage. Ceph decouples data and metadata operations by
eliminating file allocation tables and replacing them with
generating functions which allows RADOS to leverage intelligent
OSDs to manage data replication, failure detection and recovery,
low-level disk allocation, scheduling, and data migration without
encumbering any central server(s) <a class="reference internal" href="#paper-ceph" id="id418">[360]</a>.</p>
<p>The Ceph Filesystem <a class="reference internal" href="#www-cephfs" id="id419">[361]</a> is a POSIX-compliant
filesystem that uses a Ceph Storage Cluster to store its
data. Cephs dynamic subtree partitioning is a uniquely scalable
approach, offering both efficiency and the ability to adapt to
varying workloads. Ceph Object Storage supports two compatible
interfaces: Amazon S3 and Openstack Swift.</p>
</li>
<li><p class="first">FUSE</p>
<p>FUSE (Filesystem in Userspace) <a class="reference internal" href="#www-fuse" id="id420">[362]</a> &#8220;is an interface
for userspace programs to export a filesystem to the Linux
kernel&#8221;. The FUSE project consists of two components: the fuse
kernel module and the libfuse userspace library. libfuse provides
the reference implementation for communicating with the FUSE
kernel module.The code for FUSE itself is in the kernel, but the
filesystem is in userspace.  As per the 2006 paper
<a class="reference internal" href="#fuse-paper-hptfs" id="id421">[363]</a> on HPTFS which has been built on top of
FUSE. It mounts a tape as normal file system based data storage
and provides file system interfaces directly to the application.
Another implementation of FUSE FS is CloudBB
<a class="reference internal" href="#fuse-paper-cloudbb" id="id422">[364]</a>. Unlike conventional filesystems
CloudBB creates an on-demand two-level hierarchical storage
system and caches popular files to accelerate I/O performance. On
evaluating performance of real data-intensive HPC applications in
Amazon EC2/S3, results show CloudBB improves performance by up to
28.7 times while reducing cost by up to 94.7% compared to the
ones without CloudBB.</p>
<p>Some more implementation examples of FUSE are - mp3fs (A VFS to
convert FLAC files to MP3 files instantly), Copy-FUSE(To access
cloud storage on Copy.com), mtpfs(To mount MTP devices) etc.</p>
</li>
<li><p class="first">Gluster</p>
</li>
<li><p class="first">Lustre</p>
<p>The Lustre file system <a class="reference internal" href="#www-lustre" id="id423">[365]</a> is an open-source,
parallel file system that supports many requirements of
leadership class HPC simulation environments and Enterprise
environments worldwide. Because Lustre file systems have high
performance capabilities and open licensing, it is often used in
supercomputers.Lustre file systems are scalable and can be part
of multiple computer clusters with tens of thousands of client
nodes, tens of petabytes of storage on hundreds of servers, and
more than a terabyte per second of aggregate I/O
throughput. Lustre file systems a popular choice for businesses
with large data centers, including those in industries such as
meteorology, simulation, oil and gas, life science, rich media,
and finance. Lustre provides a POSIX compliant interface and many
of the largest and most powerful supercomputers on Earth today
are powered by the Lustre file system.</p>
</li>
<li><p class="first">GPFS</p>
<p>IBM General Parallel File System (GPFS) was rebranded to IBM
Spectrum Scale on February 17, 2015 <a class="reference internal" href="#www-wikigpfs" id="id424">[366]</a>.
See 380.</p>
</li>
</ol>
<ol class="arabic" start="380">
<li><p class="first">IBM Spectrum Scale</p>
<p>General Parallel File System (GPFS) was rebranded as IBM Spectrum
Scale on February 17, 2015 <a class="reference internal" href="#www-wikigpfs" id="id425">[366]</a>.</p>
<p>Spectrum Scale is a clustered file system, developed by IBM, designed
for high performance. It &#8220;provides concurrent high-speed file access
to applications executing on multiple nodes of clusters&#8221;
<a class="reference internal" href="#www-wikigpfs" id="id426">[366]</a> and can be deployed in either shared-nothing
or shared disk modes. Spectrum Scale is available on AIX, Linux,
Windows Server, and IBM System Cluster 1350 <a class="reference internal" href="#www-wikigpfs" id="id427">[366]</a>.
Due to its focus on performance and scalability, Spectrum Scale has
been utilized in compute clusters, big data and analytics - including
support for Hadoop Distributed File System (HDFS), backups and
restores, and private clouds <a class="reference internal" href="#www-spectrumscale" id="id428">[367]</a>.</p>
</li>
</ol>
<ol class="arabic" start="296">
<li><p class="first">GFFS</p>
<p>The Global Federated File System (GFFS) <a class="reference internal" href="#www-gffs" id="id429">[368]</a> is a
computing technology that allows linking of data from Windows,
Mac OS X, Linux, AFS, and Lustre file systems into a global
namespace, making them available to multiple systems. It is a
federated, secure, standardized, scalable, and transparent
mechanism to access and share resources across organizational
boundaries It is useful when, for data resources, boundaries do
not require application modification and do not disrupt existing
data access patterns. It uses FUSE to handle access control and
allows research collaborators on remote systems to access a
shared file system. Existing applications can access resources
anywhere in the GFFS without modification. It helps in rapid
development of code, which can then be exported via GFFS and
implemented in-place on a given computational resource or Science
Gateway.</p>
</li>
<li><p class="first">Public Cloud: Amazon S3</p>
<p>Amazon Simple Storage Service (Amazon S3) <a class="reference internal" href="#www-amazon-s3" id="id430">[369]</a> is
storage object which provides a simple web service interface to
store and retrieve any amount of data from anywhere on the
web. With Amazon S3, users can store as much data as they want
and can scale it up and down based on the requirements.For
developers Amazon S3 provides full REST API&#8217;s and SDK&#8217;s which can
be integrated with third-party technologies. Amazon S3 is also
deeply integrated with other AWS services to make it easier to
build solutions that use a range of AWS services which include
Amazon CloudFront, Amazon CloudWatch, Amazon Kinesis, Amazon RDS,
Amazon Glacier etc. Amazon S3 provides auotmatic encryption of
data once the data is uploaded in the cloud. Amazon S3 uses the
concept of Buckets and Objects for storing data wherein Buckets
are used to store objects. Amazon S3 services can be used using
the Amazon Console Management. <a class="reference internal" href="#www-amazon-s3-docs" id="id431">[370]</a> The steps
for using the Amazon S3 are as follows: (1) Sign up for Amazon S3
(2) After sign up, create a Bucket in your account, (3) Create
and object which might be an file or folder, and (4) Perform
operations on the object which is stored in the cloud.</p>
</li>
<li><p class="first">Azure Blob</p>
<p>Azure Blob storage is a service that stores unstructured data in the cloud
as objects/blobs. Blob storage can store any type of text or binary data,
such as a document, media file, or application installer <a class="reference internal" href="#www-azure-3" id="id432">[371]</a>
Blob storage is also referred to as object storage. The word Blob expands
to Binary Large OBject. There are three types of blobs in the service offe-
red by Windows Azure namely block, append and page blobs. <a class="reference internal" href="#www-azure-2" id="id433">[372]</a>
1. Block blobs are collection of individual blocks with unique block ID.
The block blobs allow the users to upload large amount of data.
2. Append blobs are optimized blocks that helps in making the operations
efficient.
3. Page blobs are compilation of pages. They allow random read and write
operations. While creating a blob, if the type is not specified they are
set to block type by default. All the blobs must be inside a container in
your storage.
Azure Blob storage is a service for storing large amounts of unstructured
object data, such as text or binary data, that can be accessed from
anywhere in the world via HTTP or HTTPS. You can use Blob storage to expose
data publicly to the world, or to store application data privately. Common
uses of Blob storage include serving images or documents directly to a
browser, storing files for distributed access, streaming video and audio,
storing data for backup and restore, disaster recovery, and archiving and
storing data for analysis by an on-premises or Azure-hosted service.
Azure Storage is massively scalable and elastic with an auto-partitioning
system that automatically load-balances your data. Blob storage is a
specialized storage account for storing your unstructured data as blobs
(objects) in Azure Storage. Blob storage is similar to existing
general-purpose storage accounts and shares all the great durability,
availability, scalability, and performance features. Blob storage has two
types of access tiers that can be specified, hot access tier, which will be
accessed more frequently, and a cool access tier, which will be less
frequently accessed. There are many reasons why you should consider using
BLOB storage. Perhaps you want to share files with clients, or off-load
some of the static content from your web servers to reduce the load on
them. <a class="reference internal" href="#www-azure-3" id="id434">[371]</a></p>
</li>
<li><p class="first">Google Cloud Storage</p>
<p>Google Cloud Storage is the cloud enabled storage offered by
Google. <a class="reference internal" href="#www-google-cloud-storage" id="id435">[373]</a> It is unified object
storage. To have high availability and performance among
different regions in the geo-redundant storage offering. If you
want high availability and redundancy with a single region one
can go for Regional storage. Nearline and Coldline are the
different archival storage techniques. Nearline storage
offering is for the archived data which the user access less than
once a month . Coldline storage is the storage which is used
for the data which is touched less than once a year.</p>
<p>All the data in Google Cloud storage belongs inside a project. A
project will contains different buckets. Each bucket has
different objects. We need to make sure that the name of the
bucket is unique across all Google cloud name space . And the
name of the objects should unique in a bucket.</p>
</li>
</ol>
</div>
<div class="section" id="interoperability">
<h2>Interoperability<a class="headerlink" href="#interoperability" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="300">
<li><p class="first">Libvirt</p>
<p>Libvirt is an open source API to manage hardware virtualization
developed by Red Hat.  It is a standard C library but has
accessibility from other languages such as Python, Perl, Java
and others. <a class="reference internal" href="#www-libvirt" id="id436">[374]</a> Multiple virtual machine
monitors(VMM) or hypervisors are supported such as KVM,QEMU,
Xen, Virtuozzo, VMWare ESX, LXC, and BHyve.  It can be divided
into five categories such as hypervisor connection, domain,
network, storage volume and pool.   <a class="reference internal" href="#www-ibm" id="id437">[375]</a>  It is accessible
by many operating systems such as Linux, FreeBSD, Mac OS, and
Windows OS.</p>
</li>
<li><p class="first">Libcloud</p>
<p>:cite::<cite>www-libcloudwiki</cite> Libcloud is a python library that
allows to interact with several popular cloud service
providers. It is primarily designed to ease development of
software products that work with one or more cloud services
supported by Libcloud. It provides a unified API to interact with
these different cloud services. Current API includes methods for
list, reboot, create, destroy, list images and list
sizes. :cite::<cite>www-libclouddoc</cite> lists Libcloud key component APIs
Compute, Storage, Load Balancers, DNS, Container and
Backup. Compute API allows users to manage cloud servers. Storage
API allows users to manage cloud object storage and also provides
CDN management functionality. Load balancer, DNS and Backup APIs
allows users to manage their respective functionalities, as
services, and related products of different cloud service
providers. Container API allows users to deploy containers on to
container virtualization platforms. Libcloud supports Python 2,
Python 3 and PyPy.</p>
</li>
<li><p class="first">JClouds</p>
<p><a class="reference internal" href="#cloud-portability-book" id="id438">[376]</a> Primary goals of cross-platform
cloud APIs is that application built using these APIs can be
seamlessly ported to different cloud providers. The APIs also
bring interoperability such that cloud platforms can communicate
and exchange information using these common or shared interfaces.
Jclouds or apache jclouds <a class="reference internal" href="#www-jclouds" id="id439">[377]</a> is a java based
library to provide seamless access to cloud platforms. Jclouds
library provides interfaces for most of cloud providers like
docker, openstack, amazon web services, microsoft azure, google
cloud engine etc. It will allow users build applications which
can be portable across different cloud environments.  Key
components of jcloud are:</p>
<ol class="arabic">
<li><p class="first">Views: abstracts functionality from a specific vendor and
allow user to write more generic code. For example odbc
abstracts the underlying relational data source. However, odbc
driver converts to native format. In this case user can switch
databases without rewriting the application. Jcloud provide
following views: blob store, compute service, loadBalancer
service</p>
</li>
<li><p class="first">API: APIs are requests to execute a particular
functionality. Jcloud provide a single set of APIs for all
cloud vendors which is also location aware. If a cloud vendor
doesnt support customers from a particular region the API
will not work from that region.</p>
</li>
<li><p class="first">Provider: a particular cloud vendor is a provider. Jcloud uses
provider information to initialize its context.</p>
</li>
<li><p class="first">Context: it can be termed as a handle to a particular
provider. Its like a ODBC connection object. Once connection
is initialized for a particular database, it can used to make
any api call.</p>
<p>Jclouds provides test library to mock context, APIs etc to
different providers so that user can write unit test for his
implementation rather than waiting to test with the cloud
provider. Jcloud library certifies support after testing the
interfaces with live cloud provider. These features make
jclouds robust and adoptable, hiding most of the complexity of
cloud providers.</p>
</li>
</ol>
</li>
<li><p class="first">TOSCA</p>
</li>
<li><p class="first">OCCI</p>
<p>The Open Cloud Computing Interface (OCCI) is a RESTful
Protocol and API that provides specifications  and remote
management for the development of interoperable tools
<a class="reference internal" href="#www-occi" id="id440">[378]</a>. It supports IaaS, PaaS and SaaS and
focuses on integration, portability, interoperability,
innovation and extensibility. It provides a set of documents
that describe an OCCI Core model, contain best practices
of interaction with the model, combined into OCCI Protocols,
explain methods of communication between components via
HTTP protocol introduced in the OCCI Renderings, and
define infrastructure for IaaS presented in the OCCI
Extensions.</p>
<p>The current version 1.2 OCCI consists of seven documents that
identify require and optional components. Of the Core Model.  In
particular, the following components are required to implement:
a)Core Model, b)HTTP protocol, c)Text rendering and d)JSON
rendering. Meanwhile, Infrastructure, Platform and SLA models are
optional.  The OCCI Core model defines instance types and</p>
<p>provides a layer of abstraction that allows the OCCI client
to interact with the model without knowing of its potential
structural changes. The model supports extensibility via
inheritance and using mixin types that represent ability to
add new components and capabilities at run-time.
<a class="reference internal" href="#nyren-edmonds-papaspyrou-2016" id="id441">[379]</a></p>
<p>The OCCI Protocol defines the common set of names provided
for the IaaS cloud services user that specify requested
system requirements. It is often denoted as resource
templates or flavours   <a class="reference internal" href="#drescher-parak-wallom-2015" id="id442">[380]</a>.</p>
<p>OCCI RESTful HTTP Protocol describes communications between
server and client on OCCI platform via HTTP protocol
<a class="reference internal" href="#nyren-edmonds-metsch-2016" id="id443">[381]</a>. It defines a minimum set of HTTP
headers and status codes to ensure compliance with the
OCCI Protocol. Separate requirements for Server and Client
for versioning need to be implemented using HTTP &#8216;Server&#8217;
header and &#8216;User-Agent&#8217; header respectively.</p>
<p>JSON rendering  <a class="reference internal" href="#nyren-feldhaus-parak-2016" id="id444">[382]</a> protocol provides
JSON specifications to allow &#8220;render OCCI instances
independently of the protocol being used.&#8221; In addition, it
provides details of the JSON object declaration, OCCI Action
Invocation, object members required for OCCI Link Instance
Rendering, &#8220;location maps to OCCI Core&#8217;s source and target
model attributes and kind maps to OCCI Core&#8217;s target&#8221; to
satisfy OCCI Link Instance Source/Target Rendering requirements.
Finally, it specifies various attributes and collection
rendering requirements.
The text rendering process is depricated and will be
removed from the next major version  <a class="reference internal" href="#edmonds-metsch-2016" id="id445">[383]</a>.</p>
</li>
<li><p class="first">CDMI</p>
<p>The Storage Networking Industry Association (SNIA)
<a class="reference internal" href="#www-sniawebsite" id="id446">[384]</a> is a non-profit organization formed by
various companies, suppliers and consumers of data storage and
network products. SNIA defines various standards to ensure the
quality and interoperability of various storage systems. One of
the standards defined by SNIA to for providers and users of cloud
is Cloud Data Management Interface (CDMI). According latest issue
of CDMI <a class="reference internal" href="#cdmi-manual" id="id447">[385]</a>, &#8220;CDMI International Standard is
intended for application developers who are implementing or using
cloud storage. It documents how to access cloud storage and to
manage the data stored there.&#8221; It defines functional interface
for applications that will use cloud for various functionalities
like create, retrieve, update and delete data elements from the
cloud. These interface could be used to manage containers along
with the data. The interface could be used by administrative and
management applications as well. Also, the CDMI specification
uses RESTful principles in the interface design. All the
standards issued on CDMI can be found on SNIA web page
<a class="reference internal" href="#www-cdmiwebsite" id="id448">[386]</a>.</p>
</li>
<li><p class="first">Whirr</p>
</li>
<li><p class="first">Saga</p>
<p>SAGA(Simple API for Grid Applications) provides an abstraction layer
to make it easier for applications to utilize and exploit infra
effectively. With infrastructure being changed continuously its
becoming difficult for most applications to utilize the advances in
hardware. SAGA API provides a high level abstraction of the most
common Grid functions so as to be independent of the diverse and
dynamic Grid environments. <a class="reference internal" href="#saga-paper" id="id449">[387]</a> This shall address the
problem of applications developers developing an application tailored
to a specific set of infrastructure.  SAGA allows computer scientists
to write their applications at high level just once and not to worry
about low level hardware changes. SAGA provides this high level
interface which has the underlying mechanisms and adapters to make the
appropriate calls in an intelligent fashion so that it can work on any
underlying grid system. SAGA was built to provide a standardized,
common interface across various grid middleware systems and their
versions.  <a class="reference internal" href="#www-saga-ogf-document" id="id450">[388]</a></p>
<p>As SAGA is to be implemented on different types
of middleware it does not specify a single security model but provides
hooks to interfaces of various security models. The SAGA API provides
a set of packages to implement its objectivity : SAGA supports data
management, resource discovery, asynchronous notification, event
generation, event delivery etc. It does so by providing set of
functional packages namely SAGA file package, replica package, stream
package, RPC package, etc. SAGA provides interoperability by allowing
the same application code to run on multiple grids and also
communicate with applications running on others. <a class="reference internal" href="#saga-paper" id="id451">[387]</a></p>
</li>
<li><p class="first">Genesis</p>
</li>
</ol>
</div>
<div class="section" id="devops">
<h2>DevOps<a class="headerlink" href="#devops" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="309">
<li><p class="first">Docker (Machine, Swarm)</p>
<p>Docker is an open-source container based technology.A container
allows a developer to package up an application and all its part
includig the stack it runs on, dependencies it is associated with
and everything the application requirs to run within an isolated
enviorment . Docker seperates Application from the underlying
Operating System in a similar way as Virtual Machines seperates
the Operating System from the underlying Hardware.Dockerizing an
application is very lightweight in comparison with running the
application on the Virtual Machine as all the containers share
the same underlying kernel, the Host OS should be same as the
container OS (eliminating guest OS) and an average machine cannot
have more than few VMs running o them.</p>
<p>:cite:&#8217;docker-book&#8217; Docker Machine is a tool that lets you
install Docker Engine on virtual hosts, and manage the hosts with
docker-machine commands. You can use Machine to create Docker
hosts on your local Mac or Windows box, on your company network,
in your data center, or on cloud providers like AWS or Digital
Ocean. For Docker 1.12 or higher swarm mode is integerated with
the Docker Engine, but on the older versions with Machine&#8217;s swarm
option, we can configure a swarm cluster Docker Swarm provides
native clustering capabilities to turn a group of Docker engines
into a single, virtual Docker Engine. With these pooled resources
,:cite:&#8217;www-docker&#8216;&#8220;you can scale out your application as if it
were running on a single, huge computer&#8221; as swarm can be scaled
upto 1000 Nodes or upto 50,000 containers</p>
</li>
<li><p class="first">Puppet</p>
<p>Puppet is an open source software configuration management
tool <a class="reference internal" href="#www-puppet-wiki-puppet" id="id452">[389]</a>.This aims at automatic
configuration of the software
applications and infrastructure. This configuration is done
using the easy to use languge.
Puppet works on major linux distributions and also on
microsoft windows ,
it is also cross-platform application making it easy to manage
and portable. <a class="reference internal" href="#www-puppet-puppet-site" id="id453">[390]</a></p>
<p>Puppet works with a client server model. All the clients (
nodes)  which needs to be managed will have &#8216;Puppet Agent&#8217;
installed and &#8216;Puppet Master&#8217; contains the configuration for
different hosts this demon process rund on master server. The
connection between &#8216;Puppet Master&#8217; and &#8216;Puppet agent&#8217; will be
established using thesecured SSL connection. The configiration
at client will be validated as per the set up in Puppet master
at a predefined interval. If configration at client is not
matching with the master puppet agent fetches the equired
changes from master. <a class="reference internal" href="#www-puppet-slashroot" id="id454">[391]</a></p>
<p>Puppet is developed by Puppet Labs
using ruby language and released as GNU General Public License
(GPL) until version 2.7.0 and the Apache License 2.0 after
that. <a class="reference internal" href="#www-puppet-wiki-puppet" id="id455">[389]</a></p>
</li>
<li><p class="first">Chef</p>
<p>Chef is a configuration management tool. It is implemented in
Ruby and Erlang. Chef can be used to configure and maintain
servers on-premise as well as cloud platforms like Amazon EC2,
Google Cloud Platform and Open Stack. The book
<a class="reference internal" href="#chef-book" id="id456">[392]</a> explains the use of concept called &#8216;recipes&#8217; in
Chef to manage server applications and utilities such as database
servers like MySQL, or HTTP servers like Apache HTPP and systems
like Apache Hadoop.</p>
<p>Chef is available in open source version and it also has
commercial products for the companies which need it
<a class="reference internal" href="#www-chef-commercial" id="id457">[393]</a></p>
</li>
<li><p class="first">Ansible</p>
<p>Ansible is an IT automation tool that automates cloud
provisioning, configuration management, and application
deployment. <a class="reference internal" href="#www-ansible" id="id458">[394]</a> Once Ansible gets installed on a
control node, which is an agentless architecture, it connects to
a managed node through the default OpenSSH connection
type. <a class="reference internal" href="#www-ansible2" id="id459">[395]</a></p>
<p>As with most configuration management softwares, Ansible
distinguishes two types of servers: controlling machines and
nodes. First, there is a single controlling machine which is
where orchestration begins. Nodes are managed by a controlling
machine over SSH. The controlling machine describes the location
of nodes through its inventory.</p>
<p>Ansible manages machines in an agent-less manner. Ansible is
decentralized, if needed, Ansible can easily connect with
Kerberos, LDAP, and other centralized authentication management
systems.</p>
</li>
<li><p class="first">SaltStack</p>
</li>
<li><p class="first">Boto</p>
<p><a class="reference internal" href="#www-boto" id="id460">[396]</a> The latest version of Boto is Boto3.
<a class="reference internal" href="#www-boto-github" id="id461">[397]</a> Boto3 is the Amazon Web Services (AWS) Software
Development Kit (SDK) for Python. It enables the Python developers to
make use of services like Amazon S3 and Amazon EC2.
<a class="reference internal" href="#www-boto3-documentation" id="id462">[398]</a> It provides object oriented APIs along
with low-level direct service access. It provides simple in-built
functions and interfaces to work with Amazon S3 and EC2.</p>
<p><a class="reference internal" href="#www-boto-amazon-python-sdk" id="id463">[399]</a> Boto3 has two distinct levels of APIs
- client and resource. One-to-one mappings to underlying HTTP API is
provided by the client APIs. Resource APIs provide resource objects and
collections to perform various actions by accessing the attributes.
Boto3 also comes with &#8216;waiters&#8217;. Waiters are used for polling status
changes in AWS, automatically. Boto3 has these waiters for both the APIs
- client as well as resource.</p>
</li>
<li><p class="first">Cobbler</p>
<p>Cobbler is a Linux provisioning system that facilitates and
automates the network based system installation of multiple computer
operating systems from a central point using services such as DHCP,
TFTP and DNS <a class="reference internal" href="#www-cobbler" id="id464">[400]</a>.It is a nifty piece of code that
assemble s all the usual
setup bits required for a large network installation like TFTP, DNS,
PXE installation trees. and automates the process[1].It can be
configured for PXE, reinstallations and virtualized guests using Xen,
KVM or VMware.  Cobbler interacts with the koan program for
re-installation and virtualization support.  Cobbler builds the
Kickstart mechanism and offers installation profiles that can be
applied to one or many machines.  Cobbler has features to dynamically
change the information contained in a kickstart template (definition),
either by passing variables called ksmeta or by using so-called
snippets.</p>
</li>
<li><p class="first">Xcat</p>
</li>
<li><p class="first">Razor</p>
<p>Razor is a hardware provisioning application, developed by Puppet
Labs and EMC. Razor was introduced as open, pluggable, and
programmable since most of the provisioning tools that existed
were vendor-specific, monolithic, and closed. According to
<a class="reference internal" href="#razorwiki" id="id465">[401]</a> it can deploy both bare-metal and virtual
systems. During boot the Razor client automatically discovers the
inventory of the server hardware  CPUs, disk, memory, etc.,
feeds this to the Razor server in real-time and the latest state
of every server is updated. It maintains a set of rules to
dynamically match the appropriate operating system images with
server capabilities as expressed in metadata. User-created policy
rules are referred to choose the preconfigured model to be
applied to a new node. The node follows the model&#8217;s directions,
giving feedback to Razor as it completes various steps as
specified in <a class="reference internal" href="#razorpuppet" id="id466">[402]</a>. Models can include steps for
handoff to a DevOps system or to any other system capable of
controlling the node.</p>
</li>
<li><p class="first">CloudMesh</p>
</li>
<li><p class="first">Juju</p>
<p>Juju (formerly Ensemble) <a class="reference internal" href="#juju-paper" id="id467">[403]</a> is software from
Canonical that provides open source service orchestration. It is
used to easily and quickly deploy and manage services on cloud
and physical servers. Juju charms can be deployed on cloud
services such as Amazon Web Services (AWS), Microsoft Azure and
OpenStack. It can also be used on bare metal using MAAS.
Specifically <a class="reference internal" href="#www-juju" id="id468">[404]</a> lists around 300 charms available
for services available in the Juju store. Charms can be written
in any language. It also supports Bundles which are
pre-configured collection of Charms that helps in quick
deployment of whole infrastructure.</p>
</li>
<li><p class="first">Foreman</p>
</li>
<li><p class="first">OpenStack Heat</p>
<p>Openstack Heat, a template deployment service was the project
launched by Openstack, a cloud operating system similar to AWS
Cloud Formation. <a class="reference internal" href="#www-heat-blog-introduction" id="id469">[405]</a> states - Heat
is an orchestration service which allows us to define resources
over the cloud and connections amongst them using a simple text
file called referred as a template. &#8220;A Heat template describes
the infrastructure for a cloud application in a text file that is
readable and writable by humans, and can be checked into version
control&#8221; <a class="reference internal" href="#www-heat-wiki" id="id470">[406]</a></p>
<p>Once the execution enviroment has been setup and a user wants to
modify the architecture of resources in the future, a user needs
to simply change the template and check it in. Heat shall make
the necessary changes. Heat provides 2 types of template -
HOT(Heat Orchestration Template) and CFN (AWS Cloud Formation
Template). The HOT can be defined as YAML and is not compatible
with AWS. The CFN is expressed as JSON and follows the syntax of
AWS Cloud Formation and thus is AWS compatible. Further, heat
provides an additional &#64;parameters section in its template which
can be used to parameterize resources to make the template
generic.</p>
</li>
<li><p class="first">Sahara</p>
<p>The Sahara product provides users with the capability to
provision data processing frameworks (such as Hadoop, Spark and
Storm) on OpenStack <a class="reference internal" href="#www-openstack" id="id471">[407]</a> by specifying several
parameters such as the version,cluster topology and hardware node
details.As specified in <a class="reference internal" href="#www-sahara" id="id472">[408]</a> the solution allows
for fast provisioning of data processing clusters on OpenStack
for development and quality assurance and utilisation of unused
computer power from a general purpose OpenStack Iaas Cloud.Sahara
is managed via a REST API with a User Interface available as part
of OpenStack Dashboard.</p>
</li>
<li><p class="first">Rocks</p>
</li>
<li><p class="first">Cisco Intelligent Automation for Cloud</p>
<p>Cisco Intelligent automation for cloud desires to help different
service providers and software professionals in delivering highly
secure infrastructure as a service on demand. It provides a
foundation for organizational transformation by expanding the
uses of cloud technology beyond its infrastructure
<a class="reference internal" href="#cis1" id="id473">[409]</a>. From a single self-service portal, it automates
standard business processes and sophisticated data center which
is beyond the provision of virtual machines. Cisco Intelligent
automation for cloud is a unified cloud platform that can deliver
any type of service across mixed environments <a class="reference internal" href="#cis2" id="id474">[410]</a>. This
leads to an increase in cloud penetration across different
business and IT holdings. Its services range from underlying
infrastructure to anything-as-a-service by allowing its users to
evaluate, transform and deploy the IT and business services in a
way they desire.</p>
</li>
<li><p class="first">Ubuntu MaaS</p>
</li>
<li><p class="first">Facebook Tupperware</p>
<p>Facebook Tupperware is a system which provisions services by taking
requirements from engineers and mapping them to actual hardware allocations
using containers <a class="reference internal" href="#www-facetup" id="id475">[411]</a>.Facebook Tupperware simplifies
the  task of configuring and running services in production and
allows engineers to focus on actual application logic.The tupperware
system consists of a Scheduler , Agent process and a Server Databse.
The Scheduler consists of set of machines with one of them as master
and the others in standby.The machines share state among them.The Agent process
runs on each and every machine and manages all the tasks and co-ordinates
with the Scheduler.The Server database stores the details of resources
available across machines which is used by the scheduler for scheduling
jobs and tasks.Tupperware allows for sandboxing of the tasks which allows
for isolation of the tasks.Initially isolation was implemented using chroots
but now it is switched to Linux Containers(LXC) .The
configuration for the container is done by a specific config file written
in a dialect of python by the owner of the process.</p>
</li>
<li><p class="first">AWS OpsWorks</p>
<p>AWS Opsworks is a configuration service provided by Amazon Web
Services that uses Chef, a Ruby and Erlang based configuration
management tool <a class="reference internal" href="#www-wikichef" id="id476">[412]</a>, to automate the
configuration, deployment, and management of servers and
applications. There are two versions of AWS Opsworks.
The first, a fee based offering called AWS OpsWorks for Chef
Automate, provides a Chef Server and suite of tools to enable
full stack automation. The second, AWS OpsWorks Stacks, is a
free offering in which applications are modeled as stacks
containing various layers. Amazon Elastic Cloud Compute (EC2)
instances or other resources can be deployed and configured
in each layer of AWS OpsWorks Stacks <a class="reference internal" href="#www-awsopsworks" id="id477">[413]</a>.</p>
</li>
<li><p class="first">OpenStack Ironic</p>
<p>Ironic <a class="reference internal" href="#www-ironicwebsite" id="id478">[414]</a> project is developed and
supported by OpenStack. Ironic provisions bare metal machines
instead of virtual machines and functions as hypervisor API that
is developed using open source technologies like Preboot
Execution Environment (PXE), Dynamic Host Configuration Protocol
(DHCP), Network Bootstrap Program (NBP), Trivial File Transfer
Protocol (TFTP) and Intelligent Platform Management Interface
(IPMI). A properly configured Bare Metal service with the Compute
and Network services, could provision both virtual and physical
machines through the Compute services API. But, the number of
instance actions are limited, due to physical servers and switch
hardware. For example, live migration is not possible on a bare
metal instance. The Ironic service has five key components. A
RESTful API service, through which other components would
interact with the bare metal servers, a Conductor service,
various drivers, messaging queue and a database. Ironic could be
integrated with other OpenStack projects like Identity
(keystone), Compute (nova), Network (neutron), Image (glance) and
Object (swift) services.</p>
</li>
<li><p class="first">Google Kubernetes</p>
<p>Google Kubernetes is a cluster management platform developed by
Google. According to <a class="reference internal" href="#www-kubernetesdoc" id="id479">[415]</a> is an open source
system for &#8220;automating deployment, scaling and management of
containerized applications&#8221;. It primarily manages clusters
through containers as they decouple applications from the
host operating system dependencies and allowing their quick and
seamless deployment, maintenance and scaling.</p>
<p>Kubernetes components are designed to extensible primarily
through Kubernetes API. Kubernetes follows a master-slave
architecture, according to <a class="reference internal" href="#www-kuberneteswiki" id="id480">[416]</a> Kubernetes
Master controls and manages the clusters workload and
communications of the system. Its main components are etcd, API
server, scheduler and controller manager. The individual
Kubernetes nodes are the workers where containers are
deployed. The components of a node are Kubelet, Kube-proxy and
cAdvisor. Kunernetes makes it easier to run application on public
and private clouds. It is also said to be self-healing due to
features like auto-restart and auto-scaling.</p>
</li>
<li><p class="first">Buildstep</p>
<p>Buildsteps is an open software developed under MIT license.
It is a base for Dockerfile and it activates Heroku-style
application. Heroku is a platform-as-service (PaaS) that
automates deployment of applications on the cloud. The
program is pushed to the PaaS using git push, and then
PaaS detects the programming language, builds, and runs
application on a cloud platform <a class="reference internal" href="#plassnig15" id="id481">[417]</a>.
Buildstep takes two parameters: a tar file that contains
the application and a new application container name to
create a new container for this application. Build script
is dependent on buildpacks that are pre-requisites for
buildstep to run. The builder script runs inside the new
container.  The resulting build app can be run with Docker
using docker build -t your_app_name command.
<a class="reference internal" href="#github-buildstep" id="id482">[418]</a>.</p>
</li>
<li><p class="first">Gitreceive</p>
<p>Gitreceive is used to create an ssh+git user which can accept
repository pushes right away and also triggers a hook
script. Gitreceive is used to push code anywhere as well as
extend your Git workflow. &#8220;Gitreceive dynamically creates bare
repositories with a special pre-receive hook that triggers your
own general gitreceive hook giving you easy access to the code
that was pushed while still being able to send output back to the
git user&#8221; Gitreceive can also be used to provide feedback to the
user not only just to trigger code on git push.  Gitreceive can
used for the following: &#8220;a)for putting a git push deploy
interface in front of App Engine b)Run your company build/test
system as a separate remote c)Integrate custom systems into your
workflow d)Build your own Heroku e)Push code
anywhere&#8221;.:cite:<cite>lindsay2016</cite></p>
</li>
<li><p class="first">OpenTOSCA</p>
</li>
<li><p class="first">Winery</p>
<p>Eclipse Winery <a class="reference internal" href="#www-winery" id="id483">[419]</a> is a &#8220;web-based environment to
graphically model [Topology and Orchestration Specification for
Cloud Applications] TOSCA topologies and plans managing these
topologies.&#8221; Winery <a class="reference internal" href="#winery-paper-2013" id="id484">[420]</a> is a &#8220;tool
offering an HTML5-based environment for graph-based modeling of
application topologies and defining reusable component and
relationship types.&#8221; This web-based <a class="reference internal" href="#winery-paper-2013" id="id485">[420]</a>
interface enables users to drag and drop icons to create
automated &#8220;provisioning, management, and termination of
applications in a portable and interoperable way.&#8221;
Essentially, this web-based interface <a class="reference internal" href="#winery-paper-2013" id="id486">[420]</a>
allows users to create an application topology, which
&#8220;describes software and hardware components involved and
relationships between them&#8221; as well a management plan, which
&#8220;captures knowledge [regarding how] to deploy and manage an
application.&#8221;</p>
</li>
<li><p class="first">CloudML</p>
</li>
<li><p class="first">Blueprints</p>
<p>In <a class="reference internal" href="#www-blueprints" id="id487">[421]</a>, it is explained that &#8220;IBM Blueprint
has been replaced by IBM Blueworks Live.&#8221; In
<a class="reference internal" href="#www-blueworks-live2" id="id488">[422]</a>, IBM Blueworks Live is described &#8220;as
a cloud-based business process modeller, belonging under the set
of IBM SmartCloud applications&#8221; that as
<a class="reference internal" href="#www-blueworks-live" id="id489">[423]</a> states &#8220;drive[s] out inefficiencies
and improve[s] business operations.&#8221; Similarly to Google Docs,
IBM Blueworks Live is &#8220;designed to help organizations discover
and document their business processes, business decisions and
policies in a collaborative manner.&#8221; While Google Docs and IBM
Blueworks Live are both simple to use in a collaborative manner,
<a class="reference internal" href="#www-blueworks-live2" id="id490">[422]</a> explains that IBM Blueworks Live
has the &#8220;capabilities to implement more complex models.&#8221;</p>
</li>
<li><p class="first">Terraform</p>
<p>Terraform, developed by HashiCorp, is an infrastructure
management tool, it has an open source platform as well as an
enterprise version and uses infrastructure as a code to increase
operator productivity. Its latest release is Terraform 0.8
According to the website <a class="reference internal" href="#www-terraform" id="id491">[424]</a> it enables users
to safely and predictably create, change and improve the
production infrastructure and codifies APIs into declarative
configuration files that can be shared amongst other users and
can be treated as a code, edited, reviewed and versioned at the
same time. The book <a class="reference internal" href="#www-terraform-book" id="id492">[425]</a> explains that it
can manage the existing and popular service it provides as well
as create customized in-house solutions. It builds an execution
plan that describes what it can do next after it reaches a
desired state to accomplish the goal state. It provides a
declarative executive plan which is used for creating
applications and implementing the infrastructures. Terraform is
mainly used to manage cloud based and SaaS infrastructure, it
also supports Docker and VMWare vSphere.</p>
</li>
<li><p class="first">DevOpSlang</p>
</li>
<li><p class="first">Any2Api</p>
<p>This framework <a class="reference internal" href="#wettinger-any2api" id="id493">[426]</a> allows user to wrap an
executable program or scripts, for example scripts, chef
cookbooks, ansible playbooks, juju charms, other compiled
programs etc. to generate APIs from your existing code.  These
APIs are also containerized so that they can be hosted on a
docker container, vagrant box etc Any2Api helps to deal with
problems like scale of application, technical expertise, large
codebase and different API formats. The generated API hide the
tool specific details simplifying the integration and
orchestration different kinds of artifacts. The APIfication
framework contains various modules:</p>
<ol class="arabic simple">
<li>Invokers, which are capable of running a given type of
executable for example cookbook invoker can be used to run Chef
cookbooks</li>
<li>Scanners, which are capable of scanning modules of certain type for
example cookbook scanner scans Chef cookbooks.</li>
<li>API impl generators, which are doingthe actual work to
generate the API implementation.</li>
</ol>
<p>The final API implementation <a class="reference internal" href="#www-any2api" id="id494">[427]</a> is is packages
with executable in container.  The module is packaged as npm
module. Currently any2api-cli provides a command line interface
and web based interface is planned for future
development. Any2Api is very useful for by devops to orchestrate
open source ecosystem without dealing with low level details of
chef cookbook or ansible playbook or puppet. It can also be very
useful in writing microservices where services talk to each other
using well defined APIs.</p>
</li>
</ol>
</div>
<div class="section" id="iaas-management-from-hpc-to-hypervisors">
<h2>IaaS Management from HPC to hypervisors<a class="headerlink" href="#iaas-management-from-hpc-to-hypervisors" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="339">
<li><p class="first">Xen</p>
<p>Xen is the only open-source bare-metal hypervisor based on
microkernel design <a class="reference internal" href="#www-xen-wikipedia" id="id495">[428]</a>. The hypervisor
runs at the highest privilege among all the processes on the
host. It&#8217;s responsibility is to manage CPU and memory and
handle interrupts <a class="reference internal" href="#www-xen-overview" id="id496">[429]</a>. Virtual
machines are deployed in the guest domain called DomU which
has no access privilege to hardware. A special virtual machine
is deployed in the control domain called Domain 0. It contains
hardware drivers and the toolstack to control the VMs and is
the first VM to be deployed. Xen supports both Paravirtualization
and hardware assisted virtualization. The hypervisor itself has
a very small footprint. It&#8217;s being actively maintained by Linux
Foundation under the trademark &#8220;XEN Project&#8221;. Some of the
features included in the latest releases include &#8220;Reboot-free
Live Patching&#8221; (to enable application of security patches without
rebooting the system) and KCONFIG support (compilation support to
create a lighter version for requirements such as embedded
systems) <a class="reference internal" href="#www-xen-fl" id="id497">[430]</a>.</p>
</li>
<li><p class="first">KVM</p>
<p><a class="reference internal" href="#www-kvm-wiki" id="id498">[431]</a> It is an acronym for Kernel-based Virtual
Machine for the Linux Kernel that turns it into a
hypervisor upon installation. It was originally developed
by Qumranet in 2007. It has a kernel model and uses kernel
as VMM. It only supports fully virtualized VMs. It is very
active for Linux users due to its ease of use, it can be
completely controlled by ourselves and there is an ease for
migration from or to other platforms. [www-KVM- webpage] It is built to run on a x86 machine on an Intel
processor with virtualization technology extensions (VT-x)
or an AMD-V. It supports 32 and 64 bit guests on a 64 bit
host and hardware visualization features. The supported
guest systems are Solaris , Linux, Windows and BSD Unix.</p>
</li>
<li><p class="first">QEMU</p>
<p>QEMU (Quick Emulator) is a generic open source hosted hypervisor
<a class="reference internal" href="#www-hypervisor" id="id500">[432]</a> that performs hardware virtualization
(virtualization of computers as complete hardware platform,
certain logical abstraction of their componentry or only the
certain functionality required to run various operating systems)
:cite-<cite>www-qemu</cite> and also emulates CPUs through dynamic binary
translations and provides a set of device models, enabling it to
run a variety of unmodified guest operating systems.</p>
<p>When used as an emulator, QEMU can run Operating Systems and programs
made for one machine (ARM board) on a different machine (e.g. a
personal computer) and achieve good performance by using dynamic
translations.  When used as a virtualizer, QEMU achieves near native
performance by executing the guest code directly on the host CPU. QEMU
supports virtualization when executing under the Xen hypervisor or
using KVM kernel module in Linux <a class="reference internal" href="#www-qemuwiki" id="id501">[433]</a>.</p>
<p>Compared to other virtualization programs like VMWare and VirtualBox,
QEMU does not provide a GUI interface to manage virtual machines nor
does it provide a way to create persistent virtual machine with saved
settings. All parameters to run virtual machine have to be specified
on a command line at every launch. Its worth noting that there are
several GUI front-ends for QEMU like virt-manager and gnome-box.</p>
</li>
<li><p class="first">Hyper-V</p>
</li>
<li><p class="first">VirtualBox</p>
</li>
<li><p class="first">OpenVZ</p>
<p>OpenVZ (Open Virtuozzo) is an operating system-level virtualization
technology for Linux. It allows a physical server to run multiple isolated
operating system instances, called containers, virtual private servers, or
virtual environments (VEs). OpenVZ is similar to Solaris Containers and
LXC. <a class="reference internal" href="#www-openvz-3" id="id502">[434]</a> While virtualization technologies like VMware and
Xen provide full virtualization and can run multiple operating systems and
different kernel versions, OpenVZ uses a single patched Linux kernel and
therefore can run only Linux. All OpenVZ containers share the same archite-
cture and kernel version. This can be a disadvantage in situations where
guests require different kernel versions than that of the host. However, as
it does not have the overhead of a true hypervisor, it is very fast and
efficient. Memory allocation with OpenVZ is soft in that memory not used in
one virtual environment can be used by others or for disk caching. <a class="reference internal" href="#www-openvz-2" id="id503">[435]</a>
While old versions of OpenVZ used a common file system (where each virtual
environment is just a directory of files that is isolated using chroot),
current versions of OpenVZ allow each container to have its own file system.
OpenVZ has four main features, <a class="reference internal" href="#www-openvz-1" id="id504">[436]</a>
1. OS virtualization: A container (CT) looks and behaves like a regular
Linux system. It has standard startup scripts; software from vendors can
run inside a container without OpenVZ-specific modifications or adjustment;
A user can change any configuration file and install additional software;
Containers are completely isolated from each other and are not bound to
only one CPU and can use all available CPU power.
2. Network virtualization: Each CT has its own IP address and CTs are
isolated from the other CTs meaning containers are protected from each
other in the way that makes traffic snooping impossible; Firewalling may
be used inside a CT
3. Resource management: All the CTs are use the same kernel. OpenVZ
resource management consists of four main components: two-level disk quota,
fair CPU scheduler, disk I/O scheduler, and user beancounters.
4. Checkpointing and live migration: Checkpointing allows to migrate a
container from one physical server to another without a need to
shutdown/restart a container. This feature makes possible scenarios such as
upgrading your server without any need to reboot it: if your database needs
more memory or CPU resources, you just buy a newer better server and live
migrate your container to it, then increase its limits.</p>
</li>
<li><p class="first">LXC</p>
<p>LXC (Linux Containers) is an operating-system-level
virtualization method for running multiple isolated
Linux systems (containers) on a control host using a single
Linux kernel <a class="reference internal" href="#www-wiki-lxc" id="id505">[437]</a>. LXC are similar to the treditional virtual
machines but instead of having seperate kernel process for the
guest operating system being run, containers would share the
kernal process with the host operating system. This is made
possible with the implementation of namespaces and cgroups. <a class="reference internal" href="#www-jpablo" id="id506">[438]</a></p>
<p>Containers are light weighed ( As guest operating system
loading and booting is eleminated ) and more customizable
compared to VM technologies.The basis for docker developement
is also LXC. <a class="reference internal" href="#www-infoworld" id="id507">[439]</a>. Linux containers would
work on the major distributions of linux this would not work
on Microsoft Windows.</p>
</li>
<li><p class="first">Linux-Vserver</p>
<p>Linux-VServers are used on web hosting services, pooling resources and containing
any security breach. [www.linux-vserver.org/Paper] Linux servers consist
of three building blocks Hardware, Kernel and Applications the purpose of kernel
is to provide abstraction layer between hardware and application. Linux-Vserver
provides VPS securely partitioning the resources on computer system in such a way
that process cannot mount denial of service out of the partition.</p>
<p>It utilises the power of Linux kernel and top of it with additional modification
provides secure layer to each process (VPS)  feel like it is running separate system.
By providing context separation, context capabilities, each partition called as
security context, chroot barrier created on provate directory of each VPS to prevent
unauthorized modification. Booting VPS in new secure context is just matter of booting
server, context is so robust to boot many server simultaneously.</p>
<p>The virtual servers shares same system calls, shares common file system, process
within VS are queued to same scheduler that of host allowing guest process to run
concurrently on SMP systems. No additional overhead of network virtualization.
These few advantages of Linux-VServer.</p>
</li>
<li><p class="first">OpenStack</p>
</li>
<li><p class="first">OpenNebula</p>
<p>According to OpenNebula webpage <a class="reference internal" href="#www-opennebula-org" id="id509">[440]</a> it
provides simple but feature-rich and flexible solutions for the
comprehensive management of virtualized data centers to enable
private, public and hybrid laaS clouds. It is a cloud computing
platform for managing heterogenous distributed data centers
infrastructures. The OpenNebula toolkit includes features for
management, scalability, security and accounting. It used in
various sectors like hosting providers, telecom providers,
telecom operators, IT service providers, supercomputing centers,
research labs, and international research projects
<a class="reference internal" href="#www-opennebula-wiki" id="id510">[441]</a>. More about OpenNebula can be found
in the following paper that is published at ieee computer society
<a class="reference internal" href="#paper-opennebula" id="id511">[442]</a></p>
</li>
<li><p class="first">Eucalyptus</p>
<p>Eucalyptus is a Linux-based open source software framework for
cloud computing that implements Infrastructure as a Service
(IaaS). IaaS are systems that give users the ability to run and
control entire virtual machine instances deployed across a
variety physical resources <a class="reference internal" href="#paper-eucalyptus" id="id512">[443]</a>. Eucalyptus
is an acronym for Elastic Utility Computing Architecture for
Linking Your Programs to Useful Systems.</p>
<p>A Eucalyptus private cloud is deployed on an enterprises data
center infrastructure and is accessed by users over the
enterprises intranet. Sensitive data remains entirely secure
from external interference behind the enterprise firewall
<a class="reference internal" href="#www-eucalyptus" id="id513">[444]</a>.</p>
</li>
<li><p class="first">Nimbus</p>
<p>Nimbus Infrastructure <a class="reference internal" href="#www-nimbus-wiki" id="id514">[445]</a> is an open source
IaaS implementation. It allows deployment of self-configured
virtual clusters and it supports configuration of scheduling,
networking leases, and usage metering.</p>
<p>Nimbus Platform <a class="reference internal" href="#www-nimbus" id="id515">[446]</a> provides an integrated set of
tools which enable users to launch large virtual clusters as well
as launch and monitor the cloud apps. It also includes service
that provides auto-scaling and high availability of resources
deployed over multiple IaaS cloud.  The Nimubs Platform tools are
cloudinit.d, Phantom and Context Broker.  In this paper
<a class="reference internal" href="#nimbus-paper" id="id516">[447]</a>, the use of Nimbus Phantom
to deploy auto-scaling solution across multiple NSF FutureGrid
clouds is explained. In this implementation Phantom was responsible
for deploying instances across multiple clouds and monitoring those
instance.  Nimbus platform supports Nimbus, Open Stack, Amazon
and several other clouds.</p>
</li>
<li><p class="first">CloudStack</p>
<p>Apache CloudStack is open source software designed to deploy and
manage large networks of virtual machines, as a highly available,
highly scalable Infrastructure as a Service (IaaS) cloud
computing platform. It uses existing hypervisors such as KVM,
VMware vSphere, and XenServer/XCP for virtualization. In addition
to its own API, CloudStack also supports the Amazon Web Services
(AWS) API and the Open Cloud Computing Interface from the Open
Grid Forum. <a class="reference internal" href="#www-cloudstack" id="id517">[448]</a></p>
<p>ColudStack features like built-in high-availability for hosts
and VMs, AJAX web GUI for management, AWS API compatibility,
Hypervisor agnostic, snapshot management, usage metering, network
management (VLAN&#8217;s, security groups), virtual routers, firewalls,
load balancers and multi-role support. <a class="reference internal" href="#www-cloudstack2" id="id518">[449]</a></p>
</li>
<li><p class="first">CoreOS</p>
<p><a class="reference internal" href="#www-core" id="id519">[450]</a> states that CoreOS is a linux operating system
used for clustered deployments. CoreOS allows applications to
run on containers. CoreOS can be run on clouds, virtual or
physical servers. CoreOS allows the ability for automatic software
updates inorder to make sure containers in cluster are secure and
reliable. It also makes managing large cluster environements
easier. CoreOS provides open source tools like CoreOS Linux,
etcd,rkt and flannel. CoreOS also has commercial products
Kubernetes and CoreOS stack. In CoreOS linux service
discovery is achieved by etcd, applications are run on Docker and
process management is achieved by fleet.</p>
</li>
<li><p class="first">rkt</p>
<p>rkt is an container manager developed by CoreOS <a class="reference internal" href="#www-coreos" id="id520">[451]</a>
designed for Linux clusters. It is an alternative for Docker
runtime and is designed for server environments with high
security and composibity requirement. It is the first
implementation of the open container standard called
&#8220;App Container&#8221; or &#8220;appc&#8221; specification but not the only one.
It is a standalone tool that lives outside of the core operating
system and can be used on variety of platforms such as Ubuntu,
RHEL, CentOS, etc. rkt implements the facilities specified by
the App Container as a command line tool. It allows execution
of App Containers with pluggable isolation and also varying
degrees of protection. Unlike Docker, rkt runs containers as
un-priviliged users making it impossible for attackers to break
out of the containers and take control of the entire physical
server. rkt&#8217;s primary interface comprises a single executable
allowing it easily integrate with existing init systems and
also advanced cluster environments. rkt is open source and is
written in the Go programming language <a class="reference internal" href="#www-github-rkt" id="id521">[452]</a>.</p>
</li>
<li><p class="first">VMware ESXi</p>
<p>VMware ESXi (formerly ESX) is an enterprise-class, type-1
hypervisor developed by VMware for deploying and serving virtual
computers <a class="reference internal" href="#wiki-vmwareesxi" id="id522">[453]</a>. The name ESX originated as an
abbreviation of Elastic Sky X. ESXi installs directly onto your
physical server enabling it to be partitioned into multiple
logical servers referred to as virtual machines.  Management of
VMware ESXi is done via APIs. This allows for an agent-less
approach to hardware monitoring and system management. VMware
also provides remote command lines, such as the vSphere Command
Line Interface (vCLI) and PowerCLI, to provide command and
scripting capabilities in a more controlled manner. These remote
command line sets include a variety of commands for
configuration, diagnostics and troubleshooting. For low-level
diagnostics and the initial configuration, menu-driven and
command line interfaces are available on the local console of the
server <a class="reference internal" href="#vmware-esxi" id="id523">[454]</a>.</p>
</li>
<li><p class="first">vSphere and vCloud</p>
<p>vSphere was developed by VMware and is a cloud computing
virtualization platform. <a class="reference internal" href="#www-vmware" id="id524">[455]</a>  vSphere is not
one piece of software but a suite of tools that contains software
such as vCenter, ESXi, vSphere client and a number of other
technologies.  ESXi server is a type 1 hypervisor on a physical
machine of which all virtual machines are installed.
The vSphere client then allows administrators to connect to
the ESXi and manage the virtual machines.  The vCenter server
is a virtual machine that is also installed on the
ESXi server which is used in environments when multiple ESXi
servers areexist.  Similarly, vCloud is also a suite of
applications but for establishing an infrastructure for a
private cloud. <a class="reference internal" href="#www-mustbegeek" id="id525">[456]</a>  The suite includes the
vsphere suite, but also contains site recovery management for
disaster recovery,  site networking and security.  Additionally,
a management suite that can give a visual of the infrastructure
to determine where potential issues might arise.</p>
</li>
<li><p class="first">Amazon</p>
<p>Amazons AWS (Amazon Web Services) is a provider of Infrastructure
as a Service (IaaS) on cloud. It provides a broad set of infrastructure
services, such as computing, data storage, networking and databases.
One can leverage AWS services by creating an account with AWS and then
creating a virtual server, called as an instance, on the AWS cloud.
In this instance you can select the hard disk volume, number of CPUs
and other hardware configuration based on your application needs.
You can also select operating system and other software required
to run your application. AWS lets you select from the countless services.
Some of them are mentioned below:</p>
<ul class="simple">
<li>Amazon Elastic Computer Cloud (EC2)</li>
<li>Amazon Simple Storage Service (Amazon S3)</li>
<li>Amazon CloudFront</li>
<li>Amazon Relational Database Service (Amazon RDS)</li>
<li>Amazon SimpleDB</li>
<li>Amazon Simple Notification Service (Amazon SNS)</li>
<li>Amazon Simple Queue Service (Amazon SQS)</li>
<li>Amazon Virtual Private Cloud (Amazon VPC)</li>
</ul>
<p>Amazon EC2 and Amazon S3 are the two core IaaS services, which are
used by cloud application solution developers worldwide. :cite:&#8217;www-aws&#8217;</p>
<p><strong>Improve: all of them need bibentries</strong></p>
</li>
<li><p class="first">Azure</p>
</li>
<li><p class="first">Google and other public Clouds</p>
</li>
<li><p class="first">Networking: Google Cloud DNS</p>
<p>Under the umbrella of google cloud platform, helps user to publish their domain using
Googles infrastructure. It is highly scalable, low latency, high availability DNS
service residing on infrastructure same as google.</p>
<p>It is build around projects a resource container, domain for access control, and billing
configuration. Managed zones holds records for same DNS name. The resource record sets
collection holds current state of the DNS that make up managed zones it is unmodifiable
or cannot be modified easily and changes to record sets. It supports A address records,
AAAA IPv6, CAA Certificate authority, CNAME canonical name, MX mail exchange,
NAPTR naming authority pointer, NS Name server record, SOA start of authority,
SPF Sender policy framework, SRV service locator, TXT text record.</p>
</li>
<li><p class="first">Amazon Route 53</p>
</li>
</ol>
</div>
<div class="section" id="cross-cutting-functions">
<h2>Cross-Cutting Functions<a class="headerlink" href="#cross-cutting-functions" title="Permalink to this headline"></a></h2>
<div class="section" id="monitoring">
<h3>Monitoring<a class="headerlink" href="#monitoring" title="Permalink to this headline"></a></h3>
<ol class="arabic" start="361">
<li><p class="first">Ambari</p>
<p>Apache Amabari is an open source platform that enables easy
management and maintenance of Hadoop clusters, regardless of
cluster size. Ambari has a simplified Web UI and robust REST API
for automating and controlling cluster operations.
<a class="reference internal" href="#www-hortonworks-ambari" id="id526">[457]</a> illustrates Ambari to provide key
benefits including easy installation, configuration, and
management with features such as Smart Configs and cluster
recommendations and Ambari Blueprints, to provide repeatable and
automated cluster creation. Ambari provides a centralized
security setup that automates security capabilities of
clusters. Ambari provides a holistic view for cluster monitoring
and provides visualizations for operation
metrics. <a class="reference internal" href="#www-ambari" id="id527">[458]</a> provides documentation about Ambari,
including a quick start guide for installing a cluster with
Ambari. <a class="reference internal" href="#www-github-ambari" id="id528">[459]</a> provides the project documents
for ambari on github.</p>
</li>
<li><p class="first">Ganglia</p>
</li>
<li><p class="first">Nagios <a class="reference internal" href="#www-nagios" id="id529">[460]</a></p>
<p>Nagios is a platform, which provides a set of software for
network infrastructure monitoring. It also offers administrative
tools to diagnose when failure events happen, and to notify
operators when hardware issues are detected. Specifically,
illustrates that Nagios is consist of modules including
<a class="reference internal" href="#nagios-book" id="id530">[461]</a>: a core and its dedicated tool for core
configuration, extensible plugins and its frontend. Nagios core
is designed with scalability in mind.  Nagios contains a
specification language allowing for building an extensible
monitoring systems.  Through the Nagios API components can
integrate with the Nagios core services. Plugins can be developed
via static languages like C or script languages. This mechanism
empowers Nagios to monitor a large set of various scenarios yet
being very flexible. <a class="reference internal" href="#nagios-paper-2012" id="id531">[462]</a> Besides its open
source components, Nagios also has commercial products to serve
needing clients.</p>
</li>
<li><p class="first">Inca</p>
<p>Inca is a grid monitoring <a class="reference internal" href="#inca-book" id="id532">[463]</a> software suite. It
provides grid monitoring features. These monitoring features
provide operators failure trends, debugging support, email
notifications, environmental issues etc. <a class="reference internal" href="#www-inca" id="id533">[464]</a>. It
enables users to automate the tests which can be executed on a
periodic basis. Tests can be added and configured as and when
needed. It helps users with different portfolios like system
administrators, grid operators, end users etc Inca provides
user-level grid monitoring. For each user it stores results as
well as allows users to deploy new tests as well as share the
results with other users. The incat web ui allows users to view
the status of test, manage test and results. The architectural
blocks of inca include report repository, agent, data consumers
and depot. Reporter is an executable program which is used to
collect the data from grid source. Reporters can be written in
perl and python. Inca repository is a collection of pre build
reporters.  These can be accessed using a web url. Inca
repository has 150+ reporters available. Reporters are versioned
and allow automatic updates. Inca agent does the configuration
management. Agent can be managed using the incat web ui. Inca
depot provides storage and archival of reports. Depot uses
relational database for this purpose. The database is accessed
using hibernate backend.  Inca web UI or incat provides real time
as well as historical view of inca data.  All communication
between inca components is secured using SSL certificates. It
requires user credentials for any access to the
system. Credentials are created at the time of the setup and
installation. Inca&#8217;s performance has been phenomenal in
production deployments. Some of the deployments are running for
more than a decade and has been very stable. Overall Inca
provides a solid monitoring system which not only monitors but
also detects problems very early on.</p>
</li>
</ol>
</div>
<div class="section" id="security-privacy">
<h3>Security &amp; Privacy<a class="headerlink" href="#security-privacy" title="Permalink to this headline"></a></h3>
<ol class="arabic" start="365">
<li><p class="first">InCommon</p>
</li>
<li><p class="first">Eduroam <a class="reference internal" href="#www-eduroam" id="id534">[465]</a></p>
<p>Eduroam is an initiative started in the year 2003 when the number
of personal computers with in the academia are growing
rapidly. The goal is to solve the problem of secure access to
WI-FI due to increasing number of students and reasearch teams
becoming mobile which was increasing the administrative problems
for provide access to WI-FI. Eduroam provides any user from an
eduroam participating site to get network access at any
instituion connected through eduroam. According to the
orgnizatioin it uses a combination of radius-based infrastructuor
with 802.1X standard techonology to provide roaming acess across
reasearch and educational networks. The role of the RADIUS
hierarchy is to forward user crednetials to the users home
instituion where they can be verified. This proved to be a
successful solution when compared to other traditonal ways like
using MAC-adress, SSID, WEP, 802.1x(EAP-TLS, EAP-TTLS), VPN
Clients, Mobile-IP etc which have their own short comings when
used for this purpose <a class="reference internal" href="#eduroam-paper-2005" id="id535">[466]</a>. Today by
enabling eduroam users get access to internet across 70 countries
and tens of thousands of access points worldwide.</p>
</li>
<li><p class="first">OpenStack Keystone</p>
<p><a class="reference internal" href="#www-keystone-wiki" id="id536">[467]</a> Keystone is the identity service used by
OpenStack for authentication (authN) and high-level authorization (authZ).
There are two authentication mechanisms in Keystone, UUID token, and PKI.
Universally unique identifier (UUID) is a 128-bit number used to identify
information (user). Each application&nbsp;after&nbsp;each request&nbsp;of the client
checks token validity online. PKI was introduced later and improved the
security of Keystone <a class="reference internal" href="#cui2015security" id="id537">[468]</a>. In PKI, each token has its
own&nbsp;digital signature&nbsp;that can be checked by any service and OpenStack
application with no necessity to ask for Keystone database <a class="reference internal" href="#www-cloudberrylab-kstn" id="id538">[469]</a>.</p>
<p>Thus, Keystone enables ensuring users identity with no need to transmit
its password to applications. It has recently been rearchitected to allow
for expansion to support proxying external services and AuthN/AuthZ
mechanisms such as oAuth, SAML and openID in future versions <a class="reference internal" href="#www-keystone" id="id539">[470]</a>.</p>
</li>
<li><p class="first">LDAP</p>
<p>LDAP stands for Lightweight Directory Access Protocol. It is a software
protocol for enabling anyone to locate organizations, individuals, and
other resources such as files and devices in a network, whether on the
Internet or on corporate internet.
<a class="reference internal" href="#www-ldap" id="id540">[471]</a></p>
<p>LDAP is a &#8220;lightweight&#8221; (smaller amount of code) version of
Directory Access Protocol (DAP), which is part of X.500, a
standard for directory services in a network.  In a network, a
directory tells you where in the network something is located. On
TCP/IP networks (including the Internet), the domain name system
(DNS) is the directory system used to relate the domain name to a
specific network address (a unique location on the
network). However, you may not know the domain name. LDAP allows
you to search for an individual without knowing where they&#8217;re
located (although additional information will help with the
search).An LDAP directory can be distributed among many
servers. Each server can have a replicated version of the total
directory that is synchronized periodically.  An LDAP server is
called a Directory System Agent (DSA). An LDAP server that
receives a request from a user takes responsibility for the
request, passing it to other DSAs as necessary, but ensuring a
single coordinated response for the user.</p>
</li>
<li><p class="first">Sentry</p>
<p><a class="reference internal" href="#www-sentry" id="id541">[472]</a> &#8220;Apache Sentry is a granular, role-based authorization
module for Hadoop. Sentry provides the ability to control and enforce
precise levels of privileges on data for authenticated users and
applications on a Hadoop cluster. Sentry currently works out of the box
with Apache Hive, Hive Metastore/HCatalog, Apache Solr, Impala and HDFS
(limited to Hive table data). Sentry is designed to be a pluggable
authorization engine for Hadoop components. It allows the client to define
authorization rules to validate a user or applications access requests
for Hadoop resources. Sentry is highly modular and can support authorization
for a wide variety of data models in Hadoop.&#8221;</p>
</li>
<li><p class="first">Sqrrl</p>
</li>
<li><p class="first">OpenID</p>
<p>OpenID is an authentication protocol that allows users to log in
to different websites, which are not related, using the same
login credentials for each, i.e. without having to create
separate id and password for all the websites. The login
credentials used are of the existing account. The password is
known only to the identity provider and nobody else which
relieves the users concern about identity being known to an
insecure website. <a class="reference internal" href="#ope1" id="id542">[473]</a> It provides a mechanism that makes
the users control the information that can be shared among
multiple websites. OpenID is being adopted all over the web. Most
of the leading organizations including Microsoft, Facebook,
Google, etc. are accepting the OpenIDs <a class="reference internal" href="#ope2" id="id543">[474]</a>. It is an
open source and not owned by anyone. Anyone can use OpenID or be
an OpenID provider and there is no need for an individual to be
approved.</p>
</li>
<li><p class="first">SAML OAuth</p>
<p>As explained in <a class="reference internal" href="#saml" id="id544">[475]</a>, Security Assertion Markup Language
(SAML) is a secured XML based communication mechanism for
communicating identities between organizations. The primary use
case of SAML is Internet SSO. It eliminates the need to maintain
multiple authentication credentials in multiple locations. This
enhances security by elimination opportunities for identity
theft/Phishing. It increases application access by eliminating
barriers to usage. It reduces administration time and cost by
excluding the effort to maintain duplicate credentials and
helpdesk calls to reset forgotten passwords. Three entities of
SAML are the users, Identity Provider (IdP-Organization that
maintains a directory of users and an authentication mechanism)
and Service Provider(SP-Hosts the application /service). User
tries to access the application by clicking on a link or through
an URL on the internet. The Federated identity software running
in the IdP validates the users identity and the user is then
authenticated. A specifically formatted message is then
communicated to the federated identity software running at SP. SP
creates a session for the user in the target application and
allows the user to get direct access once it receives the
authorization message from a known identity provider.</p>
</li>
</ol>
</div>
<div class="section" id="distributed-coordination">
<h3>Distributed Coordination<a class="headerlink" href="#distributed-coordination" title="Permalink to this headline"></a></h3>
<ol class="arabic" start="373">
<li><p class="first">Google Chubby</p>
<p>Chubby Distributed lock service <a class="reference internal" href="#www-chubby" id="id545">[476]</a>
is intended for use within a loosely-coupled distributed system
consisting of moderately large numbers of small machines
connected by a high-speed network. Asynchronous consensus is
solved by the Paxos protocol. The implementation in Chubby is
based on coarse grained lock server and a library that the client
applications link against.  As per the 2016 paper
<a class="reference internal" href="#chubby-paper-2016" id="id546">[477]</a>, an open-source implementation of the
Google Chubby lock service was provided by the Apache ZooKeeper
project. ZooKeeper used a Paxos-variant protocol Zab for solving
the distributed consensus problem.  Google stack and Facebook
stack both use versions of zookeeper.</p>
</li>
<li><p class="first">Zookeeper</p>
<p>Zookeeper provides coordination services to distributed applications.
It includes synchronization, configuration management and naming
services among others. The interfaces are available in Java and C
<a class="reference internal" href="#www-zoo-overiew" id="id547">[478]</a>. The services themselves can be distributed
across multiple Zookeeper servers to avoid single point of failure.
If the leader fails to answer, the clients can fall-back to other
nodes. The state of the cluster is maintained in an in-memory image
along with a persistent storage file called znode by each server. The
cluster namespace is maintained in a hierarchical order. The changes to the
data are totally ordered <a class="reference internal" href="#www-zoo-wiki" id="id548">[479]</a> by stamping each update
with a number. Clients can also set a watch on a znode to be notified
of any change <a class="reference internal" href="#www-zoo-ibm" id="id549">[480]</a>. The performance of the ZooKeeper
is optimum for &#8220;read-dominant&#8221; workloads. It&#8217;s maintained by Apache
and is open-source.</p>
</li>
<li><p class="first">Giraffe</p>
<p>Giraffe is a scalable distributed coordination
service. Distributed coordination is a media access technique
used in distributed systems to perform functions like providing
group membership, gaining lock over resources, publishing,
subscribing, granting ownership and synchronization together
among multiple servers without issues. Giraffe was proposed as
alternative to coordinating services like Zookeeper and Chubby
which were efficient only in read-intensive scenario and small
ensembles. To overcome this three important aspects were included
in the design of Giraffe <a class="reference internal" href="#giraffepaper" id="id550">[481]</a>. First feature is
Giraffe uses interior-node joint trees to organize coordination
servers for better scalability. Second, Giraffe uses Paxos
protocol for better consistency and to provide more
fault-tolerance. Finally, Giraffe also facilitates hierarchical
data organization and in-memory storage for high throughput and
low latency.</p>
</li>
<li><p class="first">JGroups</p>
</li>
</ol>
</div>
<div class="section" id="message-and-data-protocols">
<h3>Message and Data Protocols<a class="headerlink" href="#message-and-data-protocols" title="Permalink to this headline"></a></h3>
<ol class="arabic" start="377">
<li><p class="first">Avro</p>
</li>
<li><p class="first">Thrift</p>
<p>The Apache Thrift software framework, for scalable cross-language services
development, combines a software stack with a code generation engine to
build services that work efficiently and seamlessly between C++, Java,
Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js,
Smalltalk, OCaml and Delphi and other languages. It includes a complete
stack for creating clients and servers. It includes a server infrastructure
to tie the protocols and transports together. There are blocking,
non-blocking, single and multithreaded servers available.</p>
<p>Thrift was originally developed at Facebook, it was open sourced in April
2007 and entered the Apache Incubator in May, 2008. It became an Apache TLP
in October, 2010. <a class="reference internal" href="#git-thrift" id="id551">[482]</a></p>
</li>
<li><p class="first">Protobuf</p>
<p>Protocol Buffer <a class="reference internal" href="#www-protobuf" id="id552">[483]</a> is a way to serialize
structured data into binary form (stream of bytes) in order to
transfer it over wires or for storage. It is used for inter
apllication communication or for remote procedure call (RPC). It
involves a interface description that describes the structure of
some data and a program that can generate source code or parse it
back to the binary form. It emphasizes on simplicity and
performance over xml. Though xml is more readable but requires
more resources in parsing and storing.  This is developed by
Google and available under open source licensing. The parser
program is available in many languages including java and python.</p>
</li>
</ol>
</div>
</div>
<div class="section" id="new-technologies-to-be-integrated">
<h2>New Technologies to be integrated<a class="headerlink" href="#new-technologies-to-be-integrated" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="382">
<li><p class="first">Snort</p>
<p><a class="reference internal" href="#www-snort" id="id553">[484]</a> Snort is a Network Intrusion Prevention System (NIPS) and
Network Intrusion Detection System (NIDS). Snort&#8217;s open source network-based
intrusion detection system (NIDS) has the ability to perform real-time traffic
analysis and packet logging on Internet Protocol (IP) networks. Snort performs
protocol analysis, content searching and matching. These basic services have
many purposes including application-aware triggered quality of service,
to de-prioritize bulk traffic when latency-sensitive applications are in use.
The program can also be used to detect probes or attacks, including, but not
limited to, operating system fingerprinting attempts, common gateway interface,
buffer overflows, server message block probes, and stealth port scans.
Snort can be configured in three main modes: sniffer, packet logger, and
network intrusion detection. In sniffer mode, the program will read
network packets and display them on the console. In packet logger mode,
the program will log packets to the disk. In intrusion detection mode,
the program will monitor network traffic and analyze it against a rule set
defined by the user. The program will then perform a specific action based on
what has been identified.</p>
</li>
<li><p class="first">Fiddler</p>
<p>Fiddler is an HTTP debugging proxy server application. Fiddler captures HTTP
and HTTPS traffic and logs it for the user to review by implementing
man-in-the-middle interception using self-signed certificates. Fiddler can also
be used to modify (&#8220;fiddle with&#8221;) HTTP traffic for troubleshooting purposes as
it is being sent or received.[5] By default, traffic from Microsoft&#8217;s WinINET
HTTP(S) stack is automatically directed to the proxy at runtime, but any browser
or Web application (and most mobile devices) can be configured to route its traffic
through Fiddler <a class="reference internal" href="#www-fiddler" id="id554">[485]</a>.</p>
</li>
<li><p class="first">Zeppelin</p>
<p>Apache Zeppelin <a class="reference internal" href="#www-zeppelinwebsite" id="id555">[486]</a> provides an
interactive environment for big data data analytics on
applications using distributed data processing systems like
Hadoop and Spark. It supports various tasks like data ingestion,
data discovery, data visualization, data analytics and
collaboration. Apache Zeppelin provides built-in Apache Spark
integration and is compatible with many languages/data-processing
backends like Python, R, SQL, Cassandra and JDBC. It also
supports adding new language backend. Zeppelin also lets users to
collaborate by sharing their Notebooks, Paragraph and has option
to broadcast any changes in realtime.</p>
</li>
<li><p class="first">TBA</p>
</li>
</ol>
</div>
<div class="section" id="excersise">
<span id="techs-exercise"></span><h2>Excersise<a class="headerlink" href="#excersise" title="Permalink to this headline"></a></h2>
<dl class="docutils">
<dt>TechList.1: In class you will be given an HID and you will be assigned</dt>
<dd><p class="first">a number of technologies that you need to research and create a
summary as well as one or more relevant references to be added to the
Web page. All technologies for TechList.1 are marked with a (1)
behind the technology.  An example text is given for Nagios in this
page.  Please create a pull request with your responses. You are
responsible for making sure the request shows up and each commit is
using gitchangelog in the commit message:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">new</span><span class="p">:</span><span class="n">usr</span><span class="p">:</span> <span class="n">added</span> <span class="n">paragraph</span> <span class="n">about</span> <span class="o">&lt;</span><span class="n">PUTTECHHERE</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>You can create one or more pull requests for the technology and the
references. We have created in the referens file a placeholder using
your HID to simplify the management of the references while avoiding
conflicts.  For the technologies you are responsible to invesitgate
them and write an academic summary of the technology. Make sure to
add your reference to refs.bib.  Many technologies may have
additional references than the Web page. Please add the most
important once while limiting it to three if you can. Avoid
plagearism and use proper quotations or better rewrite the text.</p>
<p>You must look at <a class="reference internal" href="technologies-hw.html"><span class="doc">Completing Techlist Assignments</span></a> to sucessfully complete the
homework</p>
<p>A video about this hoemwork is posted at
<a class="reference external" href="https://www.youtube.com/watch?v=roi7vezNmfo">https://www.youtube.com/watch?v=roi7vezNmfo</a> showing how to
do references in emacs and jabref, it shows you how to configure
git, it shows you how to do the fork request while asking you to add
&#8220;new:usr ....&#8221; to the commit messages). As this is a homework
realated video we put a lot of information in it that is not only
useful for beginners. We recommend you watch it.</p>
<p>This homework can be done in steps. First you can collect all the
content in an editor. Second you can create a fork. Third you can
add the new content to the fork. Fourth you can commit. Fith you
can push. Six if the TAs have commend improve. The commit message
must have new:usr: at the beginning.</p>
<p>While the Nagios entry is a good example (make sure grammer is ok
the Google app engine is an example for a bad entry.</p>
<p class="last">Do Techlist 1.a 1.b 1.c first. We  will assign Techlist 1.d and
TechList 2 in February.</p>
</dd>
<dt>TechList.1.a:</dt>
<dd>Complete the pull request with the technologies assigned to you.
Details for the assignment are posted in Piazza. Search for TechList.</dd>
<dt>TechList.1.b: Identify how to cite. We are using &#8220;scientific&#8221; citation</dt>
<dd>formats such as IEEEtran, and ACM. We are <strong>not</strong> using citation
formats such as Chicago, MLA, or ALP. The later are all for non
scientific publications and thus of no use to us. Also when writing
about a technology do not use the names of the person, simply say
something like. In [1] the definition of a turing machine is given
as follows, ...  and do not use elaborate sentences such as: In his
groundbraking work conducted in England, Allan Turing, introduced
the turing machine in the years 1936-37 [2]. Its definition is base
on ... The difference is clear, while the first focusses on results
and technological concepts, the second introduces a colorful
description that is more suitable for a magazine or a computer
history paper.</dd>
<dt>TechList 1.c:</dt>
<dd>Learn about Plagearism and how to avoid it.
Many Web pages will conduct self advertisement while adding
suspicious and subjective adjectives or phrases such as cheaper,
superior, best, most important, with no equal, and others that you
may not want to copy into your descriptions. Please focus on facts
not on what the author of the Web page claims.</dd>
<dt>TechList 1.d:</dt>
<dd>Identify technologies from the Apache project or other
Big Data related Web pages and projects that are not yet listed here
and add the name and descriptions as well as references and that you
find important.</dd>
<dt>TechList.2:</dt>
<dd>In this hopweork we provide you with additional technologies
that you need to compleate They are marked with (2) in the HID
assignment.</dd>
<dt>TechList.3:</dt>
<dd>Identify technologies that are not listed here and add
them. Provide a description and a refrence just as you did before.
Make sure duplicated entries will be merged. Before you start doing a
technology to avoid adding technologies that have already been done by
others.</dd>
</dl>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<p id="bibtex-bibliography-i524/technologies-0"><table class="docutils citation" frame="void" id="www-bpel-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Bpel wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Business_Process_Execution_Language">https://en.wikipedia.org/wiki/Business_Process_Execution_Language</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ode-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[2]</a></td><td>Ode wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Apache_ODE">https://en.wikipedia.org/wiki/Apache_ODE</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ode-web" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[3]</a></td><td>Ode website. Web Page. URL: <a class="reference external" href="http://ode.apache.org/">http://ode.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bpel" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[4]</a></td><td>ActiveBPEL. Communicating with the activebpel server administration interface via web services. Web Page, February 2017. Accessed: 2017-02-12. URL: <a class="reference external" href="http://www.activevos.com/content/developers/education/sample_active_bpel_admin_api/doc/index.html">http://www.activevos.com/content/developers/education/sample_active_bpel_admin_api/doc/index.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-pegasus" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[5]</a></td><td>Pegasus. Web Page. Accessed:2/3/2017. URL: <a class="reference external" href="https://pegasus.isi.edu/">https://pegasus.isi.edu/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kepler" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[6]</a></td><td>Kepler Project. The kepler project. Web Page, February 2017. Accessed: 2017-02-12. URL: <a class="reference external" href="https://kepler-project.org">https://kepler-project.org</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-taverna" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id7">[7]</a></td><td>Taverna. Web Page. URL: <a class="reference external" href="https://taverna.incubator.apache.org/introduction/">https://taverna.incubator.apache.org/introduction/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="taverna-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id8">[8]</a></td><td><em>Taverna Workflows: Syntax and Semantics</em>, IEEE, December 2007. URL: <a class="reference external" href="http://ieeexplore.ieee.org/document/4426917/">http://ieeexplore.ieee.org/document/4426917/</a>, <a class="reference external" href="https://doi.org/10.1109/E-SCIENCE.2007.71">doi:10.1109/E-SCIENCE.2007.71</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="trianadocumentation-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id9">[9]</a></td><td>Triana documentation. Web Page, 2012. Accessed: 2017-2-20. URL: <a class="reference external" href="http://www.trianacode.org/">http://www.trianacode.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-trident-tutorial" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[10]</td><td><em>(<a class="fn-backref" href="#id10">1</a>, <a class="fn-backref" href="#id11">2</a>, <a class="fn-backref" href="#id13">3</a>)</em> Trident tutorial. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://storm.apache.org/releases/0.10.1/Trident-tutorial.html">http://storm.apache.org/releases/0.10.1/Trident-tutorial.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-trident-overview" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id12">[11]</a></td><td>Trident api overview. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://storm.apache.org/releases/1.0.0/Trident-API-Overview.html">http://storm.apache.org/releases/1.0.0/Trident-API-Overview.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-biokepler" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id14">[12]</a></td><td>bioKepler. What is biokepler. WebPage. URL: <a class="reference external" href="http://www.biokepler.org/faq#what-is-biokepler">http://www.biokepler.org/faq#what-is-biokepler</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-biokepler-demos" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id15">[13]</a></td><td>bioKepler. Demo workflow. WebPage. URL: <a class="reference external" href="http://www.biokepler.org/userguide#demos">http://www.biokepler.org/userguide#demos</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="bioactors" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id16">[14]</a></td><td>Weizhong Li. Introduction to bioactors. In bioKepler, editor, <em>bioKepler Tools and Its Applications</em>, number&nbsp;1 in 1st Workshop on bioKepler Tools and Its Applications, 31. San Diego Supercomputer Center 10100 Hopkins Dr, La Jolla, CA 92093, September 2012. UCSD;SDSC, bioKepler. URL: <a class="reference external" href="http://www.biokepler.org/sites/swat.sdsc.edu.biokepler/files/workshops/2012-09-05/slides/2012-09-05-02-Li.pdf">http://www.biokepler.org/sites/swat.sdsc.edu.biokepler/files/workshops/2012-09-05/slides/2012-09-05-02-Li.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-galaxy-ansible" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id17">[15]</a></td><td>About ansible galaxy. Web Page. Accessed: 2017-2-06. URL: <a class="reference external" href="https://galaxy.ansible.com/intro/">https://galaxy.ansible.com/intro/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ansible-book-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id18">[16]</a></td><td>Michael Heap. <em>Ansible From Beginner to Pro</em>. apress, 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-github-galaxy" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id19">[17]</a></td><td>GitHub ansible/ galaxy. Web Page. Accessed: 2017-2-06. URL: <a class="reference external" href="https://github.com/ansible/galaxy/">https://github.com/ansible/galaxy/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-naiad" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id20">[18]</a></td><td>Derek&nbsp;G. Murray, Frank McSherry, Rebecca Isaacs, Michael Isard, Paul Barham, and Martin Abadi. Naiad: a timely dataflow system. In <em>Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</em>, SOSP &#8216;13, 439455. New York, NY, USA, 2013. ACM. Accessed: 2017-1-27. URL: <a class="reference external" href="http://doi.acm.org/10.1145/2517349.2522738">http://doi.acm.org/10.1145/2517349.2522738</a>, <a class="reference external" href="https://doi.org/10.1145/2517349.2522738">doi:10.1145/2517349.2522738</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-naiad" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id21">[19]</a></td><td>Chandu Thekkath. Naiad - microsoft research. webpage, October 2011. Accessed: 2017-1-27. URL: <a class="reference external" href="https://www.microsoft.com/en-us/research/project/naiad/">https://www.microsoft.com/en-us/research/project/naiad/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-oozie" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[20]</td><td><em>(<a class="fn-backref" href="#id22">1</a>, <a class="fn-backref" href="#id25">2</a>, <a class="fn-backref" href="#id26">3</a>, <a class="fn-backref" href="#id27">4</a>)</em> Mohammad Islam, Angelo&nbsp;K Huang, Mohamed Battisha, Michelle Chiang, Santhosh Srinivasan, Craig Peters, Andreas Neumann, and Alejandro Abdelnur. Oozie: towards a scalable workflow management system for hadoop. In <em>Proceedings of the 1st ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies</em>, 4. ACM, 2012.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-oozie1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id23">[21]</a></td><td>Why Oozie? \textbar  Just a simple Hadoop DBA. Web Page. URL: <a class="reference external" href="https://prodlife.wordpress.com/2013/12/09/why-oozie/">https://prodlife.wordpress.com/2013/12/09/why-oozie/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-oozie2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id24">[22]</a></td><td>Oozie - Apache Oozie Workflow Scheduler for Hadoop. Web Page. URL: <a class="reference external" href="http://oozie.apache.org/">http://oozie.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cascading" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id28">[23]</a></td><td>Cascading. Web Page, 2017. URL: <a class="reference external" href="http://www.cascading.org/projects/cascading/">http://www.cascading.org/projects/cascading/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="e-science-central-paper-2010" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[24]</td><td><em>(<a class="fn-backref" href="#id29">1</a>, <a class="fn-backref" href="#id30">2</a>, <a class="fn-backref" href="#id32">3</a>, <a class="fn-backref" href="#id33">4</a>)</em> Hugo Hiden, Paul Watson, Simon Woodman, and David Leahy. E-science central: cloud-based e-science and its application to chemical property modelling. In <em>University of Newcastle upon Tyne, Computing Science, Technical Report Series, No. CS-TR-1227</em>. University of Newcastle upon Tyne, November 2010.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-e-science-central" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id31">[25]</a></td><td>Escience central overview. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://bitbucket.org/digitalinstitute/esciencecentral/">https://bitbucket.org/digitalinstitute/esciencecentral/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="azure-df" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id34">[26]</a></td><td>Archive. Azure_df. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="http://www.jamesserra.com/archive/2014/11/what-is-azure-data-factory/">http://www.jamesserra.com/archive/2014/11/what-is-azure-data-factory/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="azure-ms" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id35">[27]</a></td><td>Microsoft. Azure_ms. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/data-factory/data-factory-introduction">https://docs.microsoft.com/en-us/azure/data-factory/data-factory-introduction</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-dataflow" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id36">[28]</a></td><td>Google. Cloud dataflow. WebPage. URL: <a class="reference external" href="https://cloud.google.com/dataflow/">https://cloud.google.com/dataflow/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-googlelivestream" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id37">[29]</a></td><td>Jacob Jackson. Google service analyzes live streaming data. WebPage, June 2014. URL: <a class="reference external" href="http://www.infoworld.com/article/2607938/data-mining/google-service-analyzes-live-streaming-data.html">http://www.infoworld.com/article/2607938/data-mining/google-service-analyzes-live-streaming-data.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-nifi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id38">[30]</a></td><td>Apachi nifi. Web Page. URL: <a class="reference external" href="https://nifi.apache.org">https://nifi.apache.org</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hortanworks" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id39">[31]</a></td><td>Apache nifi (aka hdf) data flow across data center. Web Page. URL: <a class="reference external" href="https://community.hortonworks.com/articles/9933/apache-nifi-aka-hdf-data-flow-across-data-center.html">https://community.hortonworks.com/articles/9933/apache-nifi-aka-hdf-data-flow-across-data-center.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-forbes" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id40">[32]</a></td><td>Nsa &#8216;nifi&#8217; big data automation project out in the open. Web Page. URL: <a class="reference external" href="http://www.forbes.com/sites/adrianbridgwater/2015/07/21/nsa-nifi-big-data-automation-project-out-in-the-open/#6b37cac55d9a">http://www.forbes.com/sites/adrianbridgwater/2015/07/21/nsa-nifi-big-data-automation-project-out-in-the-open/#6b37cac55d9a</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="datasheet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id41">[33]</a></td><td><em>Integration Made Easy - Jitterbit</em>. Jitterbit,Inc., 1 Kaiser Plaza Suite 701 Oakland, CA 94612. Accessed: 2017-1-26. URL: <a class="reference external" href="http://www.jitterbit.com/Files/Product/Jitterbit-General-Datasheet.pdf">http://www.jitterbit.com/Files/Product/Jitterbit-General-Datasheet.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jitetl" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id42">[34]</a></td><td>Jitterbit. Data integration and etl | jitterbit | integrate data from any source. Web Page. accessed: 2017-1-26. URL: <a class="reference external" href="https://www.jitterbit.com/etl-data-integration/">https://www.jitterbit.com/etl-data-integration/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tech-manual" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id43">[35]</a></td><td><em>Technical Overview - Jitterbit</em>. Jitterbit,Inc, 2011. Accessed: 2017-1-26. URL: <a class="reference external" href="http://www.jitterbit.com/Files/Product/JitterbitTechnicalOverview.pdf">http://www.jitterbit.com/Files/Product/JitterbitTechnicalOverview.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pent1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id44">[36]</a></td><td>Pentaho. webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Pentaho">https://en.wikipedia.org/wiki/Pentaho</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pent2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id45">[37]</a></td><td>Any analytics, any data, simplified. webpage. URL: <a class="reference external" href="http://www.pentaho.com/product/product-overview">http://www.pentaho.com/product/product-overview</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mahout" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id46">[38]</a></td><td>Apache mahout. Web Page. URL: <a class="reference external" href="http://mahout.apache.org/">http://mahout.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-datafu" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id47">[39]</a></td><td>Datafu. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="https://datafu.incubator.apache.org/">https://datafu.incubator.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-r" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id48">[40]</a></td><td>R: what is r? Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://www.r-project.org/about.html">https://www.r-project.org/about.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-r" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id49">[41]</a></td><td>Vignesh Prajapati. <em>Big data analytics with R and Hadoop</em>. Packt Publishing, 2013. ISBN 9781782163282.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-pbdr" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id50">[42]</a></td><td>G.&nbsp;Ostrouchov, W.-C. Chen, D.&nbsp;Schmidt, and P.&nbsp;Patel. Web Page, 2012. URL: <a class="reference external" href="http://r-pbd.org/">http://r-pbd.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="bioconductor-article-2004" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id51">[43]</a></td><td>Robert&nbsp;C. Gentleman, Vincent&nbsp;J. Carey, Douglas&nbsp;M. Bates, Ben Bolstad, Marcel Dettling, Sandrine Dudoit, Byron Ellis, Laurent Gautier, Yongchao Ge, Jeff Gentry, Kurt Hornik, Torsten Hothorn, Wolfgang Huber, Stefano Iacus, Rafael Irizarry, Friedrich Leisch, Cheng Li, Martin Maechler, Anthony&nbsp;J. Rossini, Gunther Sawitzki, Colin Smith, Gordon Smyth, Luke Tierney, Jean&nbsp;YH Yang, and Jianhua Zhang. Bioconductor: open software development for computational biology and bioinformatics. <em>Genome Biology</em>, 5(10):R80, 2004. URL: <a class="reference external" href="http://dx.doi.org/10.1186/gb-2004-5-10-r80">http://dx.doi.org/10.1186/gb-2004-5-10-r80</a>, <a class="reference external" href="https://doi.org/10.1186/gb-2004-5-10-r80">doi:10.1186/gb-2004-5-10-r80</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bioconductor-about" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id52">[44]</a></td><td>About bioconductor. Web Page. Accessed: 2017-02-10. URL: <a class="reference external" href="https://www.bioconductor.org/about/">https://www.bioconductor.org/about/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-imagej" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id53">[45]</a></td><td>ImageJ. Imagej introduction. Web Page, February 2017. Accessed: 2017-02-12. URL: <a class="reference external" href="https://imagej.nih.gov/ij/docs/intro.html">https://imagej.nih.gov/ij/docs/intro.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-opencv" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id54">[46]</a></td><td>web page. URL: <a class="reference external" href="http://opencv.org/">http://opencv.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="opencv-version" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id55">[47]</a></td><td>web page. URL: <a class="reference external" href="http://opencv.org/opencv-3-2.html">http://opencv.org/opencv-3-2.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="git-scalapack" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id56">[48]</a></td><td>About scalapack. Web Page. Accessed: 2017-02-12. URL: <a class="reference external" href="http://www.netlib.org/scalapack/slug/">http://www.netlib.org/scalapack/slug/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id57">[49]</a></td><td>Alfredo Buttari, Julien Langou, Jakub Kurzak, and Jack Dongarra. A class of parallel tiled linear algebra algorithms for multicore architectures. <em>Parallel Computing</em>, 35(1):3853, 2009. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0167819108001117">http://www.sciencedirect.com/science/article/pii/S0167819108001117</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-plasma-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id58">[50]</a></td><td>PLASMA README. Web Page. URL: <a class="reference external" href="http://icl.cs.utk.edu/projectsfiles/plasma/html/README.html">http://icl.cs.utk.edu/projectsfiles/plasma/html/README.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id59">[51]</a></td><td>Jack Dongarra, Mark Gates, Azzam Haidar, Jakub Kurzak, Piotr Luszczek, Stanimire Tomov, and Ichitaro Yamazaki. Accelerating numerical dense linear algebra calculations with GPUs. In <em>Numerical Computations with GPUs</em>, pages 328. Springer, 2014. URL: <a class="reference external" href="http://link.springer.com/chapter/10.1007/978-3-319-06548-9_1">http://link.springer.com/chapter/10.1007/978-3-319-06548-9_1</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id60">[52]</a></td><td>Azzam Haidar, Jakub Kurzak, and Piotr Luszczek. An improved parallel singular value algorithm and its implementation for multicore hardware. In <em>Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis</em>, 90. ACM, 2013. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?id=2503292">http://dl.acm.org/citation.cfm?id=2503292</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id61">[53]</a></td><td>Jakub Kurzak, Hatem Ltaief, Jack Dongarra, and Rosa&nbsp;M. Badia. Scheduling dense linear algebra operations on multicore processors. <em>Concurrency and Computation: Practice and Experience</em>, 22(1):1544, 2010. URL: <a class="reference external" href="http://onlinelibrary.wiley.com/doi/10.1002/cpe.1467/full">http://onlinelibrary.wiley.com/doi/10.1002/cpe.1467/full</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id62">[54]</a></td><td>Stanimire Tomov, Jack Dongarra, and Marc Baboulin. Towards dense linear algebra for hybrid GPU accelerated manycore systems. <em>Parallel Computing</em>, 36(5):232240, 2010. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0167819109001276">http://www.sciencedirect.com/science/article/pii/S0167819109001276</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id63">[55]</a></td><td>Stanimire Tomov, Rajib Nath, Hatem Ltaief, and Jack Dongarra. Dense linear algebra solvers for multicore with GPU accelerators. In <em>Parallel &amp; Distributed Processing, Workshops and Phd Forum (IPDPSW), 2010 IEEE International Symposium on</em>, 18. IEEE, 2010. URL: <a class="reference external" href="http://ieeexplore.ieee.org/abstract/document/5470941/">http://ieeexplore.ieee.org/abstract/document/5470941/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azuremlsite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id64">[56]</a></td><td>C.J. Gronlund, Gary Ericson, Larry Francs, and Paulette McKay. Azure machine learning. Web Page, January 2017. Accessed: 2017-2-19. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-what-is-machine-learning#what-is-machine-learning-in-the-microsoft-azure-cloud">https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-what-is-machine-learning#what-is-machine-learning-in-the-microsoft-azure-cloud</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-prediction" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id65">[57]</a></td><td>Google. Google cloud prediction api documentation. WebPage. Accessed 2017-1-26. URL: <a class="reference external" href="https://cloud.google.com/prediction/docs/">https://cloud.google.com/prediction/docs/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-translation" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id66">[58]</a></td><td>Google. Google cloud translation api documentation. WebPage. URL: <a class="reference external" href="https://cloud.google.com/translate/docs/">https://cloud.google.com/translate/docs/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="dblp-journals-corr-abs-1202-6548" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id67">[59]</a></td><td>Davide Albanese, Roberto Visintainer, Stefano Merler, Samantha Riccadonna, Giuseppe Jurman, and Cesare Furlanello. Mlpy: machine learning python. <em>CoRR</em>, 2012. URL: <a class="reference external" href="http://arxiv.org/abs/1202.6548">http://arxiv.org/abs/1202.6548</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mlpy" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id68">[60]</a></td><td>Mlpy site. Web Page. URL: <a class="reference external" href="http://mlpy.sourceforge.net/docs/3.5/">http://mlpy.sourceforge.net/docs/3.5/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="scik1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id69">[61]</a></td><td>Jason Brownlee. A gentle introduction to scikit-learn: a python machine learning library. webpage, 2014. URL: <a class="reference external" href="http://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/">http://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="scik2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id70">[62]</a></td><td>Scikit-learn. webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Scikit-learn">https://en.wikipedia.org/wiki/Scikit-learn</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="comp1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[63]</td><td><em>(<a class="fn-backref" href="#id71">1</a>, <a class="fn-backref" href="#id72">2</a>)</em> Rudi Cilibrasi, Anna&nbsp;Lissa Cruz, Steven de&nbsp;Rooij, and Maarten Keijzer. What is complearn. webpage. URL: <a class="reference external" href="http://complearn.org/">http://complearn.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-caffe" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id73">[64]</a></td><td>Caffe-deep learning. Accessed: 02-06-2017. URL: <a class="reference external" href="http://caffe.berkeleyvision.org/">http://caffe.berkeleyvision.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-torch" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id74">[65]</a></td><td>Torch-machine learning. Accessed: 02-06-2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Torch_(machine_learning)">https://en.wikipedia.org/wiki/Torch_(machine_learning)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-theano" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id75">[66]</a></td><td>Theano. Web Page. Accessed: 2017-1-21. URL: <a class="reference external" href="http://deeplearning.net/software/theano/introduction.html">http://deeplearning.net/software/theano/introduction.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-dl4j" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id76">[67]</a></td><td>DL4j. Webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Deeplearning4j">https://en.wikipedia.org/wiki/Deeplearning4j</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-h20-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id79">[68]</a></td><td>H2O Wikipedia Documentation components. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/H2O_(software)l">https://en.wikipedia.org/wiki/H2O_(software)l</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibmwatson-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id80">[69]</a></td><td>Ibm watson. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Watson_(computer)">https://en.wikipedia.org/wiki/Watson_(computer)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibmwatson" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[70]</td><td><em>(<a class="fn-backref" href="#id81">1</a>, <a class="fn-backref" href="#id82">2</a>)</em> Ibm watson product page. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.ibm.com/watson">https://www.ibm.com/watson</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-pgx" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id83">[71]</a></td><td>Oracle labs pgx. Web Page. Accessed: 2017-02-07. URL: <a class="reference external" href="https://docs.oracle.com/cd/E56133_01/2.3.1/index.html">https://docs.oracle.com/cd/E56133_01/2.3.1/index.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ora" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id84">[72]</a></td><td>Parallel graph analytics (pgx). Web Page. Accessed: 2017-02-11. URL: <a class="reference external" href="http://www.oracle.com/technetwork/oracle-labs/parallel-graph-analytics/overview/index.html">http://www.oracle.com/technetwork/oracle-labs/parallel-graph-analytics/overview/index.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-graphlab" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id85">[73]</a></td><td>Graphlab. Webpage. Accessed: 2017-01-28. URL: <a class="reference external" href="http://www.select.cs.cmu.edu/code/graphlab/">http://www.select.cs.cmu.edu/code/graphlab/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-graphx" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id86">[74]</a></td><td>GraphX. Web Page. Accessed: 2017-01-30. URL: <a class="reference external" href="http://spark.apache.org/graphx/">http://spark.apache.org/graphx/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-graphx1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id87">[75]</a></td><td>Reynold&nbsp;S. Xin, Joseph&nbsp;E. Gonzalez, Michael&nbsp;J. Franklin, and Ion Stoica. Graphx: a resilient distributed graph system on spark. In <em>First International Workshop on Graph Data Management Experiences and Systems</em>, GRADES &#8216;13, 2:12:6. New York, NY, USA, 2013. ACM. URL: <a class="reference external" href="http://doi.acm.org/10.1145/2484425.2484427">http://doi.acm.org/10.1145/2484425.2484427</a>, <a class="reference external" href="https://doi.org/10.1145/2484425.2484427">doi:10.1145/2484425.2484427</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ibmsystemgdocumentation-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id88">[76]</a></td><td>Ibm system g documentation. Web Page, 2014. Accessed: 2017-2-19. URL: <a class="reference external" href="http://systemg.research.ibm.com/">http://systemg.research.ibm.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ibmsystemgdocumentation-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id89">[77]</a></td><td>Ibm system g documentation-2. Web Page, 2017. Accessed: 2017-2-17. URL: <a class="reference external" href="http://www.predictiveanalyticstoday.com/ibm-system-g-native-store/">http://www.predictiveanalyticstoday.com/ibm-system-g-native-store/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ibmsystemgpaper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id90">[78]</a></td><td>Ching&nbsp;Yung Lin. Graph computing and linked big data. In <em>8th IEEE/ICSC International Conference on Semantic Computing</em>, 163. IBM Corporation, 2014. URL: <a class="reference external" href="http://ieee-icsc.org/icsc2014/GraphComputing_IBM_Lin.pdf">http://ieee-icsc.org/icsc2014/GraphComputing_IBM_Lin.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-news" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id91">[79]</a></td><td>Dylan Raithel. Apache tinkerpop graduates to top-level project. Web Page, 2016. URL: <a class="reference external" href="https://www.infoq.com/news/2016/06/tinkerpop-top-level-apache/">https://www.infoq.com/news/2016/06/tinkerpop-top-level-apache/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apachetinkerpophome" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id92">[80]</a></td><td>Apache Tinker Pop Home. Apache tinkerpop home. Web Page, 2016. URL: <a class="reference external" href="https://tinkerpop.apache.org/">https://tinkerpop.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-parasol" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id93">[81]</a></td><td>Parasol. Webpage. URL: <a class="reference external" href="https://parasol.tamu.edu/research.php">https://parasol.tamu.edu/research.php</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="dream" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[82]</td><td><em>(<a class="fn-backref" href="#id94">1</a>, <a class="fn-backref" href="#id95">2</a>)</em> Overview. Online. The contact information listed on the webpage was for Yogesh Simmhan. URL: <a class="reference external" href="http://dream-lab.cds.iisc.ac.in/about/overview/">http://dream-lab.cds.iisc.ac.in/about/overview/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="rao" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[83]</td><td><em>(<a class="fn-backref" href="#id96">1</a>, <a class="fn-backref" href="#id97">2</a>)</em> Siddharth Rao. Dream:lab  democratising computing through the cloud. Online, March 2016. URL: <a class="reference external" href="http://iisc.researchmedia.center/article/dreamlab-">http://iisc.researchmedia.center/article/dreamlab-</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="denero" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[84]</td><td><em>(<a class="fn-backref" href="#id98">1</a>, <a class="fn-backref" href="#id99">2</a>)</em> John Denero. <em>CS61A: Online Textbook</em>. Berkeley, http://www-inst.eecs.berkeley.edu/~cs61a/sp12/book/, 2011. &#8220;This book is derived from the classic textbook Structure and Interpretation of Computer Programs by Abelson, Sussman, and Sussman. John Denero originally modified if for Python for the Fall 2011 semester.&#8221; http://www-inst.eecs.berkeley.edu/~cs61a/sp12/book/. URL: <a class="reference external" href="http://www-inst.eecs.berkeley.edu/~cs61a/sp12/book/communication.html">http://www-inst.eecs.berkeley.edu/~cs61a/sp12/book/communication.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fusiontablesupport" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id100">[85]</a></td><td>Google. Fusiontablesupporthelp. Web Page, 2017. URL: <a class="reference external" href="https://support.google.com/fusiontables/answer/171181?hl=en">https://support.google.com/fusiontables/answer/171181?hl=en</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wiki-fusiontable" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id101">[86]</a></td><td>Wikipedia. Fusion table support. Web Page, Last updated in 1 November 2016. URL: <a class="reference external" href="https://support.google.com/fusiontables/answer/171181?hl=en">https://support.google.com/fusiontables/answer/171181?hl=en</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="googlefusiontable2012" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id102">[87]</a></td><td>Google. Google fusion tables: data management, integration and collaboration in the cloud. 2012. URL: <a class="reference external" href="http://homes.cs.washington.edu/~alon/files/socc10.pdf">http://homes.cs.washington.edu/~alon/files/socc10.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-nwb-edu" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id105">[88]</a></td><td>Nwb. Web Page, 2017. URL: <a class="reference external" href="http://nwb.cns.iu.edu/about.html">http://nwb.cns.iu.edu/about.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-elasticsearch" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id106">[89]</a></td><td>Elastic search. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.elastic.co/products/elasticsearch">https://www.elastic.co/products/elasticsearch</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-elasticsearch-intro" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id107">[90]</a></td><td>Elastic search getting started. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="elasticsearch-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id108">[91]</a></td><td>Clinton Gormley and Zachary Tong. <em>Elasticsearch - The Definitive Guide : A Distributed REAL-TIME SEARCH AND ANALYTICS ENGINE</em>. O&#8217;Reilly Media,Inc, 1005 Gravenstein Highway North, Sebastopol, CA 95472, 1st edition, 2015. ISBN 9781449358549.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-elasticsearch-hadoop" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id109">[92]</a></td><td>Elastic search on hadoop. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.elastic.co/products/hadoop">https://www.elastic.co/products/hadoop</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikipedia-elasticsearch" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id110">[93]</a></td><td>Elastic Search. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Elasticsearch">https://en.wikipedia.org/wiki/Elasticsearch</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-logstash" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id111">[94]</a></td><td>Logstash. Webpage. URL: <a class="reference external" href="https://www.elastic.co/guide/en/logstash/current/introduction.html">https://www.elastic.co/guide/en/logstash/current/introduction.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tableau-tutorial" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id112">[95]</a></td><td>Tableau tutorial. Web Page. URL: <a class="reference external" href="https://casci.umd.edu/wp-content/uploads/2013/12/Tableau-Tutorial.pdf">https://casci.umd.edu/wp-content/uploads/2013/12/Tableau-Tutorial.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tableau-web" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id113">[96]</a></td><td>Tableau official website. Web Page. URL: <a class="reference external" href="https://www.tableau.com/products/technology">https://www.tableau.com/products/technology</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-d3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id114">[97]</a></td><td>D3 data-driven documents. Web Page. Accessed: 2017-02-11. URL: <a class="reference external" href="https://d3js.org/">https://d3js.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-potree" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id115">[98]</a></td><td>Potree. URL: <a class="reference external" href="http://potree.org/wp/about/">http://potree.org/wp/about/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="potree-paper-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id116">[99]</a></td><td>Martinez-Rubi.O, Verhoeven.S, Van Meersbergen.M, Schtz.M Van, Oosterom.P Gonalves.R, and Tijssen.T. Taming the beast: free and open-source massive point cloud web visualization. 2015. URL: <a class="reference external" href="http://repository.tudelft.nl/islandora/object/uuid:0472e0d1-ec75-465a-840e-fd53d427c177?collection=research">http://repository.tudelft.nl/islandora/object/uuid:0472e0d1-ec75-465a-840e-fd53d427c177?collection=research</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="potree-paper-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id117">[100]</a></td><td>H.&nbsp;F. Halaoui. A spatio temporal indexing structure for efficient retrieval and manipulation of discretely changing spatial data. <em>Journal of Spatial Science</em>, 53(2):112, 2008. URL: <a class="reference external" href="http://dx.doi.org/10.1080/14498596.2008.9635146">http://dx.doi.org/10.1080/14498596.2008.9635146</a>, <a class="reference external" href="https://doi.org/10.1080/14498596.2008.9635146">doi:10.1080/14498596.2008.9635146</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-dcjs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id118">[101]</a></td><td>DC.js. Dc.js - dimensional charting javascript library. Web Page, January 2017. accessed: 2017-01-21. URL: <a class="reference external" href="https://dc-js.github.io/dc.js/">https://dc-js.github.io/dc.js/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tensorflow-paper-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id119">[102]</a></td><td>M.&nbsp;Abadi, A.&nbsp;Agarwal, P.&nbsp;Barham, E.&nbsp;Brevdo, Z.&nbsp;Chen, C.&nbsp;Citroand&nbsp;GS. Corrado, A.&nbsp;Davis, J.&nbsp;Dean, M.&nbsp;Devin, S.&nbsp;Ghemawat, I.&nbsp;Goodfellow, A.&nbsp;Harp, G.&nbsp;Irving, M.&nbsp;Isard, Y.&nbsp;Jia, R.&nbsp;Jozefowicz, L.&nbsp;Kaiser, M.&nbsp;Kudlur, J.&nbsp;Levenberg, D.&nbsp;Mane, R.&nbsp;Monga, S.&nbsp;Moore, D.&nbsp;Murray, C.&nbsp;Olah, M.&nbsp;Schuster, J.&nbsp;Shlens, B.&nbsp;Steiner, I.&nbsp;Sutskever, K.&nbsp;Talwar, P.&nbsp;Tucker, V.&nbsp;Vanhoucke, V.&nbsp;Vasudevan, F.&nbsp;Viegas, O.&nbsp;Vinyals, P.&nbsp;Warden, M.&nbsp;Wattenberg, M.&nbsp;Wicke, Y.&nbsp;Yu, and X.&nbsp;Zheng. Tensorflow: large-scale machine learning on heterogeneous distributed systems. Technical Report, Cornell University, March 2016. Accessed: 2017-1-24.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tensorflow" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id120">[103]</a></td><td>TensorFlow. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://www.tensorflow.org">https://www.tensorflow.org</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-cntk" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id121">[104]</a></td><td>CNTK/CNTKBook-20160217..pdf at master  Microsoft/CNTK  GitHub. Web Page. URL: <a class="reference external" href="https://github.com/Microsoft/CNTK/blob/master/Documentation/CNTK-TechReport/lyx/CNTKBook-20160217..pdf">https://github.com/Microsoft/CNTK/blob/master/Documentation/CNTK-TechReport/lyx/CNTKBook-20160217..pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cntk" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id122">[105]</a></td><td>Home  Microsoft/CNTK Wiki  GitHub. Web Page. URL: <a class="reference external" href="https://github.com/Microsoft/CNTK/wiki">https://github.com/Microsoft/CNTK/wiki</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-gae" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id123">[106]</a></td><td>Google app engine. Web Page. URL: <a class="reference external" href="https://cloud.google.com/appengine/">https://cloud.google.com/appengine/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-appscale" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id124">[107]</a></td><td>AppScale. What-is-appscale. Web Page, 2016. URL: <a class="reference external" href="https://www.appscale.com/community/what-is-appscale/">https://www.appscale.com/community/what-is-appscale/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-developers-openshift" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id127">[108]</a></td><td>developers-openshift components. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://developers.openshift.com/">https://developers.openshift.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openshift" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id128">[109]</a></td><td>openshift components. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://www.openshift.org/">https://www.openshift.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-heroku" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id130">[110]</a></td><td>Heroku. Web Page. Accessed:1/27/2017. URL: <a class="reference external" href="https://devcenter.heroku.com/">https://devcenter.heroku.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cedar" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id131">[111]</a></td><td>Cedar stack. Web Page. Accessed:1/27/2017. URL: <a class="reference external" href="https://devcenter.heroku.com/articles/stack#cedar">https://devcenter.heroku.com/articles/stack#cedar</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-aero" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id132">[112]</a></td><td>Aerobatic. Aerobatic - overview. Web Page, January 2017. accessed: 2017-01-25. URL: <a class="reference external" href="https://www.aerobatic.com/docs/overview/">https://www.aerobatic.com/docs/overview/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikipedia-cloud" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id133">[113]</a></td><td>Wikipedia. Cloud computing  wikipedia, the free encyclopedia. web page, jan 2017. Online; accessed 21-jan-2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Cloud_computing">https://en.wikipedia.org/wiki/Cloud_computing</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azure-msft" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[114]</td><td><em>(<a class="fn-backref" href="#id134">1</a>, <a class="fn-backref" href="#id135">2</a>, <a class="fn-backref" href="#id141">3</a>)</em> Microsoft Corp. Web Page, jan 2017. Online; accessed 21-jan-2017. URL: <a class="reference external" href="https://azure.microsoft.com/en-us/">https://azure.microsoft.com/en-us/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sec-edgar-msft" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id136">[115]</a></td><td>Microsoft Corp. Form 10-k. Web page, july 2016. Online; accessed 21-jan-2017. URL: <a class="reference external" href="https://www.sec.gov/Archives/edgar/data/789019/000119312516662209/d187868d10k.htm">https://www.sec.gov/Archives/edgar/data/789019/000119312516662209/d187868d10k.htm</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-aws-amzn" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id137">[116]</a></td><td>Amazon.com, Inc. Web Page, jan 2017. Online; accessed 25-jan-2017. URL: <a class="reference external" href="{https://aws.amazon.com}">https://aws.amazon.com</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-softlayer-ibm" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id138">[117]</a></td><td>IBM Corp. Web Page, jan 2017. Online; accessed 25-jan-2017. URL: <a class="reference external" href="www.softlayer.com">www.softlayer.com</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bluemix-ibm" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id139">[118]</a></td><td>IBM Corp. Web Page, jan 2017. Online; accessed 25-jan-2017. URL: <a class="reference external" href="{https://www.ibm.com/cloud-computing/bluemix/}">https://www.ibm.com/cloud-computing/bluemix/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloud-google" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id140">[119]</a></td><td>Alphabet, Inc. Web Page, jan 2017. Online; accessed 25-jan-2017. URL: <a class="reference external" href="{https://cloud.google.com}">https://cloud.google.com</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudfoundry-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id142">[120]</a></td><td>Duncan&nbsp;C.E Winn. <em>Cloud Foundry-The Cloud Native Platform</em>. O&#8217;Reilley Media, 2016. URL: <a class="reference external" href="https://books.google.com/books?id=XP2wDAAAQBAJ&amp;printsec=frontcover&amp;dq=Cloud+foundry">https://books.google.com/books?id=XP2wDAAAQBAJ&amp;printsec=frontcover&amp;dq=Cloud+foundry</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudfoundry-blog" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id143">[121]</a></td><td>Cloud foundry blog. Web Page. URL: <a class="reference external" href="http://cloudacademy.com/blog/cloud-foundry-benefits/">http://cloudacademy.com/blog/cloud-foundry-benefits/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pivotal-www" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id144">[122]</a></td><td>Pivotal. Web Page. URL: <a class="reference external" href="https://run.pivotal.io/features/">https://run.pivotal.io/features/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ninefoldsite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id145">[123]</a></td><td>Ninefold. Ninefold news. Web Page, November 2015. Accessed: 2017-1-28. URL: <a class="reference external" href="http://ninefold.com/news/">http://ninefold.com/news/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jelastic-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id146">[124]</a></td><td>Jelastic. Web Page, December 2016. Page Version ID: 754931676. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Jelastic&amp;oldid=754931676">https://en.wikipedia.org/w/index.php?title=Jelastic&amp;oldid=754931676</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jelastic-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id147">[125]</a></td><td>Grow Hosting Business with Software Platform. Web Page. URL: <a class="reference external" href="https://jelastic.com/cloud-business-for-hosting-providers/">https://jelastic.com/cloud-business-for-hosting-providers/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hpe" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id148">[126]</a></td><td>HPE. Hpe helion stackato. Webpage. URL: <a class="reference external" href="https://www.hpe.com/us/en/software/multi-cloud-platform.html">https://www.hpe.com/us/en/software/multi-cloud-platform.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-virt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id149">[127]</a></td><td>Mike Kavis. The pros and cons of private and public paas. Webpage, 06 2013. URL: <a class="reference external" href="https://www.virtualizationpractice.com/the-pros-and-cons-of-private-and-public-paas-21961/">https://www.virtualizationpractice.com/the-pros-and-cons-of-private-and-public-paas-21961/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wee" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[128]</td><td><em>(<a class="fn-backref" href="#id150">1</a>, <a class="fn-backref" href="#id154">2</a>)</em> Pau&nbsp;Kiat Wee. <em>Instant AppFog</em>, chapter&nbsp;1. Packt Publishing, July 2013. URL: <a class="reference external" href="https://www.packtpub.com/mapt/book/Web-Development/9781782167624/1/ch01lvl1sec03/So,+what+is+AppFog">https://www.packtpub.com/mapt/book/Web-Development/9781782167624/1/ch01lvl1sec03/So,+what+is+AppFog</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="kepes" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[129]</td><td><em>(<a class="fn-backref" href="#id151">1</a>, <a class="fn-backref" href="#id152">2</a>)</em> Ben Kepes. Understanding the cloud computing stack: saas, paas, iaas. white paper, Racspace, 2015. URL: <a class="reference external" href="https://support.rackspace.com/white-paper/understanding-the-cloud-computing-stack-saas-paas-iaas/">https://support.rackspace.com/white-paper/understanding-the-cloud-computing-stack-saas-paas-iaas/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="appfog" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id153">[130]</a></td><td>appfog. Overview. Online. URL: <a class="reference external" href="https://www.ctl.io/appfog/#Overview">https://www.ctl.io/appfog/#Overview</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tweney" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id155">[131]</a></td><td>Dylan Tweney. Appfog gives developers an easier way to deploy cloud apps (interview). Online, May 2012. URL: <a class="reference external" href="http://venturebeat.com/2012/05/15/appfog-gives-developers-an-easier-way-to-deploy-cloud-apps-interview/">http://venturebeat.com/2012/05/15/appfog-gives-developers-an-easier-way-to-deploy-cloud-apps-interview/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudbees-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id156">[132]</a></td><td>Cloudbees wikipedia documentation. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/CloudBees">https://en.wikipedia.org/wiki/CloudBees</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudbees-webpage" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id157">[133]</a></td><td>Cloudbees webpage documentation. Web Page. URL: <a class="reference external" href="https://www.cloudbees.com/products">https://www.cloudbees.com/products</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id158">[134]</a></td><td>Wikipedia. Cloudcontrol. Wiki Page, February 2016. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/CloudControl">https://en.wikipedia.org/wiki/CloudControl</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-dotcloud" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id159">[135]</a></td><td>Jordan Novet. Dotcloud. Web Page, January 2016. Accessed: 2017-1-31. URL: <a class="reference external" href="http://venturebeat.com/2016/01/22/dotcloud-the-cloud-service-that-gave-birth-to-docker-is-shutting-down-on-february-29/">http://venturebeat.com/2016/01/22/dotcloud-the-cloud-service-that-gave-birth-to-docker-is-shutting-down-on-february-29/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="agave-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[136]</td><td><em>(<a class="fn-backref" href="#id161">1</a>, <a class="fn-backref" href="#id163">2</a>)</em> Liya Wang, Peter&nbsp;Van Buren, and Doreen Ware. Architecting a distributed bioinformatics platform with irods and iplant agave api. In <em>2015 International Conference on Computational Science and Computational Intelligence (CSCI)</em>. December 2015.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-agaveapi-features" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id162">[137]</a></td><td>Agave api home  features tab. webpage. Accessed : 02-04-2017. URL: <a class="reference external" href="https://agaveapi.co/platform/features/">https://agaveapi.co/platform/features/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-at1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id164">[138]</a></td><td>At1. Web Page. Accessed: 2017-1-22. URL: <a class="reference external" href="http://www.cyverse.org/atmosphere">http://www.cyverse.org/atmosphere</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-at2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id165">[139]</a></td><td>Lisa Martin Charis Cook Naim Matasci Jason Williams&nbsp;Ruth Bastow. Data mining with iplant:. Paper, Oct 2014. URL: <a class="reference external" href="https://academic.oup.com/jxb/article/66/1/1/2893405/Data-mining-with-iPlantA-meeting-report-from-the">https://academic.oup.com/jxb/article/66/1/1/2893405/Data-mining-with-iPlantA-meeting-report-from-the</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apache-tajo" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id166">[140]</a></td><td>Apache tajo. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="https://tajo.apache.org">https://tajo.apache.org</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tutorialspoint-tajo" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[141]</td><td><em>(<a class="fn-backref" href="#id167">1</a>, <a class="fn-backref" href="#id168">2</a>)</em> Apache tajo quick guide. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.tutorialspoint.com/apache_tajo/apache_tajo_quick_guide.htm">https://www.tutorialspoint.com/apache_tajo/apache_tajo_quick_guide.htm</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-phoenix-cloudera" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[142]</td><td><em>(<a class="fn-backref" href="#id169">1</a>, <a class="fn-backref" href="#id173">2</a>)</em> Justin Kestelyn. Phoenix in 15 minutes or less. Web page, march 2013. Online; accessed 25-jan-2017. URL: <a class="reference external" href="http://blog.cloudera.com/blog/2013/03/phoenix-in-15-minutes-or-less/">http://blog.cloudera.com/blog/2013/03/phoenix-in-15-minutes-or-less/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apachephoenix-org" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id171">[143]</a></td><td>Apache Software Foundation. Apache phoenix: oltp and operational analytics for apache hadoop. Web page, jan 2017. Online; accessed 25-jan-2017. URL: <a class="reference external" href="http://phoenix.apache.org/">http://phoenix.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-phoenix-salesforcedev" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id172">[144]</a></td><td>Adam Seligman. Apache phoenix - a small step for big data. Web page, may 2014. Online; accessed 25-jan-2017. URL: <a class="reference external" href="https://developer.salesforce.com/blogs/developer-relations/2014/05/apache-phoenix-small-step-big-data.html">https://developer.salesforce.com/blogs/developer-relations/2014/05/apache-phoenix-small-step-big-data.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-phoenix-infoq" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id174">[145]</a></td><td>Abel Avram. Phoenix: running sql queries on apache hbase [updated]. Web page, jan 2013. Online; accessed 25-jan-2017. URL: <a class="reference external" href="https://www.infoq.com/news/2013/01/Phoenix-HBase-SQL">https://www.infoq.com/news/2013/01/Phoenix-HBase-SQL</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-phoenix-bighadoop" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id175">[146]</a></td><td>Istvan Szegedi. Apache phoenix - an sql driver for hbase. Web page, may 2014. Online; accessed 23-jan-2017. URL: <a class="reference external" href="https://bighadoop.wordpress.com/2014/05/17/apache-phoenix-an-sql-driver-for-hbase/">https://bighadoop.wordpress.com/2014/05/17/apache-phoenix-an-sql-driver-for-hbase/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apachemrql" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[147]</td><td><em>(<a class="fn-backref" href="#id176">1</a>, <a class="fn-backref" href="#id177">2</a>)</em> Apache Software Foundation. Apache mrql. Web Page, April 2016. accessed 2017-01-29. URL: <a class="reference external" href="https://mrql.incubator.apache.org/">https://mrql.incubator.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mrqlhadoop" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id178">[148]</a></td><td>Edward&nbsp;J. Yoon. Mrql - a sql on hadoop miracle. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="http://www.hadoopsphere.com/2013/04/mrql-sql-on-hadoop-miracle.html">http://www.hadoopsphere.com/2013/04/mrql-sql-on-hadoop-miracle.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apacheincubator" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id179">[149]</a></td><td>Apache Software Foundation. Apache incubator. Web Page. accessed 2017-01-29. URL: <a class="reference external" href="http://incubator.apache.org/">http://incubator.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sap-hana" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id180">[150]</a></td><td>SAP. What is sap hana. Web Page. Accessed: 2017-1-17. URL: <a class="reference external" href="http://www.sap.com/product/technology-platform/hana.html">http://www.sap.com/product/technology-platform/hana.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="olofson-2014" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id181">[151]</a></td><td>Carl&nbsp;W Olofson. The analytic-transactional data platform: enabling the real-time enterprise (idc). Technical Report, International Data Corporation (IDC), Dec 2014. Accessed: 2017-1-17. URL: <a class="reference external" href="http://www.sap.com/documents/2016/08/3c4e546e-817c-0010-82c7-eda71af511fa.html">http://www.sap.com/documents/2016/08/3c4e546e-817c-0010-82c7-eda71af511fa.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="git-hadoopdb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id182">[152]</a></td><td>About hadoopdb. Web Page. Accessed: 2017-02-12. URL: <a class="reference external" href="http://db.cs.yale.edu/hadoopdb/">http://db.cs.yale.edu/hadoopdb/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apache-hqwq" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id183">[153]</a></td><td>Pivotal hawq. URL: <a class="reference external" href="http://hawq.incubator.apache.org/">http://hawq.incubator.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-pivotalhdb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id184">[154]</a></td><td>Pivotal hdb. URL: <a class="reference external" href="https://pivotal.io/pivotal-hdb">https://pivotal.io/pivotal-hdb</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-presto" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id185">[155]</a></td><td>Presto. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://prestodb.io/">https://prestodb.io/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="presto-paper-2014" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id186">[156]</a></td><td>Yueguo Chen, Xiongpai Qin, Haoqiong Bian, Jun Chen, Zhaoan Dong, Xiaoyong Du, Yanjie Gao, Dehai Liu, Jiaheng Lu, and Huijie Zhang. <em>A Study of SQL-on-Hadoop Systems</em>, pages 154166. Springer International Publishing, 2014.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-dremel" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id187">[157]</a></td><td>Sergey Melnik, Andrey Gubarev, Jing&nbsp;Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton, and Theo Vassilakis. Dremel: interactive analysis of web-scale datasets. <em>Communications of the ACM</em>, 54:114123, June 2011. URL: <a class="reference external" href="http://cacm.acm.org/magazines/2011/6/108648-dremel-interactive-analysis-of-web-scale-datasets/fulltext">http://cacm.acm.org/magazines/2011/6/108648-dremel-interactive-analysis-of-web-scale-datasets/fulltext</a>, <a class="reference external" href="https://doi.org/10.1145/1953122.1953148">doi:10.1145/1953122.1953148</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-redshift" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id188">[158]</a></td><td>Amazon Web services. Amazon redshift management services. Web Page, February 2017. Accessed: 2017-02-12. URL: <a class="reference external" href="http://docs.aws.amazon.com/redshift/latest/mgmt/overview.html">http://docs.aws.amazon.com/redshift/latest/mgmt/overview.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wiki-red" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id189">[159]</a></td><td>Wikipedia. Amazon redshift. Web Page, February 2017. Accessed: 2017-02-12. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Amazon_Redshift">https://en.wikipedia.org/wiki/Amazon_Redshift</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apachedrill" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id190">[160]</a></td><td>Apache drill. Web Page. Accessed:2/4/2017. URL: <a class="reference external" href="https://drill.apache.org/">https://drill.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kyotocabinet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id191">[161]</a></td><td>Kyoto cabinet. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="http://fallabs.com/kyotocabinet/">http://fallabs.com/kyotocabinet/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bytemining-sawzall" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id193">[162]</a></td><td>Ryan Rosario. Exciting tools for big data: s4, sawzall and mrjob! Web page, november 2010. Online; accessed 30-jan-2017. URL: <a class="reference external" href="http://www.bytemining.com/2010/11/exciting-tools-for-big-data-s4-sawzall-and-mrjob/">http://www.bytemining.com/2010/11/exciting-tools-for-big-data-s4-sawzall-and-mrjob/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-google-code-wiki-sawzall" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id195">[163]</a></td><td>Alphabet, Inc. Szl - overview.wiki. Code Repository. Online; accessed 30-jan-2017. URL: <a class="reference external" href="https://code.google.com/archive/p/szl/wikis/Overview.wiki">https://code.google.com/archive/p/szl/wikis/Overview.wiki</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="data-flow1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[164]</td><td><em>(<a class="fn-backref" href="#id196">1</a>, <a class="fn-backref" href="#id197">2</a>, <a class="fn-backref" href="#id199">3</a>)</em> Google. Data_flow1. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="https://cloud.google.com/dataflow/">https://cloud.google.com/dataflow/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="dataconomy" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id198">[165]</a></td><td>Eileen McNulty. Dataconomy. Blog. Accessed: 02/03/2016. URL: <a class="reference external" href="http://dataconomy.com/2014/08/google-cloud-dataflow/">http://dataconomy.com/2014/08/google-cloud-dataflow/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="storm-paper-ijctt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id200">[166]</a></td><td>Muhammad&nbsp;Hussain Iqbal and Tariq&nbsp;Rahim Soomro. Big data analysis: apache storm perspective. In <em>International Journal of Computer Trends and Technology (IJCTT)-2015</em>. January 2015.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-storm-home-concepts" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id201">[167]</a></td><td>Apache storm homepage. webpage. Accessed : 01-22-2017. URL: <a class="reference external" href="http://storm.apache.org/releases/1.0.2/Concepts.html">http://storm.apache.org/releases/1.0.2/Concepts.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-samza-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[168]</td><td><em>(<a class="fn-backref" href="#id202">1</a>, <a class="fn-backref" href="#id205">2</a>)</em> Apache Samza. Web Page, February 2017. Page Version ID: 764035647. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Apache_Samza&amp;oldid=764035647">https://en.wikipedia.org/w/index.php?title=Apache_Samza&amp;oldid=764035647</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-samza-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id203">[169]</a></td><td>Samza. Web Page. URL: <a class="reference external" href="http://samza.apache.org/">http://samza.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-samza-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id204">[170]</a></td><td>Apache Samza, LinkedIn&#8216;s Framework for Stream Processing. Web Page, January 2015. URL: <a class="reference external" href="https://thenewstack.io/apache-samza-linkedins-framework-for-stream-processing/">https://thenewstack.io/apache-samza-linkedins-framework-for-stream-processing/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-granules" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id206">[171]</a></td><td>Shrideep Pallickara Thilina Buddhika Matthew Malensek&nbsp;Ryan Stern. Granules. Project, July 2016. URL: <a class="reference external" href="http://granules.cs.colostate.edu/">http://granules.cs.colostate.edu/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="millwheel-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id207">[172]</a></td><td>Tyler Akidau, Alex Balikov, Kaya Bekiroglu, Slava Chernyak, Josh Haberman, Reuven Lax, Sam McVeety, Daniel Mills, Paul Nordstrom, and Sam Whittle. Millwheel: fault-tolerant stream processing at internet scale. In <em>Very Large Data Bases</em>, 734746. 2013.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kinesis" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id208">[173]</a></td><td>Kinesis - real-time streaming data in the aws cloud. Web Page. Accessed: 2017-01-17. URL: <a class="reference external" href="https://aws.amazon.com/kinesis/">https://aws.amazon.com/kinesis/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="big-data-analytics-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id209">[174]</a></td><td>Sumit&nbsp;Gupta Shilpi&nbsp;Saxena. <em>Real-Time Big Data Analytics</em>. Packt Publishing, 35 Livery Street, Birmingham B3 2PB, UK, 1st edition, 2016. ISBN 9781784391409.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="twitterheronopen" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id210">[175]</a></td><td>Twittter. Open sourcing twitter heron. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://blog.twitter.com/2016/open-sourcing-twitter-heron">https://blog.twitter.com/2016/open-sourcing-twitter-heron</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="twitterheron" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id211">[176]</a></td><td>Twittter. Flying faster with twitter heron. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://blog.twitter.com/2015/flying-faster-with-twitter-heron">https://blog.twitter.com/2015/flying-faster-with-twitter-heron</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azurestreamanalytics" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id212">[177]</a></td><td>Microsoft Azure real-time data analytics. Web Page. Accessed: 2017-2-03. URL: <a class="reference external" href="https://azure.microsoft.com/en-us/services/stream-analytics/">https://azure.microsoft.com/en-us/services/stream-analytics/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-docs-microsoft" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id213">[178]</a></td><td>Microsoft Docs azure stream analytics documentation. Web Page. Accessed: 2017-2-03. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/stream-analytics/">https://docs.microsoft.com/en-us/azure/stream-analytics/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-github-azure" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id214">[179]</a></td><td>Github azure/ azure-stream-analytics. Web Page. Accessed: 2017-2-03. URL: <a class="reference external" href="https://github.com/Azure/azure-stream-analytics/">https://github.com/Azure/azure-stream-analytics/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-spark" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id215">[180]</a></td><td>Abdul&nbsp;Ghaffar Shoro and Tariq&nbsp;Rahim Soomro. Big data analysis: apache spark perspective. <em>Global Journal of Computer Science and Technology</em>, 2015. Accessed: 2017-1-21. URL: <a class="reference external" href="http://www.computerresearch.org/index.php/computer/article/view/1137/1124">http://www.computerresearch.org/index.php/computer/article/view/1137/1124</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-twister1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id216">[181]</a></td><td>IU &#8220;Twister&#8221; software improves Google&#8216;s MapReduce for large-scale scientific data analysis: IU News Room: Indiana University. Web Page. URL: <a class="reference external" href="http://newsinfo.iu.edu/news-archive/13726.html">http://newsinfo.iu.edu/news-archive/13726.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-twister2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id217">[182]</a></td><td>Twister: Iterative MapReduce. Web Page. URL: <a class="reference external" href="http://www.iterativemapreduce.org/">http://www.iterativemapreduce.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-twister3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id218">[183]</a></td><td>IU &#8216;Twister&#8216; software improves Google&#8216;s MapReduce for large-scale scientific data analysis. Web Page. URL: <a class="reference external" href="https://phys.org/news/2010-03-iu-twister-software-google-mapreduce.html">https://phys.org/news/2010-03-iu-twister-software-google-mapreduce.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-twister" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id219">[184]</a></td><td>Bingjing Zhang, Yang Ruan, Tak-Lon Wu, Judy Qiu, Adam Hughes, and Geoffrey Fox. Applying twister to scientific applications. In <em>Cloud Computing Technology and Science (CloudCom), 2010 IEEE Second International Conference on</em>, 2532. IEEE, 2010.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mapreducempi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id220">[185]</a></td><td>Mr-mpi. Web Page, 2017. URL: <a class="reference external" href="http://mapreduce.sandia.gov/doc">http://mapreduce.sandia.gov/doc</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-flink" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id221">[186]</a></td><td>Wikipedia. Apache flink. Web Page, February 2017. Accessed: 2017-02-12. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Apache_Flink">https://en.wikipedia.org/wiki/Apache_Flink</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-reef" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id222">[187]</a></td><td>Reef. Webpage. Accessed: 2017-01-28. URL: <a class="reference external" href="https://wiki.apache.org/incubator/ReefProposal">https://wiki.apache.org/incubator/ReefProposal</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="apache-hama" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id223">[188]</a></td><td>Apache. web-page. URL: <a class="reference external" href="https://hama.apache.org/">https://hama.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-hama" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id224">[189]</a></td><td>James J. (Jong&nbsp;Hyuk) Park, Hai Jin, Young-Sik Jeong, and Muhammad&nbsp;Khurram Khan. <em>Advanced Multimedia and Ubiquitous Engineering</em>. Springer, 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-galoissite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id225">[190]</a></td><td>ISS team -&nbsp;University of Texas. Galois website. Web Page. Accessed: 2017-2-19. URL: <a class="reference external" href="www-galoisSite: http://iss.ices.utexas.edu/?p=projects/galois">www-galoisSite: http://iss.ices.utexas.edu/?p=projects/galois</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="taoparallelismpaper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id226">[191]</a></td><td>Keshav Pingali, Donald Nguyen, Milind Kulkarni, Martin Burtscher, M.&nbsp;Amber Hassaan, Rashid Kaleem, Tsung-Hsien Lee, Andrew Lenharth, Roman Manevich, Mario Mendez-Lojo, Dimitrios Prountzos, and Xin Sui. The tao of parallelism in algorithms. In <em>The Tao of Parallelism in Algorithms</em>, 114. June 2011. URL: <a class="reference external" href="http://iss.ices.utexas.edu/Publications/Papers/pingali11.pdf">http://iss.ices.utexas.edu/Publications/Papers/pingali11.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-netsyslab" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id227">[192]</a></td><td>Netsyslab. Totem. Webpage. URL: <a class="reference external" href="http://netsyslab.ece.ubc.ca/wiki/index.php/Totem">http://netsyslab.ece.ubc.ca/wiki/index.php/Totem</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="thesis-pub-sub" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id228">[193]</a></td><td>Vinay&nbsp;Jayarama Setty. <em>Publish/subscribe for large-scale social interaction: Design, analysis and resource provisioning</em>. PhD thesis, Department of Informatics,UNIVERSITY OF OSLO, Faculty of Mathematics and Natural Sciences, University of Oslo, January 2015. Accessed:2017-1-29. URL: <a class="reference external" href="https://www.duo.uio.no/bitstream/handle/10852/43117/1595-Setty-DUO-Thesis.pdf?sequence=1&amp;isAllowed=y">https://www.duo.uio.no/bitstream/handle/10852/43117/1595-Setty-DUO-Thesis.pdf?sequence=1&amp;isAllowed=y</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-pub-sub" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id229">[194]</a></td><td>Patrick&nbsp;Th. Eugster, Pascal&nbsp;A. Felber, Rachid Guerraoui, and Anne-Marie Kermarrec. The many faces of publish/subscribe. <em>ACM Comput. Surv.</em>, 35(2):114131, June 2003. Accessed: 2017-1-27. URL: <a class="reference external" href="http://doi.acm.org/10.1145/857076.857078">http://doi.acm.org/10.1145/857076.857078</a>, <a class="reference external" href="https://doi.org/10.1145/857076.857078">doi:10.1145/857076.857078</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-pub-sub-bigdata" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id230">[195]</a></td><td>Christian Esposito, Massimo Ficco, Francesco Palmieri, and Aniello Castiglione. A knowledge-based platform for big data analytics based on publish/subscribe services and stream processing. <em>Know.-Based Syst.</em>, 79(C):317, May 2015. Accessed: 2017-1-27. URL: <a class="reference external" href="http://dx.doi.org/10.1016/j.knosys.2014.05.003">http://dx.doi.org/10.1016/j.knosys.2014.05.003</a>, <a class="reference external" href="https://doi.org/10.1016/j.knosys.2014.05.003">doi:10.1016/j.knosys.2014.05.003</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hpx-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id231">[196]</a></td><td>High performance parallex (hpx-5). Web Page. Accessed: 2017-1-17. URL: <a class="reference external" href="https://hpx.crest.iu.edu/">https://hpx.crest.iu.edu/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hpx-5-user-guide" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id232">[197]</a></td><td><em>HPX-5: user guide</em>. HPX-5. Accessed: 2017-1-17. URL: <a class="reference external" href="https://hpx.crest.iu.edu/users_guide">https://hpx.crest.iu.edu/users_guide</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-harp" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id233">[198]</a></td><td>Harp. Webpage. Accessed: 2017-01-28. URL: <a class="reference external" href="http://harpjs.com">http://harpjs.com</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-netty" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id234">[199]</a></td><td>Netty site. Web Page. URL: <a class="reference external" href="http://netty.io/">http://netty.io/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="netty-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id235">[200]</a></td><td>Norman Maurer and Marvin Wolfthal. <em>Netty in Action</em>. Manning Publications, Greenwich, CT, USA, 1st edition, 2015. ISBN 1617291471. URL: <a class="reference external" href="http://www.ebook.de/de/product/21687528/norman_maurer_netty_in_action.html">http://www.ebook.de/de/product/21687528/norman_maurer_netty_in_action.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zeromq" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[201]</td><td><em>(<a class="fn-backref" href="#id236">1</a>, <a class="fn-backref" href="#id238">2</a>)</em> Distributed messaging. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://zeromq.org">http://zeromq.org</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zeromq2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id237">[202]</a></td><td>0mq - the guide. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://zguide.zeromq.org/page:all">http://zguide.zeromq.org/page:all</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-rabbitmq" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id239">[203]</a></td><td>RabbitMQ, components. Web Page. Accessed: 2017-01-19. URL: <a class="reference external" href="https://www.rabbitmq.com/">https://www.rabbitmq.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ampq-article" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id240">[204]</a></td><td>John O&#8217;Hara. Toward a commodity enterprise middleware. <em>Queue</em>, 5(4):4855, 2007.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="git-kestrel" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id241">[205]</a></td><td>About kestrel. Web Page. Accessed: 2017-02-12. URL: <a class="reference external" href="https://github.com/twitter-archive/kestrel">https://github.com/twitter-archive/kestrel</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jms-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id242">[206]</a></td><td>Wikipedia link for jms. webpage. Accessed : 01-18-2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Java_Message_Service">https://en.wikipedia.org/wiki/Java_Message_Service</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jms-oracle-docs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id243">[207]</a></td><td>Oracle documentation on jms. webpage. Accessed : 01-18-2017. URL: <a class="reference external" href="http://docs.oracle.com/javaee/6/tutorial/doc/bnceh.html">http://docs.oracle.com/javaee/6/tutorial/doc/bnceh.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-amqp" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id244">[208]</a></td><td>AMQP. Amqp is the internet protocol for business messaging. Web Page, January 2017. accessed: 2017-01-31. URL: <a class="reference external" href="http://www.amqp.org/about/what">http://www.amqp.org/about/what</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mqtt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id245">[209]</a></td><td>Mqtt. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="http://mqtt.org/">http://mqtt.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-floodnet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id246">[210]</a></td><td>Mqtt floodnet. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="http://mqtt.org/projects/floodnet">http://mqtt.org/projects/floodnet</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sns" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id247">[211]</a></td><td>Amazon sns. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="https://aws.amazon.com/sns/">https://aws.amazon.com/sns/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="sns-blog" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id248">[212]</a></td><td>Amazon sns blog. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="https://aws.amazon.com/blogs/aws/introducing-the-amazon-simple-notification-service/">https://aws.amazon.com/blogs/aws/introducing-the-amazon-simple-notification-service/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="sns-faq" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id249">[213]</a></td><td>Amazon sns faqs. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="https://aws.amazon.com/sns/faqs/">https://aws.amazon.com/sns/faqs/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-awslambda" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id250">[214]</a></td><td>Amazon. Awslambda. Web Page. Accessed:2/18/2017. URL: <a class="reference external" href="https://aws.amazon.com/lambda/faqs/">https://aws.amazon.com/lambda/faqs/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-awslambdaevent" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id251">[215]</a></td><td>Amazon. Awslambdaevent. Web Page. Accessed:2/18/2017. URL: <a class="reference external" href="http://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html#intro-core-components-event-sources">http://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html#intro-core-components-event-sources</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-google-pub-sub" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id252">[216]</a></td><td>What-is-google-pub-sub. Web Page. Accessed: 2017-2-09. URL: <a class="reference external" href="https://cloud.google.com/pubsub/docs/overview">https://cloud.google.com/pubsub/docs/overview</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-google-pub-sub-features" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id253">[217]</a></td><td>Google-pub-sub-scalable-messaging-middleware. Web Page. Accessed: 2017-2-10. URL: <a class="reference external" href="https://cloud.google.com/pubsub/">https://cloud.google.com/pubsub/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="silberschatz1998operating" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id254">[218]</a></td><td>Abraham Silberschatz, Peter&nbsp;B Galvin, Greg Gagne, and A&nbsp;Silberschatz. <em>Operating system concepts</em>. Volume&nbsp;4. Addison-wesley Reading, 1998.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azurequeue-web" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id255">[219]</a></td><td>Azure queue website. Web Page. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/storage/storage-dotnet-how-to-use-queues">https://docs.microsoft.com/en-us/azure/storage/storage-dotnet-how-to-use-queues</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tutorialspoint" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id256">[220]</a></td><td>Tutorialspoint website. Web Page. URL: <a class="reference external" href="https://www.tutorialspoint.com/microsoft_azure/microsoft_azure_queues.htm">https://www.tutorialspoint.com/microsoft_azure/microsoft_azure_queues.htm</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-gora" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id257">[221]</a></td><td>Gora - in-memory data model and persistence for big data. Web Page. Accessed: 2017-01-18. URL: <a class="reference external" href="http://gora.apache.org/">http://gora.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-memcached" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id258">[222]</a></td><td>Memcached. Web Page. Accessed: 2017-01-30. URL: <a class="reference external" href="http://www.memcached.org/">http://www.memcached.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-keyvalue" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id259">[223]</a></td><td>Wikipedia. Key-value database. WebPage, November 2016. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Key-value_database">https://en.wikipedia.org/wiki/Key-value_database</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-relationaldb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id260">[224]</a></td><td>Wikipedia. Relational database. WebPage, January 2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Relational_database">https://en.wikipedia.org/wiki/Relational_database</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-lmdb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id261">[225]</a></td><td>Matthew Hardin. Understanding lmdb database file sizes and memory utilization. WebPage, May 2016. URL: <a class="reference external" href="https://symas.com/understanding-lmdb-database-file-sizes-and-memory-utilization/">https://symas.com/understanding-lmdb-database-file-sizes-and-memory-utilization/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikihazel" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[226]</td><td><em>(<a class="fn-backref" href="#id262">1</a>, <a class="fn-backref" href="#id264">2</a>)</em> Wikipedia. Hazelcast. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Hazelcast">https://en.wikipedia.org/wiki/Hazelcast</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-githubhazel" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id263">[227]</a></td><td>Hazelcast. Open source in-memory data grid. Code Repository, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="https://github.com/hazelcast/hazelcast">https://github.com/hazelcast/hazelcast</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ehcache-features" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id265">[228]</a></td><td>Ehcache - features. Web Page. Accessed: 2017-01-21. URL: <a class="reference external" href="http://www.ehcache.org/about/features.html">http://www.ehcache.org/about/features.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ehcache-documentation" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id266">[229]</a></td><td>Ehcache - documentation. Web Page. Accessed: 2017-01-21. URL: <a class="reference external" href="http://www.ehcache.org/documentation/3.2/getting-started.html">http://www.ehcache.org/documentation/3.2/getting-started.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="voltdb-www" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id267">[230]</a></td><td>VoltDB. Web Page. URL: <a class="reference external" href="https://www.voltdb.com/">https://www.voltdb.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="voltdb-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id268">[231]</a></td><td>VoltDB. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/VoltDB">https://en.wikipedia.org/wiki/VoltDB</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hstore" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id269">[232]</a></td><td>H-store. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="http://hstore.cs.brown.edu/">http://hstore.cs.brown.edu/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="kallman2008" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id270">[233]</a></td><td>Robert Kallman, Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alexander Rasin, Stanley Zdonik, Evan P.&nbsp;C. Jones, Samuel Madden, Michael Stonebraker, Yang Zhang, John Hugg, and Daniel&nbsp;J. Abadi. H-Store: a high-performance, distributed main memory transaction processing system. <em>Proc. VLDB Endow.</em>, 1(2):14961499, 2008. URL: <a class="reference external" href="http://hstore.cs.brown.edu/papers/hstore-demo.pdf">http://hstore.cs.brown.edu/papers/hstore-demo.pdf</a>, <a class="reference external" href="https://doi.org/http://doi.acm.org/10.1145/1454159.1454211">doi:http://doi.acm.org/10.1145/1454159.1454211</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hstorewiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id271">[234]</a></td><td>H-storewiki. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/H-Store">https://en.wikipedia.org/wiki/H-Store</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-eclipselink" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id272">[235]</a></td><td>Eclipselink. Accessed: 02-06-2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/EclipseLink">https://en.wikipedia.org/wiki/EclipseLink</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="datanucleuswiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id273">[236]</a></td><td>Wikipedia. Datanucleus wiki. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/DataNucleus">https://en.wikipedia.org/wiki/DataNucleus</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="datanucleus" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id274">[237]</a></td><td>DataNucleus. Datanucleus. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="http://www.datanucleus.com/">http://www.datanucleus.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="datanucleusperformance" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id275">[238]</a></td><td>JPAB. Jpa performance benchmark. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="http://www.jpab.org/DataNucleus.html">http://www.jpab.org/DataNucleus.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-odbc" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id276">[239]</a></td><td>Open database connectivity. Web Page. Accessed: 2017-1-30. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Open_Database_Connectivity">https://en.wikipedia.org/wiki/Open_Database_Connectivity</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jdbc" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id277">[240]</a></td><td>Java database connectivity. Web Page. Accessed: 2017-1-30. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Java_Database_Connectivity">https://en.wikipedia.org/wiki/Java_Database_Connectivity</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="uima-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id278">[241]</a></td><td>Wikipedia. Uima_wiki. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/UIMA">https://en.wikipedia.org/wiki/UIMA</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="uima-ss" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id279">[242]</a></td><td>Slideshare. Uima_ss. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="http://www.slideshare.net/teofili/apache-uima-introduction">http://www.slideshare.net/teofili/apache-uima-introduction</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tika" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id280">[243]</a></td><td>Apache tika. Web Page. URL: <a class="reference external" href="https://tika.apache.org/">https://tika.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-oracle" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[244]</td><td><em>(<a class="fn-backref" href="#id281">1</a>, <a class="fn-backref" href="#id282">2</a>)</em> Oracle Database - Wikipedia. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Oracle_Database">https://en.wikipedia.org/wiki/Oracle_Database</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sqlserver-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id283">[245]</a></td><td>Sql server wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Microsoft_SQL_Server">https://en.wikipedia.org/wiki/Microsoft_SQL_Server</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azuresql" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id284">[246]</a></td><td>Sql server azure. Web Page. URL: <a class="reference external" href="https://azure.microsoft.com/en-us/services/sql-database/?b=16.50">https://azure.microsoft.com/en-us/services/sql-database/?b=16.50</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-sqlserver" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id285">[247]</a></td><td>Ross Mistry and Stacia Misner. <em>Introducing Microsoft SQL Server 2014 Technical Overview</em>. Microsoft Press, 2014. ISBN 978-0-7356-8475-1.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="devmysql" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[248]</td><td><em>(<a class="fn-backref" href="#id286">1</a>, <a class="fn-backref" href="#id287">2</a>, <a class="fn-backref" href="#id288">3</a>, <a class="fn-backref" href="#id289">4</a>)</em> mysql. Mysql 5.7 reference manual, what is mysql. Online. URL: <a class="reference external" href="https://dev.mysql.com/doc/refman/5.7/en/what-is-mysql.html">https://dev.mysql.com/doc/refman/5.7/en/what-is-mysql.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="howmysql" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id290">[249]</a></td><td>HowStuffWorks.com. What are relational databases? Online, March 2001. URL: <a class="reference external" href="http://computer.howstuffworks.com/question599.htm">http://computer.howstuffworks.com/question599.htm</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cubrid" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id291">[250]</a></td><td>Cubrid. Web Page, 2017. URL: <a class="reference external" href="http://www.cubrid.org/">http://www.cubrid.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-galera-cluster" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id292">[251]</a></td><td>Galera cluster. Web Page, 2017. URL: <a class="reference external" href="http://galeracluster.com/">http://galeracluster.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apachederby" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id293">[252]</a></td><td>The Apache Software Foundation. Apache derby. Web Page, October 2016. accessed: 2017-02-18. URL: <a class="reference external" href="https://db.apache.org/derby/">https://db.apache.org/derby/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apachederbycharter" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id294">[253]</a></td><td>The Apache Software Foundation. Apache derby project charter. Web page, September 2016. accessed: 2017-02-18. URL: <a class="reference external" href="https://db.apache.org/derby/derby_charter.html">https://db.apache.org/derby/derby_charter.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-derbymanual" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id295">[254]</a></td><td>The Apache Software Foundation. Getting started with derby. Web Page, September 2015. accessed: 2017-02-18. URL: <a class="reference external" href="https://db.apache.org/derby/docs/10.12/getstart/index.html">https://db.apache.org/derby/docs/10.12/getstart/index.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="amazonrds" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id296">[255]</a></td><td>Amazon RDS. Amazonrds. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="http://searchaws.techtarget.com/definition/Amazon-Relational-Database-Service-RDS">http://searchaws.techtarget.com/definition/Amazon-Relational-Database-Service-RDS</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="amazonrdscomponents" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id297">[256]</a></td><td>Amazon RDS. What is amazon rds. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html#Welcome.Concepts">http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html#Welcome.Concepts</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibm-dash-db-com" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id298">[257]</a></td><td>5 things to know about dashdb. web page. URL: <a class="reference external" href="https://www.ibm.com/developerworks/community/blogs/5things/entry/5_things_to_know_about_dashdb_placeholder?lang=en">https://www.ibm.com/developerworks/community/blogs/5things/entry/5_things_to_know_about_dashdb_placeholder?lang=en</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibm-analytics-com" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id299">[258]</a></td><td>Ibm dashdb. Web Page. URL: <a class="reference external" href="https://www.ibm.com/analytics/us/en/technology/cloud-data-services/dashdb/">https://www.ibm.com/analytics/us/en/technology/cloud-data-services/dashdb/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-lucene" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id300">[259]</a></td><td>Apache lucene. Web Page, January 2017. URL: <a class="reference external" href="http://lucene.apache.org/">http://lucene.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-solandra" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id301">[260]</a></td><td>Solandra. Webpage. URL: <a class="reference external" href="https://github.com/tjake/Solandra/wiki/Solandra-Wiki">https://github.com/tjake/Solandra/wiki/Solandra-Wiki</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-solandra2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id302">[261]</a></td><td>Solandra. Webpage. URL: <a class="reference external" href="https://github.com/tjake/Solandra">https://github.com/tjake/Solandra</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-voldemort" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id303">[262]</a></td><td>LinkedIn. Project voldemort. Web Page. Accessed: 2017-1-17. URL: <a class="reference external" href="http://www.project-voldemort.com/voldemort/">http://www.project-voldemort.com/voldemort/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="rabl-sadoghi-jacobsen-2012" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id304">[263]</a></td><td>Tilmann Rabl, Sergio Gmez-Villamor, Mohammad Sadoghi, Victor Munts-Mulero, Hans-Arno Jacobsen, and Serge Mankovskii. Solving big data challenges for enterprise application performance management. <em>Proc. VLDB Endow.</em>, 5(12):17241735, August 2012. URL: <a class="reference external" href="https://arxiv.org/pdf/1208.4167.pdf">https://arxiv.org/pdf/1208.4167.pdf</a>, <a class="reference external" href="https://doi.org/10.14778/2367502.2367512">doi:10.14778/2367502.2367512</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-riak-kv" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id305">[264]</a></td><td>Riak-kv - nosql key value database. Web Page. Accessed: 2017-01-20. URL: <a class="reference external" href="http://basho.com/products/riak-kv/">http://basho.com/products/riak-kv/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-riak-ts" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id306">[265]</a></td><td>Riak-ts - nosql time series database. Web Page. Accessed: 2017-01-20. URL: <a class="reference external" href="http://basho.com/products/riak-ts/">http://basho.com/products/riak-ts/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-riak-s2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id307">[266]</a></td><td>Riak-s2 - cloud object storage software. Web Page. Accessed: 2017-01-20. URL: <a class="reference external" href="http://basho.com/products/riak-s2/">http://basho.com/products/riak-s2/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="datasys" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id308">[267]</a></td><td>Illinois&nbsp;Institute of&nbsp;Technology Department&nbsp;of Computer&nbsp;Science. Zht: a zero-hop distributed hashtable. Online. URL: <a class="reference external" href="http://datasys.cs.iit.edu/projects/ZHT/">http://datasys.cs.iit.edu/projects/ZHT/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wiley" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[268]</td><td><em>(<a class="fn-backref" href="#id309">1</a>, <a class="fn-backref" href="#id310">2</a>, <a class="fn-backref" href="#id311">3</a>)</em> Brandon Wiley. Distributed hash ttable, part 1. <em>Linux Journal</em>, October 2003. From issue number 114. URL: <a class="reference external" href="http://www.linuxjournal.com/article/6797?page=0,0">http://www.linuxjournal.com/article/6797?page=0,0</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="li" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[269]</td><td><em>(<a class="fn-backref" href="#id312">1</a>, <a class="fn-backref" href="#id313">2</a>, <a class="fn-backref" href="#id314">3</a>)</em> T.&nbsp;Li, X.&nbsp;Zhou, K.&nbsp;Brandstatter, D.&nbsp;Zhao, K.&nbsp;Wang, A.&nbsp;Rajendran, Z.&nbsp;Zhang, and I.&nbsp;Raicu. Zht: a light-weight reliable persistent dynamic scalable zero-hop distributed hash table. In <em>2013 IEEE 27th International Symposium on Parallel and Distributed Processing</em>, 775787. May 2013. URL: <a class="reference external" href="http://datasys.cs.iit.edu/publications/2013_IPDPS13_ZHT.pdf">http://datasys.cs.iit.edu/publications/2013_IPDPS13_ZHT.pdf</a>, <a class="reference external" href="https://doi.org/10.1109/IPDPS.2013.110">doi:10.1109/IPDPS.2013.110</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bdb-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id315">[270]</a></td><td>Berkeley db wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Berkeley_DB">https://en.wikipedia.org/wiki/Berkeley_DB</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bdb-stanford" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id316">[271]</a></td><td>Stanford course website. Web Page. URL: <a class="reference external" href="https://web.stanford.edu/class/cs276a/projects/docs/berkeleydb/reftoc.html">https://web.stanford.edu/class/cs276a/projects/docs/berkeleydb/reftoc.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bdb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id317">[272]</a></td><td>Oracle website. Web Page. URL: <a class="reference external" href="http://www.oracle.com/technetwork/database/database-technologies/berkeleydb">http://www.oracle.com/technetwork/database/database-technologies/berkeleydb</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tokyo-cabinet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[273]</td><td><em>(<a class="fn-backref" href="#id318">1</a>, <a class="fn-backref" href="#id320">2</a>)</em> Tokyo cabinet. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="http://fallabs.com/tokyocabinet/">http://fallabs.com/tokyocabinet/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kyoto-cabinet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id319">[274]</a></td><td>Kyoto cabinet. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="http://fallabs.com/kyotocabinet/">http://fallabs.com/kyotocabinet/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tycoon-fl" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id321">[275]</a></td><td>Fal labs. Tycoon_fl. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="http://fallabs.com/kyototycoon/">http://fallabs.com/kyototycoon/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tycoon-cf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id322">[276]</a></td><td>Cloudflare. Tycoon_cf. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="https://blog.cloudflare.com/kyoto-tycoon-secure-replication/">https://blog.cloudflare.com/kyoto-tycoon-secure-replication/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tycoon-fl2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id323">[277]</a></td><td>Fal labs. Tycoon_fl2. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="http://fallabs.com/kyotocabinet/">http://fallabs.com/kyotocabinet/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tyrant-blog" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id324">[278]</a></td><td>Tyrant blog. Web Page. URL: <a class="reference external" href="https://www.percona.com/blog/2009/10/19/mysql_memcached_tyrant_part3/">https://www.percona.com/blog/2009/10/19/mysql_memcached_tyrant_part3/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tyrant-fal-labs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id325">[279]</a></td><td>Tyrant fallabs. Web Page. URL: <a class="reference external" href="http://fallabs.com/tokyotyrant/">http://fallabs.com/tokyotyrant/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kyoto-tycoon" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id326">[280]</a></td><td>Kyoto tycoon. Web Page. URL: <a class="reference external" href="http://fallabs.com/kyototycoon/">http://fallabs.com/kyototycoon/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-infoworld-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id327">[281]</a></td><td>Rick Grehan. Nosql showdown: mongodb vs. couchbase. Web page, march 2013. Online; accessed 29-jan-2017. URL: <a class="reference external" href="http://www.infoworld.com/article/2613970/nosql/nosql-showdown--mongodb-vs--couchbase.html">http://www.infoworld.com/article/2613970/nosql/nosql-showdown&#8211;mongodb-vs&#8211;couchbase.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-safaribooks-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id328">[282]</a></td><td>Martin Brown. The technology behind couchbase. Web page, march 2012. Online; accessed 29-jan-2017. URL: <a class="reference external" href="https://www.safaribooksonline.com/blog/2012/03/01/the-technology-behind-couchbase/">https://www.safaribooksonline.com/blog/2012/03/01/the-technology-behind-couchbase/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-erlangcentral-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id329">[283]</a></td><td>Erlang Central. Couchbase performance and scalability: iterating with dtrace observability. Web page, march 2012. Online; accessed 29-jan-2017. URL: <a class="reference external" href="http://erlangcentral.org/videos/couchbase-performance-and-scalability-iterating-with-dtrace-observability/#.WI5uYephnRY">http://erlangcentral.org/videos/couchbase-performance-and-scalability-iterating-with-dtrace-observability/#.WI5uYephnRY</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikipedia-erlang-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id330">[284]</a></td><td>Wikipedia. Erlang (programming language)  wikpedia, the free encyclopedia. Web page, jan 2017. Online; accessed: 29-jan-2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Erlang_(programming_language)">https://en.wikipedia.org/wiki/Erlang_(programming_language)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hightower-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id332">[285]</a></td><td>Riyad Kalla. Well put! when should you use mongodb vs couchbase versus redis... Web page, october 2011. Online; accessed 29-jan-2017. URL: <a class="reference external" href="http://rick-hightower.blogspot.com/2014/04/well-put-when-should-you-use-mongodb-vs.html">http://rick-hightower.blogspot.com/2014/04/well-put-when-should-you-use-mongodb-vs.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-quora-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id333">[286]</a></td><td>Russell Smith. What are the advantages and disadvantages of using mongodb vs couchdb vs cassandra vs redis? Web page, november 2015. Online; accessed 29-jan-2017. URL: <a class="reference external" href="https://www.quora.com/What-are-the-advantages-and-disadvantages-of-using-MongoDB-vs-CouchDB-vs-Cassandra-vs-Redis">https://www.quora.com/What-are-the-advantages-and-disadvantages-of-using-MongoDB-vs-CouchDB-vs-Cassandra-vs-Redis</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibm-cloudant" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id334">[287]</a></td><td>Wikipedia. Ibm cloudant. Web Page, February 2017. Accessed: 2017-02-12. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Cloudant">https://en.wikipedia.org/wiki/Cloudant</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-gemfire" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id335">[288]</a></td><td>About pivotal gemfire. Web Page. Accessed: 2017-01-28. URL: <a class="reference external" href="http://gemfire.docs.pivotal.io/gemfire/getting_started/gemfire_overview.html">http://gemfire.docs.pivotal.io/gemfire/getting_started/gemfire_overview.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hbase" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id336">[289]</a></td><td>Apache hbase. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://hbase.apache.org/">https://hbase.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudbigtable" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[290]</td><td><em>(<a class="fn-backref" href="#id337">1</a>, <a class="fn-backref" href="#id338">2</a>)</em> Google. Cloud bigtable. Web Page. accessed 2017-01-29. URL: <a class="reference external" href="https://cloud.google.com/bigtable/">https://cloud.google.com/bigtable/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikispanner" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id339">[291]</a></td><td>Wikipedia. Spanner (database). Web Page, October 2016. accessed 2017-01-29. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Spanner_(database)">https://en.wikipedia.org/wiki/Spanner_(database)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikibigtable" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[292]</td><td><em>(<a class="fn-backref" href="#id340">1</a>, <a class="fn-backref" href="#id341">2</a>)</em> Wikipedia. Bigtable. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Bigtable">https://en.wikipedia.org/wiki/Bigtable</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="corbett-spanner" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id342">[293]</a></td><td>James&nbsp;C Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, Jeffrey&nbsp;John Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, and others. Spanner: googles globally distributed database. <em>ACM Transactions on Computer Systems (TOCS)</em>, 31(3):8, 2013. URL: <a class="reference external" href="http://dl.acm.org/ft_gateway.cfm?id=2491245&amp;type=pdf">http://dl.acm.org/ft_gateway.cfm?id=2491245&amp;type=pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-magastore-spanner" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id343">[294]</a></td><td>Mikio&nbsp;L. Braun. Magastore, Spanner - distributed databases. Web Page, 11 mar 2013. Accessed: 2017-01-28. URL: <a class="reference external" href="http://blog.mikiobraun.de/2013/03/more-google-papers-megastore-spanner-voted-commits.html">http://blog.mikiobraun.de/2013/03/more-google-papers-megastore-spanner-voted-commits.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cassandra" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id344">[295]</a></td><td>Apache cassandra. Web Page, 2016. URL: <a class="reference external" href="http://cassandra.apache.org/">http://cassandra.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="punnoose" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[296]</td><td><em>(<a class="fn-backref" href="#id345">1</a>, <a class="fn-backref" href="#id346">2</a>, <a class="fn-backref" href="#id348">3</a>, <a class="fn-backref" href="#id349">4</a>)</em> Roshan Punnoose, Adina Crainiceanu, and David Rapp. Rya: a scalable rdf triple store for the clouds. In <em>Proceedings of the 1st International Workshop on Cloud Intelligence</em>, Cloud-I &#8216;12, 4:14:8. New York, NY, USA, 2012. ACM. URL: <a class="reference external" href="http://doi.acm.org/10.1145/2347673.2347677">http://doi.acm.org/10.1145/2347673.2347677</a>, <a class="reference external" href="https://doi.org/10.1145/2347673.2347677">doi:10.1145/2347673.2347677</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="w3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id347">[297]</a></td><td>RDF&nbsp;Working Group. Resource description framework (rdf). Online, February 2014. URL: <a class="reference external" href="https://www.w3.org/RDF/">https://www.w3.org/RDF/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="apacherya" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id350">[298]</a></td><td>Apache Rya. Apache rya. Online. URL: <a class="reference external" href="https://rya.apache.org/">https://rya.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-graphdb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id351">[299]</a></td><td>GraphDb. Web Page. Accessed: 2017-01-30. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Graph_database/">https://en.wikipedia.org/wiki/Graph_database/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-allegro" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id352">[300]</a></td><td>Allegro. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="http://allegrograph.com/">http://allegrograph.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-allegrow" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id353">[301]</a></td><td>Allegrow. Web Page. Accessed: 2017-1-20. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/AllegroGraph">https://en.wikipedia.org/wiki/AllegroGraph</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-titan" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id354">[302]</a></td><td>Titan db. Web Page. Accessed:2/4/2017. URL: <a class="reference external" href="http://titan.thinkaurelius">http://titan.thinkaurelius</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tinkerpop" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id355">[303]</a></td><td>Tinkerpop. Web Page. Accessed:2/6/2017. URL: <a class="reference external" href="http://tinkerpop.apache.org/">http://tinkerpop.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="jena-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id356">[304]</a></td><td>w3. Jena_wiki. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="https://www.w3.org/2001/sw/wiki/Apache_Jena">https://www.w3.org/2001/sw/wiki/Apache_Jena</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="jena-blog" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[305]</td><td><em>(<a class="fn-backref" href="#id357">1</a>, <a class="fn-backref" href="#id358">2</a>)</em> Trimac&nbsp;NLP Blog. Jena_blog. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="http://trimc-nlp.blogspot.com/2013/06/introduction-to-jena.html">http://trimc-nlp.blogspot.com/2013/06/introduction-to-jena.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-rdf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id359">[306]</a></td><td>RDF components. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.w3.org/RDF/">https://www.w3.org/RDF/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sesame" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id360">[307]</a></td><td>sesame components. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://projects.eclipse.org/projects/technology.rdf4j">https://projects.eclipse.org/projects/technology.rdf4j</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="sesame-paper-2013" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id361">[308]</a></td><td>Ana Lucia&nbsp;Varbanescu Jianbin&nbsp;Fang and Henk Sips. Sesame: a user-transparent optimizing framework for many-core processors. In <em>IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing</em>, 7073. 2013.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-what-to-use" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id362">[309]</a></td><td>Ken Smith. Azure: what to use, what to avoid. Web page, october 2014. Online; accessed 28-jan-2017. URL: <a class="reference external" href="http://blog.wouldbetheologian.com/2014/10/azure-what-to-use-what-to-avoid.html">http://blog.wouldbetheologian.com/2014/10/azure-what-to-use-what-to-avoid.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-blobqueuetable" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id363">[310]</a></td><td>The Windows Club. Understanding blob, queue and table storage for windows azure. Web page, jan 2017. Online; accessed 28-jan-2017. URL: <a class="reference external" href="http://www.thewindowsclub.com/understanding-blobqueuetable-storage-windows-azure">http://www.thewindowsclub.com/understanding-blobqueuetable-storage-windows-azure</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-scalable-partitioning" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id364">[311]</a></td><td>Microsoft Corp. Designing a scalable partitioning strategy for azure table storage. Web page, jan 2017. Online; accessed 28-jan-2017. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/rest/api/storageservices/fileservices/designing-a-scalable-partitioning-strategy-for-azure-table-storage">https://docs.microsoft.com/en-us/rest/api/storageservices/fileservices/designing-a-scalable-partitioning-strategy-for-azure-table-storage</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="irods-www" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id365">[312]</a></td><td>iRod. Web Page. URL: <a class="reference external" href="https://irods.org/">https://irods.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="github-irods-www" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id366">[313]</a></td><td>iRod. Web Page. URL: <a class="reference external" href="https://github.com/irods/irods">https://github.com/irods/irods</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-netcdf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[314]</td><td><em>(<a class="fn-backref" href="#id367">1</a>, <a class="fn-backref" href="#id369">2</a>)</em> UCAR Edward&nbsp;Hartnett and Rew RK. Experience with an enhanced netcdf data model and interface for scientific data access. In <em>24th Conference on IIPS</em>. 2008.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-netcdf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id368">[315]</a></td><td>NetCDF: Introduction and Overview. Web Page. URL: <a class="reference external" href="https://www.unidata.ucar.edu/software/netcdf/docs_rc/">https://www.unidata.ucar.edu/software/netcdf/docs_rc/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cdf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id370">[316]</a></td><td>NASA/GSFC. Cdf home page. webpage, 2017. accessed: 2017-1-28. URL: <a class="reference external" href="http://cdf.gsfc.nasa.gov/">http://cdf.gsfc.nasa.gov/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="user-guide-cdf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id371">[317]</a></td><td>NASA/GSFC. <em>CDF User&#8217;s Guide (V3.3.6)</em>. NASA/GSFC Space Physics Data Facility, NASA / Goddard Space Flight Center, Greenbelt, Maryland 20771 (U.S.A.), version 3.3.6 edition, October 2016. Accessed: 2017-1-28. URL: <a class="reference external" href="http://spdf.gsfc.nasa.gov/pub/software/cdf/doc/cdf363/cdf363ug.pdf">http://spdf.gsfc.nasa.gov/pub/software/cdf/doc/cdf363/cdf363ug.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-digitalpreserve" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id372">[318]</a></td><td>Cdf,common data format (multidimensional datasets). Web page, March 2014. Accessed: 2017-1-28. URL: <a class="reference external" href="http://www.digitalpreservation.gov/formats/fdd/fdd000226.shtml#useful">http://www.digitalpreservation.gov/formats/fdd/fdd000226.shtml#useful</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wiki-hdf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id373">[319]</a></td><td>Wikipedia. Hierarchical data format  wikipedia, the free encyclopedia&#8221;. Web Page, February 2017. Online,accessed: 2017-1-28. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Hierarchical_Data_Format">https://en.wikipedia.org/wiki/Hierarchical_Data_Format</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fits-nasa" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id374">[320]</a></td><td>Fits nasa. web. URL: <a class="reference external" href="https://fits.gsfc.nasa.gov/">https://fits.gsfc.nasa.gov/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-news-fits-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id375">[321]</a></td><td>Fits news. Web Page. URL: <a class="reference external" href="https://fits.gsfc.nasa.gov/fits_standard.html">https://fits.gsfc.nasa.gov/fits_standard.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fits-vatican-library" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id376">[322]</a></td><td>Fits vatican library. Web Page. URL: <a class="reference external" href="https://www.vatlib.it/home.php?pag=digitalizzazione&amp;ling=eng">https://www.vatlib.it/home.php?pag=digitalizzazione&amp;ling=eng</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fits-matlab" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id377">[323]</a></td><td>Fits matlab. Web Page. URL: <a class="reference external" href="https://www.mathworks.com/help/matlab/import_export/importing-flexible-image-transport-system-fits-files.html?requestedDomain=www.mathworks.com">https://www.mathworks.com/help/matlab/import_export/importing-flexible-image-transport-system-fits-files.html?requestedDomain=www.mathworks.com</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-fits-2011" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id378">[324]</a></td><td><em>Astronomical Image Processing with Hadoop</em>, volume 442, Astronomical Data Analysis Software and Systems XX. ASP Conference Proceedings, July 2011. URL: <a class="reference external" href="http://adsabs.harvard.edu/abs/2011ASPC..442...93W">http://adsabs.harvard.edu/abs/2011ASPC..442...93W</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-rcfile" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[325]</td><td><em>(<a class="fn-backref" href="#id379">1</a>, <a class="fn-backref" href="#id380">2</a>)</em> Rcfilecat - apache hive. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="https://cwiki.apache.org/confluence/display/Hive/RCFileCat">https://cwiki.apache.org/confluence/display/Hive/RCFileCat</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="he2011rcfile" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id381">[326]</a></td><td>Yongqiang He, Rubao Lee, Yin Huai, Zheng Shao, Namit Jain, Xiaodong Zhang, and Zhiwei Xu. Rcfile: a fast and space-efficient data placement structure in mapreduce-based warehouse systems. In <em>Data Engineering (ICDE), 2011 IEEE 27th International Conference on</em>, 11991208. IEEE, 2011.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-orc-docs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id382">[327]</a></td><td>Background. web-page. URL: <a class="reference external" href="https://orc.apache.org/docs/">https://orc.apache.org/docs/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-parquet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id383">[328]</a></td><td>Parquet. Accessed: 02-06-2017. URL: <a class="reference external" href="https://parquet.apache.org/documentation/latest">https://parquet.apache.org/documentation/latest</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bittorrent" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id384">[329]</a></td><td>Bittorrent. Web Page, 2017. URL: <a class="reference external" href="https://www.lifewire.com/how-torrent-downloading-works-2483513">https://www.lifewire.com/how-torrent-downloading-works-2483513</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ftp-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id385">[330]</a></td><td>Ftp wikipedia. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="https://cwiki.apache.org/confluence/display/Hive/RCFileCat">https://cwiki.apache.org/confluence/display/Hive/RCFileCat</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-rfc114" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id386">[331]</a></td><td>Rfc114 specification. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="https://cwiki.apache.org/confluence/display/Hive/RCFileCat">https://cwiki.apache.org/confluence/display/Hive/RCFileCat</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ssh-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id387">[332]</a></td><td>Ssh - wikipedia. Web Page. Accessed: 2017-01-26. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Secure_Shell">https://en.wikipedia.org/wiki/Secure_Shell</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openssh-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id388">[333]</a></td><td>Openssh - wikipedia. Web Page. Accessed: 2017-01-26. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/OpenSSH">https://en.wikipedia.org/wiki/OpenSSH</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-globusonline" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id389">[334]</a></td><td>Globus online (gridftp). Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/GridFTP">https://en.wikipedia.org/wiki/GridFTP</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ibm-flume" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id391">[335]</a></td><td>IBM. What is flume? web page. URL: <a class="reference external" href="https://www-01.ibm.com/software/data/infosphere/hadoop/flume/">https://www-01.ibm.com/software/data/infosphere/hadoop/flume/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sqoop" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id392">[336]</a></td><td>The Apache&nbsp;Software Foundation. Web Page. URL: <a class="reference external" href="http://sqoop.apache.org/">http://sqoop.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="sqoop-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id393">[337]</a></td><td>Wikipedia. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Sqoop">https://en.wikipedia.org/wiki/Sqoop</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mesos" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id394">[338]</a></td><td>Mesos site. Web Page. URL: <a class="reference external" href="http://mesos.apache.org/">http://mesos.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-mesos-abu-dbai-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id395">[339]</a></td><td>Abed Abu-Dbai, David Breitgand, Gidon Gershinsky, Alex Glikson, and Khalid Ahmed. Enterprise resource management in mesos clusters. In <em>Proceedings of the 9th ACM International on Systems and Storage Conference</em>, SYSTOR &#8216;16, 17:117:1. New York, NY, USA, 2016. ACM. URL: <a class="reference external" href="http://doi.acm.org/10.1145/2928275.2933272">http://doi.acm.org/10.1145/2928275.2933272</a>, <a class="reference external" href="https://doi.org/10.1145/2928275.2933272">doi:10.1145/2928275.2933272</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-mesos-ghodsi2011dominant" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id396">[340]</a></td><td>Ali Ghodsi, Matei Zaharia, Benjamin Hindman, Andy Konwinski, Scott Shenker, and Ion Stoica. Dominant resource fairness: fair allocation of multiple resource types. In <em>NSDI</em>, volume&nbsp;11, 2424. 2011.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-mesos-ignazio-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id397">[341]</a></td><td>Roger Ignazio. <em>Mesos in Action</em>. Manning Publications Co., Greenwich, CT, USA, 1st edition, 2016. ISBN 1617292923. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?id=3006364">http://dl.acm.org/citation.cfm?id=3006364</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudera" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id398">[342]</a></td><td>cloudera. Untangling apache hadoop yarn part 1 cluster and yarn basics. Web Page, 2015. URL: <a class="reference external" href="https://blog.cloudera.com/blog/2015/09/untangling-apache-hadoop-yarn-part-1/">https://blog.cloudera.com/blog/2015/09/untangling-apache-hadoop-yarn-part-1/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-architecture" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id399">[343]</a></td><td>Difference between application manager and application master in yarn? StackOverflow, 2015. URL: <a class="reference external" href="http://stackoverflow.com/questions/30967247/difference-between-application-manager-and-application-master-in-yarn">http://stackoverflow.com/questions/30967247/difference-between-application-manager-and-application-master-in-yarn</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hadoopapache" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id400">[344]</a></td><td>Hadoop Apache. Apache software foundation. Web Page, 2016. URL: <a class="reference external" href="https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/YARN.html">https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/YARN.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibm-helix-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id401">[345]</a></td><td>Exploring big data with helix: finding needles in a big haystack. Web Page. URL: <a class="reference external" href="https://sigmodrecord.org/publications/sigmodRecord/1412/pdfs/09_industry_Ellis.pdf">https://sigmodrecord.org/publications/sigmodRecord/1412/pdfs/09_industry_Ellis.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="celery" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id402">[346]</a></td><td>Celery. Webpage. URL: <a class="reference external" href="http://www.celeryproject.org/">http://www.celeryproject.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="celerydocs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id403">[347]</a></td><td>Celery - distributed task queue. Web Page. URL: <a class="reference external" href="http://docs.celeryproject.org/en/latest/index.html">http://docs.celeryproject.org/en/latest/index.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="htcondor" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id404">[348]</a></td><td>What is htcondor? Web Page. URL: <a class="reference external" href="https://research.cs.wisc.edu/htcondor/description.html">https://research.cs.wisc.edu/htcondor/description.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-slurm" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id405">[349]</a></td><td>Slurm. Web Page. URL: <a class="reference external" href="https://slurm.schedmd.com/">https://slurm.schedmd.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-slurmschedmdsite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id406">[350]</a></td><td>SchedMd. Slurm website. Web Page, March 2013. Accessed: 2017-1-28. URL: <a class="reference external" href="https://slurm.schedmd.com/overview.html">https://slurm.schedmd.com/overview.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-slurmplatformssite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id407">[351]</a></td><td>SchedMd. Slurm supported platforms. Web Page, April 2015. Accessed: 2017-1-28. URL: <a class="reference external" href="https://slurm.schedmd.com/platforms.html">https://slurm.schedmd.com/platforms.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pilot-job-falkon-paper-2007" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id408">[352]</a></td><td>Ioan Raicu, Yong Zhao, Catalin Dumitrescu, Ian Foster, and Mike Wilde. Falkon: a fast and light-weight task execution framework. In <em>ACM SC07</em>. 2007.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pilot-job-htcaas-paper-2007" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id409">[353]</a></td><td>Jik-Soo Kim, Seungwoo Rho, Seoyoung Kim, Sangwan Kim, Seokkyoo Kim, and Soonwook Hwang. Htcaas: leveraging distributed supercomputing infrastructures for large-scale scientific computing. In <em>ACM MTAGS 13</em>. 2013.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-pilot-job-paper-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id410">[354]</a></td><td>Matteo Turilli, Mark Santcroos, and Shantenu Jha. A comprehensive perspective on pilot-job systems. In <em>ACM arXiv</em>, 126. March 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-f4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id412">[355]</a></td><td>Subramanian Muralidhar, Wyatt Lloyd, Sabyasachi Roy, Cory Hill, Ernest Lin, Weiwen Liu, Satadru Pan, Shiva Shankar, Viswanath Sivakumar, Linpeng Tang, and others. F4: facebooks warm blob storage system. In <em>Proceedings of the 11th USENIX conference on Operating Systems Design and Implementation</em>, 383398. USENIX Association, 2014.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wiki-cinder" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[356]</td><td><em>(<a class="fn-backref" href="#id413">1</a>, <a class="fn-backref" href="#id415">2</a>)</em> Cinder - openstack. Web Page. Accessed: 2017-1-21. URL: <a class="reference external" href="https://wiki.openstack.org/wiki/Cinder">https://wiki.openstack.org/wiki/Cinder</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-cinder" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id414">[357]</a></td><td>Dan Radez. <em>OpenStack Essentials</em>. Packt Publishing Ltd., 2015. ISBN 978-1-78398-708-5. URL: <a class="reference external" href="http://ebook.konfigurasi.net/Openstack/OpenStack%20Essentials.pdf">http://ebook.konfigurasi.net/Openstack/OpenStack%20Essentials.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ceph" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id416">[358]</a></td><td>Red Hat, Inc. Ceph homepage-ceph. Web Page, 2017. Accessed: 2017-1-26. URL: <a class="reference external" href="https://ceph.com/">https://ceph.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cepharch" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id417">[359]</a></td><td>Red Hat,Inc. Ceph architecture-ceph documentation. Web Page, 2017. Accessed: 2017-1-24. URL: <a class="reference external" href="http://docs.ceph.com/docs/master/architecture/">http://docs.ceph.com/docs/master/architecture/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-ceph" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id418">[360]</a></td><td>Sage&nbsp;A. Weil, Scott&nbsp;A. Brandt, Ethan&nbsp;L. Miller, Darrell D.&nbsp;E. Long, and Carlos Maltzahn. Ceph: a scalable, high-performance distributed file system. In <em>Proceedings of the 7th symposium on Operating systems design and implementation</em>, OSDI &#8216;06, 307320. Berkeley, CA, USA, November 2006. USENIX Association. Accessed: 2017-1-26. URL: <a class="reference external" href="https://www.usenix.org/legacy/event/osdi06/tech/full_papers/weil/weil.pdf">https://www.usenix.org/legacy/event/osdi06/tech/full_papers/weil/weil.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cephfs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id419">[361]</a></td><td>Red Hat,Inc. Ceph filesystem - ceph documentation. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://docs.ceph.com/docs/master/cephfs/">http://docs.ceph.com/docs/master/cephfs/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fuse" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id420">[362]</a></td><td>Fuse site. Web Page. URL: <a class="reference external" href="http://fuse.sourceforge.net">http://fuse.sourceforge.net</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="fuse-paper-hptfs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id421">[363]</a></td><td>Xianbo Zhang, David Du, Jim Hughes, Ravi Kavuri, and Sun StorageTek. Hptfs: a high performance tape file system. In <em>Proceedings of 14th NASA Goddard/23rd IEEE conference on Mass Storage System and Technologies</em>. Ieee Computer Society (September 1995), 2006. URL: <a class="reference external" href="https://www.dtc.umn.edu/publications/reports/2006_11.pdf">https://www.dtc.umn.edu/publications/reports/2006_11.pdf</a>, <a class="reference external" href="https://doi.org/10.1.1.184.1133">doi:10.1.1.184.1133</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="fuse-paper-cloudbb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id422">[364]</a></td><td>T.&nbsp;Xu, K.&nbsp;Sato, and S.&nbsp;Matsuoka. Cloudbb: scalable i/o accelerator for shared cloud storage. In <em>2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)</em>, 509518. Institute of Electrical and Electronics Engineers (IEEE), Dec 2016. <a class="reference external" href="https://doi.org/10.1109/ICPADS.2016.0074">doi:10.1109/ICPADS.2016.0074</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-lustre" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id423">[365]</a></td><td>Lustre. Webpage. Accessed: 2017-01-28. URL: <a class="reference external" href="http://lustre.org/">http://lustre.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikigpfs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[366]</td><td><em>(<a class="fn-backref" href="#id424">1</a>, <a class="fn-backref" href="#id425">2</a>, <a class="fn-backref" href="#id426">3</a>, <a class="fn-backref" href="#id427">4</a>)</em> Wikipedia. Ibm general parallel file system. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/IBM_General_Parallel_File_System">https://en.wikipedia.org/wiki/IBM_General_Parallel_File_System</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-spectrumscale" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id428">[367]</a></td><td>IBM. Ibm spectrum scale. Web Page. accessed 2017-01-29. URL: <a class="reference external" href="http://www-03.ibm.com/systems/storage/spectrum/scale/">http://www-03.ibm.com/systems/storage/spectrum/scale/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-gffs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id429">[368]</a></td><td>Gffs. Webpage. Accessed: 2017-01-28. URL: <a class="reference external" href="http://genesis2.virginia.edu/wiki/Main/GFFS">http://genesis2.virginia.edu/wiki/Main/GFFS</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-amazon-s3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id430">[369]</a></td><td>Amazon s3. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="https://aws.amazon.com/s3/">https://aws.amazon.com/s3/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-amazon-s3-docs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id431">[370]</a></td><td>Using Amazon S3. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="http://docs.aws.amazon.com/AmazonS3/latest/gsg/CopyingAnObject.html">http://docs.aws.amazon.com/AmazonS3/latest/gsg/CopyingAnObject.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azure-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[371]</td><td><em>(<a class="fn-backref" href="#id432">1</a>, <a class="fn-backref" href="#id434">2</a>)</em> An Introduction to Windows Azure BLOB Storage. Web Page, July 2013. URL: <a class="reference external" href="https://www.simple-talk.com/cloud/cloud-data/an-introduction-to-windows-azure-blob-storage/">https://www.simple-talk.com/cloud/cloud-data/an-introduction-to-windows-azure-blob-storage/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azure-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id433">[372]</a></td><td>Get started with Azure Blob storage (object storage) using .NET \textbar  Microsoft Docs. Web Page. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/storage/storage-dotnet-how-to-use-blobs">https://docs.microsoft.com/en-us/azure/storage/storage-dotnet-how-to-use-blobs</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-google-cloud-storage" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id435">[373]</a></td><td>Google cloud storage. Accessed: 02-06-2017. URL: <a class="reference external" href="https://cloud.google.com/storage/docs">https://cloud.google.com/storage/docs</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-libvirt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id436">[374]</a></td><td>Libvirt virtualization api. Web Page. URL: <a class="reference external" href="https://libvirt.org/">https://libvirt.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibm" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id437">[375]</a></td><td>Tim Jones. Anatomy of the libvirt virtualization. Webpage, 01 2010. URL: <a class="reference external" href="https://www.ibm.com/developerworks/library/l-libvirt/">https://www.ibm.com/developerworks/library/l-libvirt/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="cloud-portability-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id438">[376]</a></td><td>Antonio&nbsp;Esposito Beniamino Di&nbsp;Martino, Giuseppina&nbsp;Cretella. <em>Cloud Portability and Interoperability</em>. Springer International Publishing, New York City, USA, illustrated edition, 2015. ISBN 331913700X, 9783319137001.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jclouds" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id439">[377]</a></td><td>JClouds - the java multi-cloud toolkit. Web Page. Accessed: 2017-01-20. URL: <a class="reference external" href="https://jclouds.apache.org/">https://jclouds.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-occi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id440">[378]</a></td><td>Open&nbsp;Grid Forum. Open cloud computing interface. Web Page. Accessed: 2017-1-17. URL: <a class="reference external" href="http://occi-wg.org/">http://occi-wg.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nyren-edmonds-papaspyrou-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id441">[379]</a></td><td>Ralf Nyren, Andy Edmonds, Alesander Papaspyrou, Thijs Metsch, and Boris Parak. Open cloud computing interface core. OGF Published Document GWD-R-P.221, Global Grid Forum, Open Grid Forum, P.O. Box 1738, Muncie IN 47308, USA, September 2016. Accessed: 2017-1-17. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.221.pdf">https://www.ogf.org/documents/GFD.221.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="drescher-parak-wallom-2015" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id442">[380]</a></td><td>Michel Drescher, Boris Parak, and David Wallom. Occi compute resource templates profile. OGF Published Document GWD-R-P.222, Global Grid Forum, Open Grid Forum, P.O. Box 1738, Muncie IN 47308, USA, April 2015. Accessed: 2017-1-17. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.222.pdf">https://www.ogf.org/documents/GFD.222.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nyren-edmonds-metsch-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id443">[381]</a></td><td>Ralf Nyren, Andy Edmonds, and Thijs&nbsp;Metsch atnd Boris&nbsp;Parak. Open cloud computing interface - http protocol. OGF Published Document GWD-R-P.223, Global Grid Forum, Open Grid Forum, P.O. Box 1738, Muncie IN 47308, USA, September 2016. Accessed: 2017-1-17. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.223.pdf">https://www.ogf.org/documents/GFD.223.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nyren-feldhaus-parak-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id444">[382]</a></td><td>Ralf Nyren, Florian Feldhaus, Boris Parak, and Zdenek Sustr. Open cloud computing interface -json rendering. OGF Published Document GWD-R-P.226, Global Grid Forum, Open Grid Forum, P.O. Box 1738, Muncie IN 47308, USA, September 2016. Accessed: 2017-1-17. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.226.pdf">https://www.ogf.org/documents/GFD.226.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="edmonds-metsch-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id445">[383]</a></td><td>Andy Edmonds and Thijs Metsch. Open cloud computing interface - text rendering. OGF Published Document GWD-R-P.229, Global Grid Forum, Open Grid Forum, P.O. Box 1738, Muncie IN 47308, USA, September 2016. Accessed: 2017-1-17. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.229.pdf">https://www.ogf.org/documents/GFD.229.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sniawebsite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id446">[384]</a></td><td>Storage Networking&nbsp;Industry Association. About the SNIA. Web Page. Accessed: 02-19-2017. URL: <a class="reference external" href="https://www.snia.org/about">https://www.snia.org/about</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="cdmi-manual" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id447">[385]</a></td><td>Storage Networking&nbsp;Industry Association. <em>Cloud Data Management Interface</em>. Storage Networking Industry Association, Colorado Springs, CO, 1.1.1 edition, March 2015. Accessed: 2017-02-19. URL: <a class="reference external" href="https://www.snia.org/sites/default/files/CDMI_Spec_v1.1.1.pdf">https://www.snia.org/sites/default/files/CDMI_Spec_v1.1.1.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cdmiwebsite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id448">[386]</a></td><td>Storage Networking&nbsp;Industry Association. Cloud data management interface. Web Page, March 2015. Accessed: 02-19-2017. URL: <a class="reference external" href="https://www.snia.org/cdmi">https://www.snia.org/cdmi</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="saga-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[387]</td><td><em>(<a class="fn-backref" href="#id449">1</a>, <a class="fn-backref" href="#id451">2</a>)</em> Shantenu Jha, Hartmut Kaiser, Andre Merzky, and Ole Weidner. Grid interoperability at the application level using saga. In <em>07 Proceedings of the Third IEEE International Conference on e-Science and Grid Computing</em>. December 2007.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-saga-ogf-document" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id450">[388]</a></td><td>Open grid forum - document. webpage. Accessed : 02-01-2017. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.90.pdf">https://www.ogf.org/documents/GFD.90.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-puppet-wiki-puppet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[389]</td><td><em>(<a class="fn-backref" href="#id452">1</a>, <a class="fn-backref" href="#id455">2</a>)</em> Puppet software. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Puppet_(software)">https://en.wikipedia.org/wiki/Puppet_(software)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-puppet-puppet-site" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id453">[390]</a></td><td>Puppet faq. Web Page. URL: <a class="reference external" href="https://puppet.com/product/faq">https://puppet.com/product/faq</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-puppet-slashroot" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id454">[391]</a></td><td>How puppet works. Web Page. URL: <a class="reference external" href="http://www.slashroot.in/puppet-tutorial-how-does-puppet-work">http://www.slashroot.in/puppet-tutorial-how-does-puppet-work</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="chef-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id456">[392]</a></td><td>Matthias Marschall. <em>Chef Infrastructure Automation Cookbook</em>. Packt Publishing, 2013. ISBN 9351105164 and 9789351105169.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-chef-commercial" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id457">[393]</a></td><td>Chef commercial support. Web Page. URL: <a class="reference external" href="https://www.chef.io/support/">https://www.chef.io/support/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ansible" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id458">[394]</a></td><td>Ansible. Webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Ansible_(software)">https://en.wikipedia.org/wiki/Ansible_(software)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ansible2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id459">[395]</a></td><td>Ansible. Webpage. URL: <a class="reference external" href="https://docs.ansible.com/ansible/index.html">https://docs.ansible.com/ansible/index.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-boto" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id460">[396]</a></td><td>boto components. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="http://boto.cloudhackers.com/en/latest/">http://boto.cloudhackers.com/en/latest/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-boto-github" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id461">[397]</a></td><td>boto-github components. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://github.com/boto/boto3">https://github.com/boto/boto3</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-boto3-documentation" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id462">[398]</a></td><td>boto3-documentation components. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://boto3.readthedocs.io/en/latest/">https://boto3.readthedocs.io/en/latest/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-boto-amazon-python-sdk" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id463">[399]</a></td><td>boto-amazon-python-sdk components. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://aws.amazon.com/sdk-for-python/">https://aws.amazon.com/sdk-for-python/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cobbler" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id464">[400]</a></td><td>Cobbler. Web Page. Accessed: 2017-02-05. URL: <a class="reference external" href="http://www.theregister.co.uk/2008/06/19/red_hat_summit_2008_cobbler/">http://www.theregister.co.uk/2008/06/19/red_hat_summit_2008_cobbler/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="razorwiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id465">[401]</a></td><td>PuppetLabsRazor. Puppetlabsrazor. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://github.com/puppetlabs/Razor/wiki">https://github.com/puppetlabs/Razor/wiki</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="razorpuppet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id466">[402]</a></td><td>PuppetLabsRazor. Puppetlabsrazor. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://puppet.com/blog/introducing-razor-a-next-generation-provisioning-solution">https://puppet.com/blog/introducing-razor-a-next-generation-provisioning-solution</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="juju-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id467">[403]</a></td><td>Kent Baxley, JD&nbsp;la&nbsp;Rosa, and Mark Wenning. Deploying workloads with juju and maas in ubuntu 14.04 lts. In <em>Deploying workloads with Juju and MAAS in Ubuntu 14.04 LTS</em>. Dell Inc, Technical White Paper, may 2014.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-juju" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id468">[404]</a></td><td>Canonical Ltd. Juju. Web Page. URL: <a class="reference external" href="https://www.ubuntu.com/cloud/juju">https://www.ubuntu.com/cloud/juju</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-heat-blog-introduction" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id469">[405]</a></td><td>Blog on introduction to heat. webpage. Accessed : 01-15-2017. URL: <a class="reference external" href="http://blog.scottlowe.org/2014/05/01/an-introduction-to-openstack-heat/">http://blog.scottlowe.org/2014/05/01/an-introduction-to-openstack-heat/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-heat-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id470">[406]</a></td><td>Wikipedia link for heat. webpage. Accessed : 01-15-2017. URL: <a class="reference external" href="https://wiki.openstack.org/wiki/Heat">https://wiki.openstack.org/wiki/Heat</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openstack" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id471">[407]</a></td><td>Openstack. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="https://www.openstack.org/">https://www.openstack.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sahara" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id472">[408]</a></td><td>Sahara. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="http://docs.openstack.org/developer/sahara/">http://docs.openstack.org/developer/sahara/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="cis1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id473">[409]</a></td><td>Cloud and systems management. webpage. URL: <a class="reference external" href="http://www.cisco.com/c/en/us/products/cloud-systems-management">http://www.cisco.com/c/en/us/products/cloud-systems-management</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="cis2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id474">[410]</a></td><td>Stuart Miniman. Cisco moves up the cloud stack with intelligent automation. webpage, 2011. URL: <a class="reference external" href="http://wikibon.org/wiki/v/Cisco_Moves_Up_the_Cloud_Stack_with_Intelligent_Automation">http://wikibon.org/wiki/v/Cisco_Moves_Up_the_Cloud_Stack_with_Intelligent_Automation</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-facetup" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id475">[411]</a></td><td>Prashanth. Facebook tupperware. Blog, December 2015. Accessed:2/18/2017. URL: <a class="reference external" href="http://blog.cspp.in/index.php/2015/12/17/facebooks-tupperware/">http://blog.cspp.in/index.php/2015/12/17/facebooks-tupperware/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikichef" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id476">[412]</a></td><td>Wikipedia. Chef (software). Web Page, January 2017. accessed 2017-01-26. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Chef_(software)">https://en.wikipedia.org/wiki/Chef_(software)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-awsopsworks" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id477">[413]</a></td><td>Amazon. Aws opsworks. Web Page. accessed 2017-01-25. URL: <a class="reference external" href="https://aws.amazon.com/opsworks/">https://aws.amazon.com/opsworks/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ironicwebsite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id478">[414]</a></td><td>Devananda Van&nbsp;Der Veen and Llama Wrangler. Introduction to Ironic. Web Page, March 2015. Accessed: 02-19-2017. URL: <a class="reference external" href="https://docs.openstack.org/developer/ironic/deploy/user-guide.html">https://docs.openstack.org/developer/ironic/deploy/user-guide.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kubernetesdoc" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id479">[415]</a></td><td>Kubernetes site. Web Page. URL: <a class="reference external" href="https://kubernetes.io/docs/whatisk8s/">https://kubernetes.io/docs/whatisk8s/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kuberneteswiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id480">[416]</a></td><td>Kubernetes wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Kubernetes">https://en.wikipedia.org/wiki/Kubernetes</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="plassnig15" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id481">[417]</a></td><td>Moritz Plassnig. Heroku-style application deployments with docker - dzone cloud. Web Page, November 2015. Accessed: 2017-1-17. URL: <a class="reference external" href="https://dzone.com/articles/heroku-style-application-deployments-with-docker">https://dzone.com/articles/heroku-style-application-deployments-with-docker</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="github-buildstep" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id482">[418]</a></td><td>Jose Gonzalez and Jeff Lindsay. Buildstep. Code repository, July 2015. Accessed: 2017-1-24. URL: <a class="reference external" href="https://github.com/progrium/buildstep">https://github.com/progrium/buildstep</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-winery" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id483">[419]</a></td><td>Eclipse winery. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://projects.eclipse.org/projects/soa.winery">https://projects.eclipse.org/projects/soa.winery</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="winery-paper-2013" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[420]</td><td><em>(<a class="fn-backref" href="#id484">1</a>, <a class="fn-backref" href="#id485">2</a>, <a class="fn-backref" href="#id486">3</a>)</em> Oliver Kopp, Tobias Binz, Uwe Breitenbcher, and Frank Leymann. Winery  a modeling tool for tosca-based cloud applications. In <em>11\textsuperscript th International Conference on Service-Oriented Computing</em>, 700704. Springer, December 2013.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-blueprints" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id487">[421]</a></td><td>Exercise: analyze business processes with ibm bpm blueprint. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://www.ibm.com/developerworks/downloads/soasandbox/blueprint.html">http://www.ibm.com/developerworks/downloads/soasandbox/blueprint.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-blueworks-live2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[422]</td><td><em>(<a class="fn-backref" href="#id488">1</a>, <a class="fn-backref" href="#id490">2</a>)</em> Blueworkslive. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://www.blueworkslive.com/home">https://www.blueworkslive.com/home</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-blueworks-live" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id489">[423]</a></td><td>Ibm blueworks live. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/IBM_Blueworks_Live">https://en.wikipedia.org/wiki/IBM_Blueworks_Live</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-terraform" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id491">[424]</a></td><td>Terraform components. Web Page. URL: <a class="reference external" href="https://www.terraform.io/intro/index.html">https://www.terraform.io/intro/index.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-terraform-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id492">[425]</a></td><td>James Turnbull. <em>The Terraform Book</em>. Number&nbsp;9780988820258. Turnbull Press; 110 edition (November 26, 2016), 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wettinger-any2api" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id493">[426]</a></td><td>Johannes Wettinger, Uwe Breitenbcher, and Frank Leymann. Any2api automated apification. In Markus Helfert, Donald Ferguson, and V\&#8217;ctor&nbsp;Mndez Muoz, editors, <em>CLOSER 2015 - Proceedings of the 5th International Conference on Cloud Computing and Services Science</em>, 475486. Lisbon, Portugal, 20-22 May 2015. SciTePress. URL: <a class="reference external" href="https://pdfs.semanticscholar.org/1cd4/4b87be8cf68ea5c4c642d38678a7b40a86de.pdf">https://pdfs.semanticscholar.org/1cd4/4b87be8cf68ea5c4c642d38678a7b40a86de.pdf</a>, <a class="reference external" href="https://doi.org/10.5220/0005472704750486">doi:10.5220/0005472704750486</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-any2api" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id494">[427]</a></td><td>Johannes Wettinger. any2api - the better way to create awesome apis. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="http://www.any2api.org/">http://www.any2api.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-xen-wikipedia" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id495">[428]</a></td><td>Xen - wikipedia. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Xen">https://en.wikipedia.org/wiki/Xen</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-xen-overview" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id496">[429]</a></td><td>Xen project overview. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://wiki.xenproject.org/wiki/Xen_Project_Software_Overview">https://wiki.xenproject.org/wiki/Xen_Project_Software_Overview</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-xen-fl" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id497">[430]</a></td><td>Xen feature list. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://wiki.xenproject.org/wiki/Xen_Project_4.7_Feature_List">https://wiki.xenproject.org/wiki/Xen_Project_4.7_Feature_List</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kvm-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id498">[431]</a></td><td>Kvm wikipedia documentation. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine">https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hypervisor" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id500">[432]</a></td><td>Wikipedia. Hypervisor. WebPage, February 2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Hypervisor">https://en.wikipedia.org/wiki/Hypervisor</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-qemuwiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id501">[433]</a></td><td>Qemu. Qemu. WebPage, February 2017. URL: <a class="reference external" href="http://wiki.qemu-project.org/index.php/Main_Page">http://wiki.qemu-project.org/index.php/Main_Page</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openvz-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id502">[434]</a></td><td>OpenVZ. Web Page, February 2017. Page Version ID: 764498783. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=OpenVZ&amp;oldid=764498783">https://en.wikipedia.org/w/index.php?title=OpenVZ&amp;oldid=764498783</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openvz-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id503">[435]</a></td><td>How To Create OpenVZ Container In OpenVZ \textbar  Unixmen. Web Page. URL: <a class="reference external" href="https://www.unixmen.com/how-to-create-openvz-container-in-openvz/">https://www.unixmen.com/how-to-create-openvz-container-in-openvz/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openvz-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id504">[436]</a></td><td>Features. Web Page. URL: <a class="reference external" href="https://openvz.org/Features">https://openvz.org/Features</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wiki-lxc" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id505">[437]</a></td><td>Linux containers. web page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/LXC">https://en.wikipedia.org/wiki/LXC</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jpablo" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id506">[438]</a></td><td>Why use lxc (linux containers) ? web page. URL: <a class="reference external" href="http://www.jpablo128.com/why-use-lxc-linux-containers/">http://www.jpablo128.com/why-use-lxc-linux-containers/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-infoworld" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id507">[439]</a></td><td>Linux containers in use. web page. URL: <a class="reference external" href="http://www.infoworld.com/article/3072929/linux/containers-101-linux-containers-and-docker-explained.html">http://www.infoworld.com/article/3072929/linux/containers-101-linux-containers-and-docker-explained.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-opennebula-org" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id509">[440]</a></td><td>About opennebula. URL: <a class="reference external" href="https://opennebula.org/about/technology/">https://opennebula.org/about/technology/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-opennebula-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id510">[441]</a></td><td>Opennebula wiki. URL: <a class="reference external" href="https://opennebula.org/about/technology/">https://opennebula.org/about/technology/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-opennebula" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id511">[442]</a></td><td>R.&nbsp;Moreno-Vozmediano, R.&nbsp;S. Montero, and I.&nbsp;M. Llorente. Iaas cloud architecture from virtualized datacenters to federated cloud infrastructures. In IEEE, editor, <em>Computer</em>, volume&nbsp;45, 6572. IEEE Computer Society, IEEE, 2012. URL: <a class="reference external" href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6165242&amp;isnumber=6383143">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6165242&amp;isnumber=6383143</a>, <a class="reference external" href="https://doi.org/10.1109/MC.2012.76">doi:10.1109/MC.2012.76</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-eucalyptus" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id512">[443]</a></td><td>Daniel Nurmi, Rich Wolski, Chris Grzegorczyk, Graziano Obertelli, Sunil Soman, Lamia Youseff, and Dmitrii Zagorodnov. The eucalyptus open-source cloud-computing system. In <em>Proceedings of the 2009 9th IEEE/ACM International Symposium on Cluster Computing and the Grid</em>, 124131. IEEE Computer Society, 2009.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-eucalyptus" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id513">[444]</a></td><td>The eucalyptus open-source private cloud. Web Page. Accessed: 2017-02-11. URL: <a class="reference external" href="http://www.cloudbook.net/resources/stories/the-eucalyptus-open-source-private-cloud">http://www.cloudbook.net/resources/stories/the-eucalyptus-open-source-private-cloud</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-nimbus-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id514">[445]</a></td><td>Nimbus wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Nimbus_(cloud_computing)">https://en.wikipedia.org/wiki/Nimbus_(cloud_computing)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-nimbus" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id515">[446]</a></td><td>Nimbus. Web Page. URL: <a class="reference external" href="http://www.nimbusproject.org/doc/nimbus/platform/">http://www.nimbusproject.org/doc/nimbus/platform/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nimbus-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id516">[447]</a></td><td><em>Rebalancing in a multi-cloud environment in Science Cloud &#8216;13</em>, ACM, 2013. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?id=2465854">http://dl.acm.org/citation.cfm?id=2465854</a>, <a class="reference external" href="https://doi.org/10.1145/2465848.2465854">doi:10.1145/2465848.2465854</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudstack" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id517">[448]</a></td><td>Cloud Stack. Webpage. URL: <a class="reference external" href="https://cloudstack.apache.org/">https://cloudstack.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudstack2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id518">[449]</a></td><td>Cloud Stack. Webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Apache_CloudStack">https://en.wikipedia.org/wiki/Apache_CloudStack</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-core" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id519">[450]</a></td><td>CoreOS. Why coreos. Web Page, January 2017. accessed: 2017-01-23. URL: <a class="reference external" href="https://coreos.com/why/">https://coreos.com/why/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-coreos" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id520">[451]</a></td><td>Coreos. Web Page. Accessed:1/27/2017. URL: <a class="reference external" href="https://www.coreos.com/">https://www.coreos.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-github-rkt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id521">[452]</a></td><td>Coreos/rkt. Web Page. Accessed:1/27/2017. URL: <a class="reference external" href="https://github.com/coreos/rkt/">https://github.com/coreos/rkt/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wiki-vmwareesxi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id522">[453]</a></td><td>Wikipedia. web-page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/VMware_ESXi">https://en.wikipedia.org/wiki/VMware_ESXi</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="vmware-esxi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id523">[454]</a></td><td>VMware. web-page. URL: <a class="reference external" href="http://www.vmware.com/products/esxi-and-esx.html">http://www.vmware.com/products/esxi-and-esx.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-vmware" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id524">[455]</a></td><td>vmware. Vcloud. Webpage. URL: <a class="reference external" href="http://www.vmware.com/products/vcloud-suite.html">http://www.vmware.com/products/vcloud-suite.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mustbegeek" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id525">[456]</a></td><td>Bipin. Difference between vsphere, esxi and vcenter. Webpage, 08 2012. URL: <a class="reference external" href="http://www.mustbegeek.com/difference-between-vsphere-esxi-and-vcenter/">http://www.mustbegeek.com/difference-between-vsphere-esxi-and-vcenter/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hortonworks-ambari" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id526">[457]</a></td><td>Hortonworks apache ambari. Web Page. Accessed: 2017-2-04. URL: <a class="reference external" href="http://hortonworks.com/apache/ambari/">http://hortonworks.com/apache/ambari/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ambari" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id527">[458]</a></td><td>Ambari. Web Page. Accessed: 2017-2-04. URL: <a class="reference external" href="https://ambari.apache.org/">https://ambari.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-github-ambari" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id528">[459]</a></td><td>Github apache/ ambari. Web Page. Accessed: 2017-2-04. URL: <a class="reference external" href="https://github.com/apache/ambari/">https://github.com/apache/ambari/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-nagios" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id529">[460]</a></td><td>Nagios components. Web Page. Accessed: 2017-1-11. URL: <a class="reference external" href="https://www.nagios.org/projects/">https://www.nagios.org/projects/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nagios-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id530">[461]</a></td><td>David Josephsen. <em>Nagios: Building Enterprise-Grade Monitoring Infrastructures for Systems and Networks</em>. Prentice Hall Press, Upper Saddle River, NJ, USA, 2nd edition, 2013. ISBN 013313573X, 9780133135732.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nagios-paper-2012" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id531">[462]</a></td><td>C.&nbsp;Issariyapat, P.&nbsp;Pongpaibool, S.&nbsp;Mongkolluksame, and K.&nbsp;Meesublak. Using nagios as a groundwork for developing a better network monitoring system. In <em>2012 Proceedings of PICMET &#8216;12: Technology Management for Emerging Technologies</em>, 27712777. July 2012.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="inca-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id532">[463]</a></td><td>Jinjun&nbsp;Chen Lizhe&nbsp;Wang, Wei&nbsp;Jie. <em>Grid Computing: Infrastructure, Service, and Applications</em>. Taylor &amp; Francis, 6000 Broken Sound Parkway NW, Suite 300 Boca Raton, FL 33487, 2009. ISBN 1420067664.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-inca" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id533">[464]</a></td><td>Inca - periodic, automated, user-level cyberinfrastructure testing. Web Page. Accessed: 2017-01-16. URL: <a class="reference external" href="http://inca.sdsc.edu/">http://inca.sdsc.edu/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-eduroam" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id534">[465]</a></td><td>Eduroam. Web Page. URL: <a class="reference external" href="https://www.eduroam.org/about/">https://www.eduroam.org/about/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="eduroam-paper-2005" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id535">[466]</a></td><td>Licia Florio and Klaas Wierenga. Eduroam, providing mobility for roaming users. <em>TERENA</em>, 2005. URL: <a class="reference external" href="https://www.terena.org/activities/tf-mobility/docs/ppt/eunis-eduroamfinal-LF.pdf">https://www.terena.org/activities/tf-mobility/docs/ppt/eunis-eduroamfinal-LF.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-keystone-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id536">[467]</a></td><td>Keystone wiki. Web Page. URL: <a class="reference external" href="https://wiki.openstack.org/wiki/Keystone">https://wiki.openstack.org/wiki/Keystone</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="cui2015security" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id537">[468]</a></td><td>Baojiang Cui and Tao Xi. Security analysis of openstack keystone. In <em>Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), 2015 9th International Conference on</em>, 283288. IEEE, 2015.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudberrylab-kstn" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id538">[469]</a></td><td>Cloudberrylab website. Web Page. URL: <a class="reference external" href="https://www.cloudberrylab.com/blog/openstack-keystone-authentication-explained/">https://www.cloudberrylab.com/blog/openstack-keystone-authentication-explained/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-keystone" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id539">[470]</a></td><td>Openstack website. Web Page. URL: <a class="reference external" href="http://docs.openstack.org/developer/keystone/architecture.html">http://docs.openstack.org/developer/keystone/architecture.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ldap" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id540">[471]</a></td><td>LDAP. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="http://searchmobilecomputing.techtarget.com/definition/LDAP">http://searchmobilecomputing.techtarget.com/definition/LDAP</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sentry" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id541">[472]</a></td><td>Apache sentry website. Web Page. URL: <a class="reference external" href="https://cwiki.apache.org/confluence/display/SENTRY/Sentry+Tutorial">https://cwiki.apache.org/confluence/display/SENTRY/Sentry+Tutorial</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ope1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id542">[473]</a></td><td>Openid. website. URL: <a class="reference external" href="http://openid.net/">http://openid.net/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ope2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id543">[474]</a></td><td>Openid. webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/OpenID">https://en.wikipedia.org/wiki/OpenID</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="saml" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id544">[475]</a></td><td>Abhijeet Sandil. Sso strategy: authentication (saml) vs authorization (oauth). Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://www.linkedin.com/pulse/sso-strategy-authentication-vs-authorization-saml-oauth-sandil">https://www.linkedin.com/pulse/sso-strategy-authentication-vs-authorization-saml-oauth-sandil</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-chubby" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id545">[476]</a></td><td>Mike Burrows. Chubby site. Web Page. URL: <a class="reference external" href="https://research.google.com/archive/chubby.html">https://research.google.com/archive/chubby.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="chubby-paper-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id546">[477]</a></td><td>A.&nbsp;Ailijiang, A.&nbsp;Charapko, and M.&nbsp;Demirbas. Consensus in the cloud: paxos systems demystified. In <em>2016 25th International Conference on Computer Communication and Networks (ICCCN)</em>, 110. August 2016. URL: <a class="reference external" href="http://ieeexplore.ieee.org/abstract/document/7568499/">http://ieeexplore.ieee.org/abstract/document/7568499/</a>, <a class="reference external" href="https://doi.org/10.1109/ICCCN.2016.7568499">doi:10.1109/ICCCN.2016.7568499</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zoo-overiew" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id547">[478]</a></td><td>Zookeeper - overview. Web Page. Accessed: 2017-01-23. URL: <a class="reference external" href="https://zookeeper.apache.org/doc/trunk/zookeeperOver.html">https://zookeeper.apache.org/doc/trunk/zookeeperOver.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zoo-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id548">[479]</a></td><td>Zookeeper - wikipedia. Web Page. Accessed: 2017-01-23. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Apache_ZooKeeper">https://en.wikipedia.org/wiki/Apache_ZooKeeper</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zoo-ibm" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id549">[480]</a></td><td>Ibm - what is zookeeper. Web Page. Accessed: 2017-01-23. URL: <a class="reference external" href="http://www-01.ibm.com/software/data/infosphere/hadoop/zookeeper/">http://www-01.ibm.com/software/data/infosphere/hadoop/zookeeper/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="giraffepaper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id550">[481]</a></td><td>Xuanhua Shi, Haohong Lin, Hai Jin, Bing&nbsp;Bing Zhou, Zuoning Yin, Sheng Di, and Song Wu. Giraffe: a scalable distributed coordination service for large-scale systems. In <em>GIRAFFE: A Scalable Distributed Coordination Service for Large-scale Systems</em>, 110. September 2014. URL: <a class="reference external" href="http://www.mcs.anl.gov/papers/P5157-0714.pdf">http://www.mcs.anl.gov/papers/P5157-0714.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="git-thrift" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id551">[482]</a></td><td>About thrift. Web Page. Accessed: 2017-02-12. URL: <a class="reference external" href="https://thrift.apache.org/">https://thrift.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-protobuf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id552">[483]</a></td><td>Protocol buffer. Web Page, September 2016. URL: <a class="reference external" href="https://developers.google.com/protocol-buffers/">https://developers.google.com/protocol-buffers/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-snort" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id553">[484]</a></td><td>Wikipedia. Snort software. Web Page, February 2017. Accessed: 2017-02-12. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Snort_(software)">https://en.wikipedia.org/wiki/Snort_(software)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fiddler" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id554">[485]</a></td><td>Wikipedia. Fiddler software. Web Page, February 2017. Accessed: 2017-02-12. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Fiddler_(software)">https://en.wikipedia.org/wiki/Fiddler_(software)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zeppelinwebsite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id555">[486]</a></td><td>Apache Zeppelin. Apache zeppelin 0.7.0 documentation. Web Page, February 2017. Accessed: 02-19-2017. URL: <a class="reference external" href="https://zeppelin.apache.org/docs/0.7.0/">https://zeppelin.apache.org/docs/0.7.0/</a>.</td></tr>
</tbody>
</table>
</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2016, Gregor von Laszewski.<br/>
    </p>
  </div>
</footer>
  </body>
</html>