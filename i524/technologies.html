<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Technologies &#8212; Big Data Classes</title>
    
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/bootswatch-3.3.6/cerulean/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     'Draft',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-3.3.6/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lessons" href="../lesson/index.html" />
    <link rel="prev" title="HID Assignment" href="hids-techs.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">
          i524</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="index.html">Overview</a></li>
                <li><a href="lectures.html">Lectures</a></li>
                <li><a href="../lesson/index.html">Lessons</a></li>
                <li><a href="../changelog.html">Changes</a></li>
                <li><a href="https://github.com/cloudmesh/classes">Fork</a></li>
                <li><a href="https://piazza.com/class/ix39m27czn5uw">Piazza</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="preface/index.html">Preface</a><ul>
<li class="toctree-l2"><a class="reference internal" href="preface/about.html">About</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/disclaimer.html">Disclaimer</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/convention.html">Conventions</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/instructors.html">Instructors</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/instructors.html#teaching-assistants">Teaching Assistants</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html">i524 Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#meeting-times">Meeting Times</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#online-meetings">Online Meetings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#who-can-take-the-class">Who can take the class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#homework">Homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#open-source-publication-of-homework">Open Source Publication of Homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#piazza">Piazza</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tips-on-how-to-achieve-your-best">Tips on how to achieve your best</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#submissions">Submissions</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#selected-project-ideas">Selected Project Ideas</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#software-project">Software Project</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#report-format">Report Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#github-repositories">Github repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#code-repositories-deliverables">Code Repositories Deliverables</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#learning-outcomes">Learning Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#academic-integrity-policy">Academic Integrity Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#links">Links</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#course-numbers">Course Numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="calendar.html">I524 Calendar</a><ul>
<li class="toctree-l2"><a class="reference internal" href="calendar.html#comments">Comments</a></li>
<li class="toctree-l2"><a class="reference internal" href="calendar.html#official-university-calendar">Official University calendar</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lectures.html">I524 Lectures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures">Lectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-theory-track">Lectures - Theory Track</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-collaboration-track">Lectures - Collaboration Track</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-systems">Lectures - Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#unreleased-lectures">Unreleased Lectures</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hids-techs.html">HID Assignment</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Technologies</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#workflow-orchestration">Workflow-Orchestration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#application-and-analytics">Application and Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#application-hosting-frameworks">Application Hosting Frameworks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#high-level-programming">High level Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="#streams">Streams</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-programming-model-and-runtime-spmd-mapreduce">Basic Programming model and runtime, SPMD, MapReduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inter-process-communication-collectives">Inter process communication Collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="#in-memory-databases-caches">In-memory databases/caches</a></li>
<li class="toctree-l2"><a class="reference internal" href="#object-relational-mapping">Object-relational mapping</a></li>
<li class="toctree-l2"><a class="reference internal" href="#extraction-tools">Extraction Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sql-newsql">SQL(NewSQL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nosql">NoSQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#file-management">File management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-transport">Data Transport</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cluster-resource-management">Cluster Resource Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#file-systems">File systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interoperability">Interoperability</a></li>
<li class="toctree-l2"><a class="reference internal" href="#devops">DevOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="#iaas-management-from-hpc-to-hypervisors">IaaS Management from HPC to hypervisors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cross-cutting-functions">Cross-Cutting Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#new-technologies-to-be-integrated">New Technologies to be integrated</a></li>
<li class="toctree-l2"><a class="reference internal" href="#excersise">Excersise</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../lesson/index.html">Lessons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../lesson/cloud/index.html">Cloud (under construction)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/contrib/index.html">Contributing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/doc/index.html">Writing Documents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/linux/index.html">Linux</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/org/index.html">Oragnization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/prg/index.html">Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/data/index.html">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/projects/index.html">Software Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/projects/draft.html">Software Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/iaas/index.html">IaaS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/ansible.html">Ansible</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/ansible-start-point.html">Ansible</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/chef.html">Chef</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/comparison.html">Compaprison of Configuration management software</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/composite_cluster.html">Composite Clusters (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/deployment.html">Dynamic deployment of arbitrary X software on VC (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/hadoop.html">Hadoop Virtual CLuster (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/juju.html">Ubuntu Juju</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/mongodb_cluster.html">MongoDB Virtual Cluster (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/openmpi_cluster.html">OpenMPI Virtual Cluster (Under Preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/openstack_heat.html">OpenStack Heat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/other.html">Other (Under Preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/overview.html">DevOps (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/overview_vc.html">Overview Virtual Cluster (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/puppet.html">Puppet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/saltstack-ex1-output.html">Full Message of <code class="docutils literal"><span class="pre">salt-call</span></code> Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/saltstack.html">SaltStack</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-do-i-ask-a-question">How do I ask a question?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-prerequisites-for-this-class">What are the prerequisites for this class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-is-this-class-not-hosted-on-edx">Why is this class not hosted on EdX?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-are-you-not-using-canvas-for-communicating-with-students">Why are you not using CANVAS for communicating with students?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-are-you-using-github-for-submitting-projects-and-papers">Why are you using github for submitting projects and papers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-full-time-student-at-iupui-can-i-take-the-online-version">I am full time student at IUPUI. Can I take the online version?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-a-residential-student-at-iu-can-i-take-the-online-version-only">I am a residential student at IU. Can I take the online version only?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#the-class-is-full-what-do-i-do">The class is full what do I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-to-buy-a-textbook">Do I need to buy a textbook?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-is-there-no-textbook">Why is there no textbook?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-a-computer-to-participate-in-this-class">Do I need a computer to participate in this class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#representative-bibliography">Representative Bibliography</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#where-is-the-official-iu-calendar-for-the-fall">Where is the official IU calendar for the Fall?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-write-a-research-article-on-computer-science">How to write a research article on computer science?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#which-bibliography-manager-is-required-for-the-class">Which bibliography manager is required for the class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-use-endnote-or-other-bibliography-managers">Can I use endnote or other bibliography managers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#plagiarism-test-and-resources-related-to-that">Plagiarism test and resources related to that</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-many-hours-will-this-course-take-to-work-on-every-week">How many hours will this course take to work on every week?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#is-all-classes-material-final">Is all classes material final?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-changes-to-the-web-page">What are the changes to the web page?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-lectures-should-i-learn-when">What lectures should I learn when?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-are-you-doing-the-papers">I524: Why are you doing the papers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-are-there-no-homework-to-test-me-on-skills-such-as-ansible-or-python">I524: Why are there no homework to test me on skills such as ansible or python?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-not-use-chef-or-another-devops-framework">I524: Why not use chef or another DevOps framework?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-lost">I am lost?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-do-not-like-technology-topic-project-etc">I do not like Technology/Topic/Project/etc?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-not-able-to-attend-the-online-hours">I am not able to attend the online hours</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-to-attend-the-online-sessions">Do I need to attend the online sessions?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-leaning-outcomes">What are the leaning outcomes?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#there-are-so-many-messages-on-piazza-i-can-not-keep-up">There are so many messages on Piazza I can not keep up.</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-find-the-hosting-web-confusing">I find the hosting Web confusing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-i-do-not-know-python-what-do-i-do">I524: I do not know python. What do I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-solve-merge-conflict-in-pull-request">How to solve merge conflict in Pull Request?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#building-cloudmesh-classes-in-local-machine">Building cloudmesh/classes in local machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-sole-merge-conflict-in-a-pull-request">How to sole Merge Conflict in a Pull Request?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#cheat-sheet-for-linux-commands">Cheat sheet for Linux commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#tips-techlist-1-homework">Tips: TechList.1 homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#techlist-1-and-paper-1-pagecount">Techlist 1 and Paper 1 : Pagecount</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#tips-to-install-virtualbox">Tips to Install Virtualbox</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-generate-the-ssh-key-on-ubuntu-vm">Do I generate the SSH key on Ubuntu VM ?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#ways-to-run-ubuntu-on-windows-10">Ways to run Ubuntu on Windows 10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#don-t-use-anaconda">Don&#8217;t use Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#using-ssh-key-for-git-push">Using SSH Key for Git Push</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-properly-research-a-bibtex-entry">How to properly research a bibtex entry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-differnt-entry-types-and-fields">What are the differnt entry types and fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-is-the-nature-of-team-collaboration-on-papers">What is the nature of team collaboration on papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-due-dates-for-assignments">What are the due dates for assignments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-good-places-to-find-refernce-entries">What are good places to find refernce entries?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../todo.html">Todos</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../todo.html#general">General</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-unreleased">%%version%% (unreleased)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id1">3.0.9 (2017-01-30)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id2">3.0.8 (2017-01-22)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id6">3.0.7 (2017-01-20)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id11">3.0.6 (2017-01-11)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id14">3.0.5 (2017-01-11)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id19">3.0.4 (2017-01-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id20">3.0.3 (2017-01-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id23">3.0.2 (2017-01-07)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id24">3.0.1 (2017-01-06)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id25">3.0 (2017-01-06)</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Technologies</a><ul>
<li><a class="reference internal" href="#workflow-orchestration">Workflow-Orchestration</a></li>
<li><a class="reference internal" href="#application-and-analytics">Application and Analytics</a></li>
<li><a class="reference internal" href="#application-hosting-frameworks">Application Hosting Frameworks</a></li>
<li><a class="reference internal" href="#high-level-programming">High level Programming</a></li>
<li><a class="reference internal" href="#streams">Streams</a></li>
<li><a class="reference internal" href="#basic-programming-model-and-runtime-spmd-mapreduce">Basic Programming model and runtime, SPMD, MapReduce</a></li>
<li><a class="reference internal" href="#inter-process-communication-collectives">Inter process communication Collectives</a></li>
<li><a class="reference internal" href="#in-memory-databases-caches">In-memory databases/caches</a></li>
<li><a class="reference internal" href="#object-relational-mapping">Object-relational mapping</a></li>
<li><a class="reference internal" href="#extraction-tools">Extraction Tools</a></li>
<li><a class="reference internal" href="#sql-newsql">SQL(NewSQL)</a></li>
<li><a class="reference internal" href="#nosql">NoSQL</a></li>
<li><a class="reference internal" href="#file-management">File management</a></li>
<li><a class="reference internal" href="#data-transport">Data Transport</a></li>
<li><a class="reference internal" href="#cluster-resource-management">Cluster Resource Management</a></li>
<li><a class="reference internal" href="#file-systems">File systems</a></li>
<li><a class="reference internal" href="#interoperability">Interoperability</a></li>
<li><a class="reference internal" href="#devops">DevOps</a></li>
<li><a class="reference internal" href="#iaas-management-from-hpc-to-hypervisors">IaaS Management from HPC to hypervisors</a></li>
<li><a class="reference internal" href="#cross-cutting-functions">Cross-Cutting Functions</a><ul>
<li><a class="reference internal" href="#monitoring">Monitoring</a></li>
<li><a class="reference internal" href="#security-privacy">Security &amp; Privacy</a></li>
<li><a class="reference internal" href="#distributed-coordination">Distributed Coordination</a></li>
<li><a class="reference internal" href="#message-and-data-protocols">Message and Data Protocols</a></li>
</ul>
</li>
<li><a class="reference internal" href="#new-technologies-to-be-integrated">New Technologies to be integrated</a></li>
<li><a class="reference internal" href="#excersise">Excersise</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
              <li class="hidden-sm"></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="preface/index.html">Preface</a><ul>
<li class="toctree-l2"><a class="reference internal" href="preface/about.html">About</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/disclaimer.html">Disclaimer</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/convention.html">Conventions</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/instructors.html">Instructors</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface/instructors.html#teaching-assistants">Teaching Assistants</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html">i524 Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#meeting-times">Meeting Times</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#online-meetings">Online Meetings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#who-can-take-the-class">Who can take the class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#homework">Homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#open-source-publication-of-homework">Open Source Publication of Homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#piazza">Piazza</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tips-on-how-to-achieve-your-best">Tips on how to achieve your best</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#submissions">Submissions</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#selected-project-ideas">Selected Project Ideas</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#software-project">Software Project</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#report-format">Report Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#github-repositories">Github repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#code-repositories-deliverables">Code Repositories Deliverables</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#learning-outcomes">Learning Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#academic-integrity-policy">Academic Integrity Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#links">Links</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#course-numbers">Course Numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="calendar.html">I524 Calendar</a><ul>
<li class="toctree-l2"><a class="reference internal" href="calendar.html#comments">Comments</a></li>
<li class="toctree-l2"><a class="reference internal" href="calendar.html#official-university-calendar">Official University calendar</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lectures.html">I524 Lectures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures">Lectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-theory-track">Lectures - Theory Track</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-collaboration-track">Lectures - Collaboration Track</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#lectures-systems">Lectures - Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="lectures.html#unreleased-lectures">Unreleased Lectures</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hids-techs.html">HID Assignment</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Technologies</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#workflow-orchestration">Workflow-Orchestration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#application-and-analytics">Application and Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#application-hosting-frameworks">Application Hosting Frameworks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#high-level-programming">High level Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="#streams">Streams</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-programming-model-and-runtime-spmd-mapreduce">Basic Programming model and runtime, SPMD, MapReduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inter-process-communication-collectives">Inter process communication Collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="#in-memory-databases-caches">In-memory databases/caches</a></li>
<li class="toctree-l2"><a class="reference internal" href="#object-relational-mapping">Object-relational mapping</a></li>
<li class="toctree-l2"><a class="reference internal" href="#extraction-tools">Extraction Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sql-newsql">SQL(NewSQL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nosql">NoSQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#file-management">File management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-transport">Data Transport</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cluster-resource-management">Cluster Resource Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#file-systems">File systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interoperability">Interoperability</a></li>
<li class="toctree-l2"><a class="reference internal" href="#devops">DevOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="#iaas-management-from-hpc-to-hypervisors">IaaS Management from HPC to hypervisors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cross-cutting-functions">Cross-Cutting Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#new-technologies-to-be-integrated">New Technologies to be integrated</a></li>
<li class="toctree-l2"><a class="reference internal" href="#excersise">Excersise</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../lesson/index.html">Lessons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../lesson/cloud/index.html">Cloud (under construction)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/contrib/index.html">Contributing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/doc/index.html">Writing Documents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/linux/index.html">Linux</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/org/index.html">Oragnization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/prg/index.html">Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/data/index.html">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/projects/index.html">Software Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/projects/draft.html">Software Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/iaas/index.html">IaaS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/ansible.html">Ansible</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/ansible-start-point.html">Ansible</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/chef.html">Chef</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/comparison.html">Compaprison of Configuration management software</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/composite_cluster.html">Composite Clusters (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/deployment.html">Dynamic deployment of arbitrary X software on VC (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/hadoop.html">Hadoop Virtual CLuster (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/juju.html">Ubuntu Juju</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/mongodb_cluster.html">MongoDB Virtual Cluster (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/openmpi_cluster.html">OpenMPI Virtual Cluster (Under Preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/openstack_heat.html">OpenStack Heat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/other.html">Other (Under Preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/overview.html">DevOps (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/overview_vc.html">Overview Virtual Cluster (under preparation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/puppet.html">Puppet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/saltstack-ex1-output.html">Full Message of <code class="docutils literal"><span class="pre">salt-call</span></code> Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lesson/devops/saltstack.html">SaltStack</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-do-i-ask-a-question">How do I ask a question?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-prerequisites-for-this-class">What are the prerequisites for this class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-is-this-class-not-hosted-on-edx">Why is this class not hosted on EdX?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-are-you-not-using-canvas-for-communicating-with-students">Why are you not using CANVAS for communicating with students?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-are-you-using-github-for-submitting-projects-and-papers">Why are you using github for submitting projects and papers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-full-time-student-at-iupui-can-i-take-the-online-version">I am full time student at IUPUI. Can I take the online version?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-a-residential-student-at-iu-can-i-take-the-online-version-only">I am a residential student at IU. Can I take the online version only?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#the-class-is-full-what-do-i-do">The class is full what do I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-to-buy-a-textbook">Do I need to buy a textbook?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-is-there-no-textbook">Why is there no textbook?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-a-computer-to-participate-in-this-class">Do I need a computer to participate in this class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#representative-bibliography">Representative Bibliography</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#where-is-the-official-iu-calendar-for-the-fall">Where is the official IU calendar for the Fall?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-write-a-research-article-on-computer-science">How to write a research article on computer science?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#which-bibliography-manager-is-required-for-the-class">Which bibliography manager is required for the class?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-use-endnote-or-other-bibliography-managers">Can I use endnote or other bibliography managers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#plagiarism-test-and-resources-related-to-that">Plagiarism test and resources related to that</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-many-hours-will-this-course-take-to-work-on-every-week">How many hours will this course take to work on every week?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#is-all-classes-material-final">Is all classes material final?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-changes-to-the-web-page">What are the changes to the web page?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-lectures-should-i-learn-when">What lectures should I learn when?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-are-you-doing-the-papers">I524: Why are you doing the papers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-are-there-no-homework-to-test-me-on-skills-such-as-ansible-or-python">I524: Why are there no homework to test me on skills such as ansible or python?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-why-not-use-chef-or-another-devops-framework">I524: Why not use chef or another DevOps framework?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-lost">I am lost?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-do-not-like-technology-topic-project-etc">I do not like Technology/Topic/Project/etc?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-am-not-able-to-attend-the-online-hours">I am not able to attend the online hours</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-need-to-attend-the-online-sessions">Do I need to attend the online sessions?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-leaning-outcomes">What are the leaning outcomes?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#there-are-so-many-messages-on-piazza-i-can-not-keep-up">There are so many messages on Piazza I can not keep up.</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-find-the-hosting-web-confusing">I find the hosting Web confusing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i524-i-do-not-know-python-what-do-i-do">I524: I do not know python. What do I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-solve-merge-conflict-in-pull-request">How to solve merge conflict in Pull Request?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#building-cloudmesh-classes-in-local-machine">Building cloudmesh/classes in local machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-sole-merge-conflict-in-a-pull-request">How to sole Merge Conflict in a Pull Request?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#cheat-sheet-for-linux-commands">Cheat sheet for Linux commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#tips-techlist-1-homework">Tips: TechList.1 homework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#techlist-1-and-paper-1-pagecount">Techlist 1 and Paper 1 : Pagecount</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#tips-to-install-virtualbox">Tips to Install Virtualbox</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#do-i-generate-the-ssh-key-on-ubuntu-vm">Do I generate the SSH key on Ubuntu VM ?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#ways-to-run-ubuntu-on-windows-10">Ways to run Ubuntu on Windows 10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#don-t-use-anaconda">Don&#8217;t use Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#using-ssh-key-for-git-push">Using SSH Key for Git Push</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-to-properly-research-a-bibtex-entry">How to properly research a bibtex entry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-differnt-entry-types-and-fields">What are the differnt entry types and fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-is-the-nature-of-team-collaboration-on-papers">What is the nature of team collaboration on papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-the-due-dates-for-assignments">What are the due dates for assignments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-are-good-places-to-find-refernce-entries">What are good places to find refernce entries?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../todo.html">Todos</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../todo.html#general">General</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-unreleased">%%version%% (unreleased)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id1">3.0.9 (2017-01-30)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id2">3.0.8 (2017-01-22)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id6">3.0.7 (2017-01-20)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id11">3.0.6 (2017-01-11)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id14">3.0.5 (2017-01-11)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id19">3.0.4 (2017-01-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id20">3.0.3 (2017-01-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id23">3.0.2 (2017-01-07)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id24">3.0.1 (2017-01-06)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#id25">3.0 (2017-01-06)</a></li>
</ul>
</li>
</ul>
</ul>
</li><ul>
<li><a class="reference internal" href="#">Technologies</a><ul>
<li><a class="reference internal" href="#workflow-orchestration">Workflow-Orchestration</a></li>
<li><a class="reference internal" href="#application-and-analytics">Application and Analytics</a></li>
<li><a class="reference internal" href="#application-hosting-frameworks">Application Hosting Frameworks</a></li>
<li><a class="reference internal" href="#high-level-programming">High level Programming</a></li>
<li><a class="reference internal" href="#streams">Streams</a></li>
<li><a class="reference internal" href="#basic-programming-model-and-runtime-spmd-mapreduce">Basic Programming model and runtime, SPMD, MapReduce</a></li>
<li><a class="reference internal" href="#inter-process-communication-collectives">Inter process communication Collectives</a></li>
<li><a class="reference internal" href="#in-memory-databases-caches">In-memory databases/caches</a></li>
<li><a class="reference internal" href="#object-relational-mapping">Object-relational mapping</a></li>
<li><a class="reference internal" href="#extraction-tools">Extraction Tools</a></li>
<li><a class="reference internal" href="#sql-newsql">SQL(NewSQL)</a></li>
<li><a class="reference internal" href="#nosql">NoSQL</a></li>
<li><a class="reference internal" href="#file-management">File management</a></li>
<li><a class="reference internal" href="#data-transport">Data Transport</a></li>
<li><a class="reference internal" href="#cluster-resource-management">Cluster Resource Management</a></li>
<li><a class="reference internal" href="#file-systems">File systems</a></li>
<li><a class="reference internal" href="#interoperability">Interoperability</a></li>
<li><a class="reference internal" href="#devops">DevOps</a></li>
<li><a class="reference internal" href="#iaas-management-from-hpc-to-hypervisors">IaaS Management from HPC to hypervisors</a></li>
<li><a class="reference internal" href="#cross-cutting-functions">Cross-Cutting Functions</a><ul>
<li><a class="reference internal" href="#monitoring">Monitoring</a></li>
<li><a class="reference internal" href="#security-privacy">Security &amp; Privacy</a></li>
<li><a class="reference internal" href="#distributed-coordination">Distributed Coordination</a></li>
<li><a class="reference internal" href="#message-and-data-protocols">Message and Data Protocols</a></li>
</ul>
</li>
<li><a class="reference internal" href="#new-technologies-to-be-integrated">New Technologies to be integrated</a></li>
<li><a class="reference internal" href="#excersise">Excersise</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<p></p>
<hr>
<small>
<b>Links</b>
<ul>
  <li><a href="https://cloudmesh.github.io/classes/faq.html">FAQ</a> </li>  
  <li><a href="https://cloudmesh.github.io/classes/i524/index.html#online-meetings">Meetings</a> </li>
  <li><a
  href="https://iu.instructure.com/courses/1603897/assignments/syllabus">Zoom</a>
  <li> <a href="https://cloudmesh.github.io/classes/i524/technologies.html#excersise">HW Techlist.1</a>
  </li>
  <li> <a href="https://cloudmesh.github.io/classes/i524/paper1-hw.html#paper-1-homework">HW Paper.1</a>
  </li>
  <li> <a href="https://cloudmesh.github.io/classes/i524/python-homework.html">HW Python</a>
  </li>
  <li> <a href="https://cloudmesh.github.io/classes/i524/calendar.html#i524-calendar">Calendar</a>
  </li>
  <li> <a href="https://cloudmesh.github.io/classes/"> <img src="https://cloudmesh.github.io/classes/_static/html.jpg" width="40" height="40" alt="pdf"> Web Page</a>
  </li>
  <li>
    <a href="https://cloudmesh.github.io/classes/i524-notes.pdf">
       <img src="https://cloudmesh.github.io/classes/_static/pdf.png" width="40" height="40" alt="pdf"> (Incomplete)</a>
  </li>
</ul>
</small>

<p></p>
<hr>
<form action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
        </div>
      </div>
    <div class="col-md-9 content">
      
  <div class="section" id="technologies">
<span id="index-0"></span><h1>Technologies<a class="headerlink" href="#technologies" title="Permalink to this headline"></a></h1>
<p>In this section we find a number of technologies that are related to
big data. Certainly a number of these projects are hosted as an Apache
project. One important resource for a general list of all apache
projects is at</p>
<ul class="simple">
<li>Apache projects: <a class="reference external" href="https://projects.apache.org/projects.html?category">https://projects.apache.org/projects.html?category</a></li>
</ul>
<div class="section" id="workflow-orchestration">
<h2>Workflow-Orchestration<a class="headerlink" href="#workflow-orchestration" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p class="first">ODE</p>
<p>Apache ODE (Orchestration Director Engine) is an open source
implementation of the WS-BPEL 2.0 standard. WS- BPEL which stands for
Web Services Business Process Execution Language, is an executable
language for writing business processes with web services <a class="reference internal" href="#www-bpel-wiki" id="id1">[1]</a>.
It includes control structures like conditions or loops as well as
elements to invoke web services and receive messages from services.
ODE uses WSDL (Web Services Description Language) for interfacing
with web services <a class="reference internal" href="#www-ode-wiki" id="id2">[2]</a>. Naming a few of its features,
It supports two communication layers for&nbsp;interacting with the outside
world, one based on Axis2 (Web Services http transport) and another
one based on the JBI standard. It also supports both long and short
living process executions for orchestrating services for applications <a class="reference internal" href="#www-ode-web" id="id3">[3]</a>.</p>
</li>
<li><p class="first">ActiveBPEL</p>
</li>
<li><p class="first">Airavata</p>
</li>
<li><p class="first">Pegasus</p>
<p>The Pegasus <a class="reference internal" href="#www-pegasus" id="id4">[4]</a> is workflow management system
that alows to compose and execute a workflow in an application
in different environment without the need  for any
modifications.&nbsp;It allows users to make high level workflow
without thinking about the low level details. It locates
the required input data and computational resources automatically.
Pegasus also maintains information about tasks done and data
produced. In case of errors Pegasus tries to recover by retrying
the whole workflow and providing check pointing at workflow-level.
It cleans up the storage as the workflow gets executed so that
data-intensive workflows can have enough required space to execute
on storage-constrained resources. Some of the other advantages of
Pegasus are:scalability, reliability and high performance. Pegasus
has been used in many scientific domains like astronomy,
bioinformatics, earthquake science , ocean science, gravitational
wave physics and others.</p>
</li>
<li><p class="first">Kepler</p>
</li>
<li><p class="first">Swift</p>
</li>
<li><p class="first">Taverna</p>
<p>Taverna is workflow management system. According to
<a class="reference internal" href="#www-taverna" id="id5">[5]</a>, Taverna is transitioning to Apache Incubator
as of Jan 2017.  Taverna suite includes 2 products:</p>
<p>(1). Taverna Workbench is desktop client where user can define the workflow.
(2). Taverna Server is responsible for executing the remote workflows.</p>
<p>Taverna workflows can also be executed on command-line.  Taverna
supports wide range of services including WSDL-style and RESTful
Web Services, BioMart, SoapLab, R, and Excel. Taverna also support
mechanism to monitor the running workflows using its web browser
interface.  In the <a class="reference internal" href="#taverna-paper" id="id6">[6]</a> paper, the formal syntax and
operational semantics of Taverna is explained.</p>
</li>
<li><p class="first">Triana</p>
</li>
<li><p class="first">Trident</p>
<p>In <a class="reference internal" href="#www-trident-tutorial" id="id7">[7]</a>, it is explained that Apache Trident
is a &#8220;high-level abstraction for doing realtime computing on top of
[Apache] Storm.&#8221; Similarly to Apache Storm, Apache Trident was
developed by Twitter. Furthermore, <a class="reference internal" href="#www-trident-tutorial" id="id8">[7]</a>
introduces Trident as a tool that &#8220;allows you to seamlessly intermix
high throughput (millions of messages per second), stateful stream
processing with low latency distributed querying.&#8221; In
<a class="reference internal" href="#www-trident-overview" id="id9">[8]</a>, the five kinds of operations in
Trident are described as &#8220;Operations that apply locally to each
partition and cause no network transfer&#8221;, &#8220;repartitioning operations
that repartition a stream but otherwise don&#8217;t change the contents
(involves network transfer)&#8221;, &#8220;aggregation operations that do
network transfer as part of the operation&#8221;, &#8220;operations on grouped
streams&#8221; and &#8220;merges and joins.&#8221; In <a class="reference internal" href="#www-trident-tutorial" id="id10">[7]</a>,
these five kinds of operations (i.e. joins, aggregations, grouping,
functions, and filters) and the general concepts of Apache Trident
are described as similar to &#8220;high level batch processing tools like
Pig or Cascading.&#8221;</p>
</li>
<li><p class="first">BioKepler</p>
<p>BioKepler is a Kepler module of scientific workflow components to
execute a set of bioinformatics tools using distributed execution
patterns <a class="reference internal" href="#www-biokepler" id="id11">[9]</a>. It contains a specialized set of
actors called bioActors for running bioinformatic tools,
directors providing distributed data-parallel(DPP) execution on
Big Data platforms such as Hadoop and Spark they are also
configurable and reusable <a class="reference internal" href="#www-biokepler-demos" id="id12">[10]</a>. BioKepler
contains over 40 example workflows that demonstrate the actors and
directors <a class="reference internal" href="#bioactors" id="id13">[11]</a>.</p>
</li>
<li><p class="first">Galaxy</p>
<p>Ansible Galaxy is a website platform and command line tool that
enables users to discover, create, and share community developed
roles. Users&#8217; GitHub accounts are used for authentication,
allowing users to import roles to share with the ansible
community. <a class="reference internal" href="#www-galaxy-ansible" id="id14">[12]</a> describes how Ansible roles
are encapsulated and reusable tools for organizing automation
content. Thus a role contains all tasks, variables, and handlers
that are necessary to complete that
role. <a class="reference internal" href="#ansible-book-2016" id="id15">[13]</a> depicts roles as the most powerful
part of Ansible as they keep playbooks simple and readable. &#8220;They
provide reusable definitions that you can include whenever you
need and customize with any variables that the role exposes.&#8221;
<a class="reference internal" href="#www-github-galaxy" id="id16">[14]</a> provides the project documents for
Ansible Galaxy on github.</p>
</li>
<li><p class="first">IPython</p>
</li>
<li><p class="first">Jupyter</p>
</li>
<li><p class="first">(Dryad)</p>
</li>
<li><p class="first">Naiad</p>
</li>
<li><p class="first">Oozie</p>
</li>
<li><p class="first">Tez</p>
</li>
<li><p class="first">Google FlumeJava</p>
</li>
<li><p class="first">Crunch</p>
</li>
<li><p class="first">Cascading</p>
<p><a class="reference internal" href="#www-cascading" id="id17">[15]</a> Cascading software authored by Chris Wensel
is development platform for building the application in Hadoop.
It basically act as an abstraction for Apache Hadoop used for
creating complex data processing workflow using the scalability of
hadoop however hiding the complexity of mapReduce jobs.  User can
write their program in java without having knowledge of
mapReduce. Applications written on cascading are portable.</p>
<p>Cascading Benefits
1. With Cascading application can be scaled as per the data sets.
2. Easily Portable
3. Single jar file for application deployment.</p>
</li>
<li><p class="first">Scalding</p>
</li>
<li><p class="first">e-Science Central</p>
<p>In <a class="reference internal" href="#e-science-central-paper-2010" id="id18">[16]</a>, it is explained
that e-Science Central is designed to address some of the
pitfalls within current Infrastructure as a Service (e.g.
Amazon EC2) and Platform as a Service (e.g. force.com)
services. For instance, in
<a class="reference internal" href="#e-science-central-paper-2010" id="id19">[16]</a>, the &#8220;majority of
potential scientific users, access to raw hardware is of
little use as they lack the skills and resources needed to
design, develop and maintain the robust, scalable
applications they require&#8221; and furthermore &#8220;current
platforms focus on services required for business
applications, rather than those needed for scientific
data storage and analysis.&#8221; In
<a class="reference internal" href="#www-e-science-central" id="id20">[17]</a>, it is explained that
e-Science Central is a &#8220;cloud based platform for
data analysis&#8221; which is &#8220;portable and can be run on
Amazon AWS, Windows Azure or your own hardware.&#8221; In
<a class="reference internal" href="#e-science-central-paper-2010" id="id21">[16]</a>, e-Science Central
is further described  as a platform, which &#8220;provides
both Software and Platform as a Service for scientific
data management, analysis and collaboration.&#8221; This
collaborative platform is designed to be scalable while
also maintaining ease of use for scientists. In
<a class="reference internal" href="#e-science-central-paper-2010" id="id22">[16]</a>, &#8220;a project
consisting of chemical modeling by cancer researchers&#8221;
demonstrates how e-Science Central &#8220;allows scientists to
upload data, edit and run workflows, and share results in
the cloud.&#8221;</p>
</li>
<li><p class="first">Azure Data Factory</p>
<p>Azure data factory is a cloud based data integration service that
can ingest data from various sources, transform/ process data and
publish the result data to the data stores. A data management
gateway enables access to data on SQL Databases
<a class="reference internal" href="#azure-df" id="id23">[18]</a>. The data processing is done by It works by
creating pipelines to transform the raw data into a format that
can be readily used by BI Tools or applications. The services
comes with rich visualization aids that aid data analysis. Data
Factory supports two types of activities: data movement activities
and data transformation activities. Data Movement <a class="reference internal" href="#azure-ms" id="id24">[19]</a>
is a Copy Activity in Data Factory that copies data from a data
source to a Data sink. Data Factory supports the following data
stores. Data from any source can be written to any sink.  Data
Transformation: Azure Data Factory supports the following
transformation activities such as Map reduce, Hive transformations
and Machine learning activities.  Data factory is a great tool to
analyze web data, sensor data and geo-spatial data.</p>
</li>
<li><p class="first">Google Cloud Dataflow</p>
<p>Google Cloud Dataflow is a unified programming model and a managed
service for developing and executing a wide variety of data processing
patterns (pipelines). Dataflow includes SDKs for defining data
processing workflows and a Cloud platform managed services to run
those workflows on a Google cloud platform resources such as Compute
Engine, BigQuery amongst others <a class="reference internal" href="#www-dataflow" id="id25">[20]</a>. Dataflow
pipelines can operate in both batch and streaming mode. The platform
resources are provided on demand, allowing users to scale to meet
their requirements, its also optimized to help balance lagging work
dynamically.</p>
<p>Being a cloud offering, Dataflow is designed to allow users to focus
on devising proper analysis without worrying about the installation
and maintaining <a class="reference internal" href="#www-googlelivestream" id="id26">[21]</a> the underlying data
piping and process infrastructure.</p>
</li>
<li><p class="first">NiFi (NSA)</p>
<p><a class="reference internal" href="#www-nifi" id="id27">[22]</a> Defines NiFi as &#8220;An Easy to use, powerful and
realiable system to process and distribute data&#8221;.
This tool aims
at automated data flow from sources with different sizes ,
formats and following diffent protocals to the centralized
location or destination. <a class="reference internal" href="#www-hortanworks" id="id28">[23]</a>.</p>
<p>This comes equipped with an easy use UI where the data flow
can be conrolled with a drag and a drop.
NiFi was initiatially developed by NSA ( called Niagarafiles )
using the concepts of flowbased
programming and latter submitted to Apachi Software
foundation. <a class="reference internal" href="#www-forbes" id="id29">[24]</a></p>
</li>
<li><p class="first">Jitterbit</p>
</li>
<li><p class="first">Talend</p>
</li>
<li><p class="first">Pentaho</p>
<p>Pentaho is a business intelligence corporation that provides data
mining, reporting, dashboarding and data integration
capabilities. Generally, organizations tend to obtain meaningful
relationships and useful information from the data present with
them. Pentaho addresses the obstacles that obstruct them from
doing so <a class="reference internal" href="#pent1" id="id30">[25]</a>. The platform includes a wide range of
tools that analyze, explore, visualize and predict data easily
which simplifies blending any data. The sole objective of pentaho
is to translate data into value. Being an open and extensible
source, pentaho provides big data tools to extract, prepare and
blend any data <a class="reference internal" href="#pent2" id="id31">[26]</a>. Along with this, the visualizations
and analytics will help in changing the path that the
organizations follow to run their business. From spark and hadoop
to noSQL, pentaho transforms big data into big insights.</p>
</li>
<li><p class="first">Apatar</p>
</li>
<li><p class="first">Docker Compose</p>
<p>Docker is an open-source container based technology.A container
allows a developer to package up an application and all its part
includig the stack it runs on, dependencies it is associated with
and everything the application requirs to run within an isolated
enviorment . Docker seperates Application from the underlying
Operating System in a similar way as Virtual Machines seperates
the Operating System from the underlying Hardware.Dockerizing an
application is very lightweight in comparison with running the
application on the Virtual Machine as all the containers share the
same underlying kernel, the Host OS should be same as the
container OS (eliminating guest OS) and an average machine cannot
have more than few VMs running o them.</p>
<p>:cite:&#8217;docker-book&#8217; Docker Machine is a tool that lets you install
Docker Engine on virtual hosts, and manage the hosts with
docker-machine commands. You can use Machine to create Docker
hosts on your local Mac or Windows box, on your company network,
in your data center, or on cloud providers like AWS or Digital
Ocean. For Docker 1.12 or higher swarm mode is integerated with
the Docker Engine, but on the older versions with Machine&#8217;s swarm
option, we can configure a swarm cluster Docker Swarm provides
native clustering capabilities to turn a group of Docker engines
into a single, virtual Docker Engine. With these pooled resources
,:cite:&#8217;www-docker&#8216;&#8220;you can scale out your application as if it
were running on a single, huge computer&#8221; as swarm can be scaled
upto 1000 Nodes or upto 50,000 containers</p>
</li>
<li><p class="first">KeystoneML</p>
</li>
</ol>
</div>
<div class="section" id="application-and-analytics">
<h2>Application and Analytics<a class="headerlink" href="#application-and-analytics" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="32">
<li><p class="first">Mahout <a class="reference internal" href="#www-mahout" id="id32">[27]</a></p>
<p>&#8220;Apache Mahout software provides three major features:
(1) A simple and extensible programming environment and framework
for building scalable algorithms
(2) A wide variety of premade algorithms for Scala + Apache Spark,
H2O, Apache Flink
(3) Samsara, a vector math experimentation environment with R-like
syntax which works at scale&#8221;</p>
</li>
<li><p class="first">MLlib</p>
</li>
<li><p class="first">Mbase</p>
</li>
<li><p class="first">DataFu</p>
<p>The Apache DataFu project was created out of the need for stable,
well-tested libraries for large scale data processing in Hadoop.
As detailed in <a class="reference internal" href="#www-datafu" id="id33">[28]</a> Apache DatFu consists of two
libraries Apache DataFu Pig and Apache DataFu Hourglass.  Apache
DataFu Pig is a collection of useful user-defined functions for
data analysis in Apache Pig. The functions are in areas of
Statistics, Bag Operations, Set Operations, Sessions, Sampling,
Estimation, Hashing and Link Analysis.  Apache DataFu Hourglass is
a library for incrementally processing data using Hadoop
MapReduce. It is designed to make computations over sliding windows
more efficient. For these types of computations, the input data is
partitioned in some way, usually according to time, and the range
of input data to process is adjusted as new data arrives.
Hourglass works with input data that is partitioned by day, as
this is a common scheme for partitioning temporal data.</p>
</li>
<li><p class="first">R</p>
<p><a class="reference internal" href="#www-r" id="id34">[29]</a> R, a GNU project, is a successor to S - a
statistical programming language. It offers a range of
capabilities  programming language, high level graphics,
interfaces to other languages and debugging. &#8220;R is an integrated
suite of software facilities for data manipulation, calculation
and graphical display&#8221;. The statistical and graphical techniques
provided by R make it popular in the statistical community. The
statistical techniques provided include linear and nonlinear
modelling, classical statistical tests, time-series analysis,
classification and clustering to name a few. <a class="reference internal" href="#book-r" id="id35">[30]</a> The
number of packages available in R has made it popular for use in
machine learning, visualization, and data operations tasks like
data extraction, cleaning, loading, transformation, analysis,
modeling and visualization. It&#8217;s strength lies in analyzing data
using its rich library but falls short when working with very
large datasets.</p>
</li>
<li><p class="first">pbdR</p>
<p>Programming with Big Data in R (pbdR) <a class="reference internal" href="#www-pbdr" id="id36">[31]</a> is an
environment having series of R packages for statistical computing
with Big Data using high-performance statistical computation. It
uses R, a popular language between statisticians and data
miners. &#8220;pbdR&#8221; focuses on distributed memory system, where data is
distributed accross several machines and processed in batch
mode. It uses MPI for inter process communications. R focuses on
single machines for data analysis using a interactive
GUI. Currenly there are two implementation of pbdR, one Rmpi and
another being pdbMpi.  Rmpi uses SPMD parallelism while pbdRMpi
uses manager/worker parallelism.</p>
</li>
<li><p class="first">Bioconductor</p>
<p>Bioconductor is an open source and open development platform used
for analysis and understanding of high throughput genomic
data. Bioconductor is used to analyze DNA microarray, flow,
sequencing, SNP, and other biological data. All contributions to
Bioconductor are under an open source
license. <a class="reference internal" href="#bioconductor-article-2004" id="id37">[32]</a> describes the goals of
Bioconductor &#8220;include fostering collaborative development and
widespread use of innovative software, reducing barriers to entry
into interdisciplinary scientific research, and promoting the
achievement of remote reproducibility of research results&#8221;
<a class="reference internal" href="#www-bioconductor-about" id="id38">[33]</a> described that Bioconductor is
primarily based on R, as most components of Bioconductor are
released in R packages. Extensive documentation is provided for
each Bioconductor package as vignettes, which include
task-oriented descriptions for the functionalities of each
package. Bioconductor has annotation functionality to associate
&#8220;genemoic data in real time with biological metadata from web
databases such as GenBank, Entrez genes and PubMed.&#8221;  Bioconductor
also has tools to process genomic annotation data.</p>
</li>
<li><p class="first">ImageJ</p>
</li>
<li><p class="first">OpenCV</p>
<p>OpenCV stands for Open source Computer Vision. It was designed for
computational efficiency and with a strong focus on real-time
applications. It has C++, C, Python and Java interfaces and
supports Windows, Linux, Mac OS, iOS and Android. It can take
advantage of the hardware acceleration of the underlying
heterogeneous compute platform as it is enabled with OpenCL(Open
Computing Language) <a class="reference internal" href="#www-opencv" id="id39">[34]</a>. OpenCV 3.2 is the latest
version of the software that is currently available
<a class="reference internal" href="#opencv-version" id="id40">[35]</a>.</p>
</li>
<li><p class="first">Scalapack</p>
</li>
<li><p class="first">PetSc</p>
</li>
<li><p class="first">PLASMA MAGMA</p>
<p>PLASMA is built to address the performance shortcomings of the LAPACK and
ScaLAPACK libraries on multicore processors and multi-socket systems of
multicore processors and their inability to efficiently utilize accelerators
such as Graphics Processing Units (GPUs). Real arithmetic and complex
arithmetic are supported in both single precision and double precision.
PLASMA has been designed by restructuring the software to achieve much
greater efficiency, where possible, on modern computers based on multicore
processors. PLASMA does not support band matrices and does not solve
eigenvalue and singular value problems. Also, PLASMA does not replace
ScaLAPACK as software for distributed memory computers, since it only
supports shared-memory machines. <a class="reference internal" href="#paper-plasma-magma-1" id="id41">[36]</a> <a class="reference internal" href="#www-plasma-1" id="id42">[37]</a>
Recent activities of major chip manufacturers, such as Intel, AMD, IBM and
NVIDIA, make it more evident than ever that future designs of
microprocessors and large HPC systems will be hybrid/heterogeneous in
nature, relying on the integration (in varying proportions) of two major
types of components: <a class="reference internal" href="#paper-plasma-magma-2" id="id43">[38]</a> <a class="reference internal" href="#paper-plasma-magma-3" id="id44">[39]</a>
1. Many-cores CPU technology, where the number of cores will continue to
escalate because of the desire to pack more and more components on a chip
while avoiding the power wall, instruction level parallelism wall, and the
memory wall;
2. Special purpose hardware and accelerators, especially Graphics Processing
Units (GPUs), which are in commodity production, have outpaced standard CPUs
in floating point performance in recent years, and have become as easy, if
not easier to program than multicore CPUs.
While the relative balance between these component types in future designs
is not clear, and will likely to vary over time, there seems to be no doubt
that future generations of computer systems, ranging from laptops to
supercomputers, will consist of a composition of heterogeneous components.
<a class="reference internal" href="#paper-plasma-magma-4" id="id45">[40]</a><a class="reference internal" href="#paper-plasma-magma-5" id="id46">[41]</a><a class="reference internal" href="#paper-plasma-magma-6" id="id47">[42]</a></p>
</li>
<li><p class="first">Azure Machine Learning</p>
<p>Azure Machine Learning is a cloud based service that can be used
to do predictive analytics, machine learning or data mining. It
has features like in-built algorithm library, machine learning
studio and a webservice <a class="reference internal" href="#www-azuremlsite" id="id48">[43]</a>. In built
algorithm library has implementation of various popular machine
learning algorithms like decision tree, SVM, linear regression,
neural networks etc. Machine learning studio facilitates creation
of predictive models using graphical user interface by dragging,
dropping and connecting of different modules that can be used by
people with minimal knowledge in the machine learning
field. Machine learning studio is a free service for basic version
and comes with a monthly charge for advanced versions. Apart from
building models, studio also has options to do preprocessing like
clean, transform and normalize the data. Webservice provides
option to deploy the machine learning algorithm as ready to
consume APIs that can be reused in future with minimal effort and
can also be published.</p>
</li>
<li><p class="first">Google Prediction API &amp; Translation API</p>
<p>Google Prediction API &amp; Translation API are part of Cloud ML API
family with specific roles. Below is a description of each and
their use.</p>
<p>Google Prediction API provides pattern-matching and machine
learning capabilities. Built on HTTP and JSON, the prediction API
uses training data to learn and consecutively use what has been
learned to predict a numeric value or choose a category that
describes new pieces of data. This makes it easier for any
standard HTTP client to send requests to it and parse the
responses. The API can be used to predict what users might like,
categorize emails as spam or non-spam, assess whether posted
comments sentiments are positive or negative or how much a user
may spend in a day. Prediction API has a 6 month limited free
trial or a paid use for $10 per project which offers up to 10,000
predictions a day <a class="reference internal" href="#www-prediction" id="id49">[44]</a>.</p>
<p>Google Translation API is a simple programmatic interface for
translating an arbitrary string into any supported
language. Google Translation API is highly responsive allowing
websites and applications to integrate for fast dynamic
translation of source text from source language to a target
language. Translation API also automatically identifies and
translate languages with a high accuracy from over a hundred
different languages.  Google Translation API is charged at $20 per
million characters making it an affordable localization
solution. Translation API is also distributed in two editions,
premium edition which is tailored for users with precise long-form
translation services like livestream, high volumes of emails or
detailed articles and documents. Theres also standard edition
which is tailored for short, real-time
conversations <a class="reference internal" href="#www-translation" id="id50">[45]</a>.</p>
</li>
<li><p class="first">mlpy</p>
<p>mlpy is an open source python library made for providing
machine learning functionality.It is built on top of popular
existing python libraries of NumPy, SciPy and GNU scientific
libraries (GSL).It also makes extensive use of Cython
language. These form the prerequisites for mlpy. <a class="reference internal" href="#dblp-journals-corr-abs-1202-6548" id="id51">[46]</a>
explains the significanceq of its components: NumPy, SciPy provide
sophisticated N-dimensional arrays, linear algebra functionality
and a variety of learning methods, GSL, which is written in C,
provides complex numerical calculation functionality.</p>
<p>mlpy provides a wide range of machine learning methods for both
supervised and unsupervised learning problems. mlpy is multiplatform
and works both on Python 2 and 3 and is distributed under GPL3. Mlpy
provides both classic and new learning algorithms for classification,
regression and dimensionality reduction. <a class="reference internal" href="#www-mlpy" id="id52">[47]</a>
provides a detailed list of functionality offered by mlpy. Though
developed for general machine learning applications, mlpy has special
applications in computational biology, particularly in functional
genomics modeling.</p>
</li>
<li><p class="first">scikit-learn</p>
<p>Scikit-learn is an open source library that provides simple and
efficient tools for data analysis and data mining. It is
accessible to everybody and reusable in various contexts. It is
built on numpy, Scipy and matplotlib and is commercially usable as
it is distributed under many linux distributions
<a class="reference internal" href="#scik1" id="id53">[48]</a>. Through a consistent interface, scikit-learn
provides a wide range of learning algorithms. Scikits are the
names given to the modules for SciPy, a fundamental library for
scientific computing and as these modules provide different
learning algorithms, the library is named as sciki-learn
<a class="reference internal" href="#scik2" id="id54">[49]</a>. It provides an in-depth focus on code quality,
performance, collaboration and documentation. Most popular models
provided by scikit-learn include clustering, cross-validation,
dimensionality reduction, parameter tuning, feature selection and
extraction.</p>
</li>
<li><p class="first">PyBrain</p>
</li>
<li><p class="first">CompLearn</p>
<p>Complearn is a system that makes use of data compression
methodologies for mining patterns in a large amount of data. So,
it is basically a compression-based machine learning system. For
identifying and learning different patterns, it provides a set of
utilities which can be used in applying standard compression
mechanisms. The most important characteristic of complearn is its
power in mining patterns even in domains that are unrelated. It
has the ability to identify and classify the language of different
bodies of text <a class="reference internal" href="#comp1" id="id55">[50]</a>. This helps in reducing the work of
providing background knowledge regarding a particular
classification. It provides such generalization through a library
that is written in ANSI C which is portable and works in many
environments <a class="reference internal" href="#comp1" id="id56">[50]</a>. Complearn provides immediate to access
every core functionality in all the major languages as it is
designed to be extensible.</p>
</li>
<li><p class="first">DAAL(Intel)</p>
</li>
<li><p class="first">Caffe</p>
<p>Caffe is a deep learning framework made with three terms namely
expression, speed and modularity <a class="reference internal" href="#www-caffe" id="id57">[51]</a>. Using Expressive
architecture, switching between CPU and GPU by setting a single
flag to train on a GPU machine then deploy to commodity cluster or
mobile devices.Here the concept of configuration file will comes
without hard coding the values . Switching between CPU and GPU can
be done by setting a flag to train on a GPU machine then deploy to
commodity clusters or mobile devices.</p>
<p>It can process over 60 million images per day with a single NVIIA
k40 GPU It is being used bu academic research projects, startup
prototypes, and even large-scale industrial applications in vision,
speech, and multimedia.</p>
</li>
<li><p class="first">Torch</p>
<p>Torch is a open source machine learning library, a scientific
computing framework <a class="reference internal" href="#www-torch" id="id58">[52]</a> .It implements LuaJIT
programming language and implements C/CUDA. It implements
N-dimensional array. It does routines of indexing, slicing,
transposing etc. It has in interface to C language via scripting
language LuaJIT. It supports different artificial intelligence
models like neural network and energy based models. It is
compatible with GPU.  The core package of is torch. It provides
a flexible N dimensional array which supports basic routings. It
has been used to build hardware implementation for data flows like
those found in neural networks.</p>
</li>
<li><p class="first">Theano
Theano is a Python library. It was written at the&nbsp;LISA&nbsp;lab.
Initially it was created with the purpose to support efficient
development of machine learning(ML) algorithms.
Theano uses recent GPUs for higher speed.
It is used to evaluate mathematical expressions and especially
those mathematical expressions that include multi-dimensional arrays.
Theanos working is dependent on combining aspects of a computer algebra
system and an optimizing compiler.
This combination of computer algebra system with optimized compilation
is highly beneficial for the tasks which involves complicated
mathematical expressions and that need to be evaluated repeatedly as
evaluation speed is highly critical in such cases.
It can also be used to generate customized C code for number of
mathematical operations.
For cases where many different expressions are there and each of them
is evaluated just once, Theano can minimize the amount of compilation
and analyses overhead <a class="reference internal" href="#www-theano" id="id59">[53]</a>.</p>
</li>
<li><p class="first">DL4j</p>
<p>DL4j stands for Deeplearning4j. <a class="reference internal" href="#www-dl4j" id="id60">[54]</a> It is a deep
learning programming library written for Java and the Java virtual
machine (JVM) and a computing framework with wide support for deep
learning algorithms. Deeplearning4j includes implementations of
the restricted Boltzmann machine, deep belief net, deep
autoencoder, stacked denoising autoencoder and recursive neural
tensor network, word2vec, doc2vec, and GloVe. These algorithms all
include distributed parallel versions that integrate with Apache
Hadoop and Spark. It is a open-source software released under
Apache License 2.0.</p>
<p>Training with Deeplearning4j occurs in a cluster. Neural nets are
trained in parallel via iterative reduce, which works on
Hadoop-YARN and on Spark. Deeplearning4j also integrates with CUDA
kernels to conduct pure GPU operations, and works with distributed
GPUs.</p>
</li>
<li><p class="first">H2O</p>
</li>
<li><p class="first">IBM Watson</p>
<p>IBM Watson <a class="reference internal" href="#www-ibmwatson-wiki" id="id61">[55]</a> is a super computer built on
cognitive technology that processes information like the way human
brain does by understanding the data in a natural language as well
as analyzing structured and unstructured data. It was initially
developed as a question and answer tool more specifically to
answer questions on the quiz show &#8220;Jeopardy&#8221; but now it has been
seen as helping doctors and nurses in the treatment of cancer. It
was developed by IBM&#8217;s DeepQA research team led by David
Ferrucci. <a class="reference internal" href="#www-ibmwatson" id="id62">[56]</a> illustrates that with Watson you
can create bots that can engage in conversation with you. You can
even provide personalized recommendations to Watson by
understanding a user&#8217;s personality, tone and emotion. Watson uses
the Apache Hadoop framework in order to process the large volume
of data needed to generate an answer by creating in-memory
datasets used at run-time. Watson&#8217;s DeepQA UIMA (Unstructured
Information Management Architecture) annotators were deployed as
mappers in the Hadoop Map-Reduce framework. Watson is written in
multiple programming languages like Java, C++, Prolog and it runs
on the SUSE Linux Enterprise Server. <a class="reference internal" href="#www-ibmwatson" id="id63">[56]</a>
mentions that today Watson is available as a set of open source
APIs and Software As a Service product as well.</p>
</li>
<li><p class="first">Oracle PGX</p>
</li>
<li><p class="first">GraphLab</p>
<p>GraphLab <a class="reference internal" href="#www-graphlab" id="id64">[57]</a> is a graph-based, distributed computation,
high performance framework for machine learning written in C++. It
is an open source project started by Prof. Carlos Guestrin of
Carnegie Mellon University in 2009, designed considering the
scale, variety and complexity of real world data. It integrates
various high level algorithms such as Stochastic Gradient Descent,
Gradient Descent &amp; Locking and provides high performance
experience. It includes scalable machine learning toolkits which
has implementation for deep learning, factor machines, topic
modeling, clustering, nearest neighbors and almost everything
required to enhance machine learning models. This framework is
targeted for sparse iterative graph algorithms. It helps data
scientists and developers easily create and install applications
at large scale.</p>
</li>
<li><p class="first">GraphX</p>
<p>GraphX is Apache Spark&#8217;s API for graph and graph-parallel computation.
<a class="reference internal" href="#www-graphx" id="id65">[58]</a></p>
<p>GraphX provides:</p>
<p>Flexibility: It seamlessly works with both graphs and collections. GraphX
unifies ETL, exploratory analysis, and iterative graph computation within a
single system. You can view the same data as both graphs and collections,
transform and join graphs with RDDs efficiently, and write custom iterative
graph algorithms using the Pregel API.</p>
<p>Speed: Its performance is comparable to the fastest specialized graph
processing systems while retaining Apache Spark&#8217;s flexibility, fault
tolerance, and ease of use.</p>
<p>Algorithms: GraphX comes with a variety of algorithms such as PageRank,
Connected Components, Label propagations, SVD++, Strongly connected
components and Triangle Count.</p>
<p>It combines the advantages of both data-parallel and graph-parallel systems
by efficiently expressing graph computataion within the Spark data-parallel
framework. <a class="reference internal" href="#www-graphx1" id="id66">[59]</a></p>
<p>It gets developed as a part of Apache Spark project. It thus gets tested and
updated with each Spark release.</p>
</li>
<li><p class="first">IBM System G</p>
</li>
<li><p class="first">GraphBuilder(Intel)</p>
</li>
<li><p class="first">TinkerPop</p>
<p>ThinkerPop is a graph computing framework from Apache software
foundation. :cite :<cite>www-ApacheTinkerPop</cite> Before coming under the
Apache project, ThinkerPop was a stack of technologies like
Blueprint, Pipes, Frames, Rexters, Furnace and Gremlin where each
part was supporting graph-based application development. Now all
parts are come under single TinkerPop project
repo. <a class="reference internal" href="#www-news" id="id67">[60]</a> It uses Gremlin, a graph traversal machine
and language. It allows user to write complex queries (traversal),
that can use for real-time transactional (OLTP) queries, graph
analytic system (OLAP) or combination of both as in
hybrid. Gremlin is written in
java. <a class="reference internal" href="#www-apachetinkerpophome" id="id68">[61]</a> TinkerPop has an ability to
create a graph in any size or complexity. Gremlin engine allows
user to write graph traversal in Gremlin language, Python,
JavaScript, Scala, Go, SQL and SPARQL. It is capable to adhere
with small graph which requires a single machine or massive graphs
that can only be possible with large cluster of machines, without
changing the code.</p>
</li>
<li><p class="first">Parasol</p>
</li>
<li><p class="first">Dream:Lab</p>
<p>DREAM:Lab stands for Distributed Research on Emerging
Applications and Machines Lab. <a class="reference internal" href="#dream" id="id69">[62]</a> DREAM:Lab is centered
around distributed systems research to enable expeditious
utilization of distributed data and computing systems. <a class="reference internal" href="#dream" id="id70">[62]</a>
DREAM:Lab utilizes the capabilities of hundereds of personal
computers to allow access to supercomputing resources to average
individuals. <a class="reference internal" href="#rao" id="id71">[63]</a> The DREAM:Lab pursues this goal by utilizing
distributed computing. <a class="reference internal" href="#rao" id="id72">[63]</a> Distributed computing consists of
independent computing resources that communicate with each other
over a network. <a class="reference internal" href="#denero" id="id73">[64]</a> A large, complex computing problem is
broken down into smaller, more manageable tasks and then these
tasks are distributed to the various components of the distributed
computing system. <a class="reference internal" href="#denero" id="id74">[64]</a></p>
</li>
<li><p class="first">Google Fusion Tables</p>
<p>Fusion Tables is a cloud based services, provided by Google for
data management and integration. Fusion Tables allow users to
upload the data in tabular format using data files like
spreadsheet, CSV, KML, .tsv up to
250MB. <a class="reference internal" href="#www-fusiontablesupport" id="id75">[65]</a> It used for data management,
visualizing data (e.g. pie-charts, bar-charts, lineplot,
scatterplot, timelines) <a class="reference internal" href="#wiki-fusiontable" id="id76">[66]</a> , sharing of
tables, filter and aggregation the data. It allows user to take
the data privately, within controlled collaborative group or in
public. It allows to integrate the data from different tables from
different users or tables.Fusion Table uses two-layer storage,
Bigtable and Magastore. The information rows are stored in bigdata
table called Rows, user can merge the multiple table in to one,
from multiple users. Megastore is a library on top of
bigtable. <a class="reference internal" href="#googlefusiontable2012" id="id77">[67]</a> Data visualization is one
the feature, where user can see the visual representation of their
data as soon as they upload it. User can store the data along with
geospatial information as well.</p>
</li>
<li><p class="first">CINET</p>
</li>
<li><p class="first">NWB</p>
<p><a class="reference internal" href="#www-nwb-edu" id="id78">[68]</a> NWB stands for Network workbench is analysis,
modelling and visualization toolkit for the network scientists.
It provides an environment which help scientist researchers and
practitioner to get online access to the shared resource
environment and network datasets for analysis, modelling and
visualization of large scale networking application.  User can
access this network datasets and algorithms previously obtained by
doing lot of research and can also add their own datasets helps in
speeding up the process and saving the time for redoing the same
analysis.</p>
<p>NWB provides advanced tools for users to understand and interact
with different types of networks.  NWB members are largely the
computer scientist, biologist, engineers, social and behavioural
scientist. The platform helps the specialist researchers to
transfer the knowledge within the broader scientific and research
communities.</p>
</li>
<li><p class="first">Elasticsearch</p>
<p>Elasticsearch <a class="reference internal" href="#www-elasticsearch" id="id79">[69]</a> is a real time
distributed, RESTful search and analytics engine which is capable
of performing full text search operations for you. It is not just
limited to full text search operations but it also allows you to
analyze your data, perform CRUD operations on data, do basic text
analysis including tokenization and
filtering. <a class="reference internal" href="#www-elasticsearch-intro" id="id80">[70]</a> For example while
developing an E-commerce website, Elasticsearch can be used to
store the entire product catalog and inventory and can be used to
provide search and autocomplete suggestions for the
products. Elasticsearch is developed in Java and is an open source
search engine which uses standard RESTful APIs and JSON on
top of Apache&#8217;s Lucene - which is a full text search engine
library. Clinton Gormley &amp; Zachary Tong <a class="reference internal" href="#elasticsearch-book" id="id81">[71]</a>
describes elastic search as &#8220;A distributed real time document
store where every field is indexed and searchable&#8221;. They also
mention that &#8220;Elastic search is capable of scaling to hundreds of
servers and petabytes of structured and unstructured
data&#8221;. <a class="reference internal" href="#www-elasticsearch-hadoop" id="id82">[72]</a> mentions that Elastic
search can be used on big data by using the Elasticsearch-Hadoop
(ES-Hadoop) connector. ES-Hadoop connector lets you index the
Hadoop data into the Elastic Stack to take full advantage of the
Elasticsearch engine and returns output through Kibana
visualizations. <a class="reference internal" href="#www-wikipedia-elasticsearch" id="id83">[73]</a> A log parsing
engine &#8220;Logstash&#8221; and analytics and visualization platform
&#8220;Kibana&#8221; are also developed alongside Elasticsearch forming a
single package.</p>
</li>
<li><p class="first">Kibana</p>
</li>
<li><p class="first">Logstash</p>
<p>Logstash is an open source data collection engine with real-time
pipelining capabilities. Logstash can dynamically unify data from
disparate sources and normalize the data into destinations of your
choice. <a class="reference internal" href="#www-logstash" id="id84">[74]</a> Cleanse and democratize all your data
for diverse advanced downstream analytics and visualization use
cases.</p>
<p>While Logstash originally drove innovation in log collection, its
capabilities extend well beyond that use case. Any type of event
can be enriched and transformed with a broad array of input,
filter, and output plugins, with many native codecs further
simplifying the ingestion process. Logstash accelerates your
insights by harnessing a greater volume and variety of data.</p>
</li>
<li><p class="first">Graylog</p>
</li>
<li><p class="first">Splunk</p>
</li>
<li><p class="first">Tableau</p>
<p><a class="reference internal" href="#www-tableau-tutorial" id="id85">[75]</a> Tableau is a family of interactive data visualization products
focused on business intelligence. The different products which
tableau has built are: Tableau Desktop, for individual use;
Tableau Server for collaboration in an organization; Tableau
Online, for Business Intelligence in the Cloud; Tableau Reader,
for reading files saved in Tableau Desktop; Tableau Public, for
journalists or anyone to publish interactive data online.
<a class="reference internal" href="#www-tableau-web" id="id86">[76]</a> Tableau uses VizQL as a  visual query language for translating
drag-and-drop actions into data queries and later expressing the
data visually. Tableau also benefits from an Advanced In-Memory
Technology for handling large amounts of data.
The strengths of Tableau are mainly the ease of use and speed.
However, it has a number of limitations, which the most prominent
are unfitness for broad business and technical user, being
closed-source, no predictive analytical capabilities and no support
for expanded analytics.</p>
</li>
<li><p class="first">D3.js</p>
</li>
<li><p class="first">three.js</p>
</li>
<li><p class="first">Potree</p>
</li>
<li><p class="first">DC.js</p>
<p>According to <a class="reference internal" href="#www-dcjs" id="id87">[77]</a>: DC.js&nbsp;is a javascript charting
library with native&nbsp;crossfilter&nbsp;support, allowing exploration on
large multi-dimensional datasets. It uses d3&nbsp;to render charts in
CSS-friendly SVG format. Charts rendered using dc.js are data
driven and reactive and therefore provide instant feedback to user
interaction. DC.js library can be used to perform data anlysis
on both mobile devices and different browsers. Under the dc
namespace the following chart classes are included: barChart,
boxplot, bubbleChart, bubbleOverlay, compositeChart, dataCount,
dataGrid, dataTable, geoChoroplethChart, heatMap,
legend,lineChart, numberDisplay, pieChart, rowChart, scatterPlot,
selectMenu and seriesChart.</p>
</li>
<li><p class="first">TensorFlow</p>
<p>TensorFlow is a platform that provides a software library for
expressing and executing machine learning
algorithms. <a class="reference internal" href="#tensorflow-paper-2016" id="id88">[78]</a> states TensorFlow has a
flexible architecture allowing it to be executed with minimal
change to many hetegeneous systems such as CPUs and GPUs of mobile
devices, desktop machines, and servers. TensorFlow can &#8220;express a
wide variety of algorithms, including training and inference
algorithms for deep neural netowrk models, and it has been used
for conducting research and for deploying machine learning systems
into production across more than a dozen
areas&#8221;. <a class="reference internal" href="#www-tensorflow" id="id89">[79]</a> describes that TensorFlow utilizes
data flow graphs in which the &#8220;nodes in the graph represent
mathematical operations, while the graph edges represent the
multidimensional data arrays (tensors) communicated between them.&#8221;
TensorFlow was developed by the Google Brain Team and has a
reference implementation that was released on 2015-11-09 under the
Apache 2.0 open source license.</p>
</li>
<li><p class="first">CNTK</p>
</li>
</ol>
</div>
<div class="section" id="application-hosting-frameworks">
<h2>Application Hosting Frameworks<a class="headerlink" href="#application-hosting-frameworks" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="80">
<li><p class="first">Google App Engine  <a class="reference internal" href="#www-gae" id="id90">[80]</a></p>
<p>On purpose we put in here a &#8220;good&#8221; example of a bad entry that woudl
receive 10 out of 100 points, e.g. an F:</p>
<p>&#8220;Google App Engine&#8221; provides platform as a service.
There are major advantages from this framework:</p>
<ol class="arabic simple">
<li>Scalable Applications</li>
<li>Easier to maintain</li>
<li>Publishing services easily</li>
</ol>
<p>Reasons: (a) &#8220;major advantages is advertisement&#8221; if you add word
major (b) grammar needs to be improved (c) the three points do not
realy say anything about Google App Engine (d) the reader will
after reading this have not much information about what it is (e)
a refernce is not included. (f) enumeration should be in this page
avoided. We like to see a number of paragraphs with text.</p>
<p><strong>Note: This is an example for a bad entry</strong></p>
</li>
<li><p class="first">AppScale</p>
<p>AppScale is an application hosting platform. This platform helps
to deploy and scale the unmodified Google App Engine application,
which run the application on any cloud infrastructure in public,
private and on premise cluster. <a class="reference internal" href="#www-appscale" id="id91">[81]</a> AppScale
provide rapid, API development platform that can run on any cloud
infrastructure. The platform separates the app logic and its
service part to have control over application deployment, data
storage, resource use, backup and migration.  AppScale is based on
Googles App Engine APIs and has support for Python, Go, PHP and
Java applications. It supports single and multimode deployment,
which will help with large, dataset or CPU. AppScale allows to
deploy app in thee main mode i.e. dev/test, production and
customize deployment.  [www-apscale-deployment]</p>
</li>
<li><p class="first">Red Hat OpenShift</p>
<p>[www-paas] OpenShift was launched as a PaaS (Platform as a
Service) by Red Hat in the Red Hat Summit, 2011.
<a class="reference internal" href="#www-developers-openshift" id="id94">[82]</a> It is a cloud application
development and hosting platform that envisages shifting of the
developer&#8217;s focus to development by automating the management and
scaling of applications.  Thus, <a class="reference internal" href="#www-openshift" id="id95">[83]</a> OpenShift
enables us to write our applications in any one web development
language (using any framework) and it itself takes up the task of
running the application on the web.  This has its advantages and
disadvantages - advantage being the developer doesn&#8217;t have to
worry about how the stuff works internally (as it is abstracted
away) and the disadvantage being that he cannot control how it
works, again because it is abstracted.</p>
<p>[openshift-blog] OpenShift is powered by Origin, which is in
turn built using Docker container packaging and Kubernetes container
cluster.  Due to this, OpenShift offers a lot of options, including
online, on-premise and open source project options.</p>
</li>
<li><p class="first">Heroku</p>
<p>Heroku <a class="reference internal" href="#www-heroku" id="id97">[84]</a> is a platform as a service that is used
for building, delivering monitoring and scaling applications. It
lets you  develop and deploy application quickly without thinking
about irrelevant problems such as infrastructure. Heroku also
provides a secure and scalable database as a service with number of
developers tools like database followers, forking, data clips and
automated health checks. It works by deploying to cedar stack
<a class="reference internal" href="#www-cedar" id="id98">[85]</a>, an online runtime environment that supports apps
buit in Java, Node.js, Scala, Clojure, Python and PHP. It uses Git
for version controlling. It is also tightly intergrated with
Salesforce, providing seamless and smooth Heroku and Salesforce
data synchronization enabling companies to develop and design creative
apps that uses both platforms.</p>
</li>
<li><p class="first">Aerobatic</p>
<p>According to <a class="reference internal" href="#www-aero" id="id99">[86]</a>: Aerobatic is a platform that allows
hosting static websites. It used to be an ad-on for Bitbucket but
now Aerobatic is transitioning to standalone CLI(command Line
Tool) and web dashboard . Aerobatic allows automatic builds to
different branches. New changes to websites can be deployed using
aero deploy command which can be executed from local desktop or
any of CD tools and services like Jenkins, Codeship,Travis and so
on.  It also allows users to configure custom error pages and
offers authentication which can also be customized. Aerobatic is
backed by AWS cloud. Aerobatic has free plan and pro plan options
for customers.</p>
</li>
<li><p class="first">AWS Elastic Beanstalk</p>
</li>
<li><p class="first">Azure</p>
<p>Microsoft Corporation (MSFT) markets its cloud products under the
<em>Azure</em> brand name. At its most basic, Azure acts as an
<em>infrastructure- as-a-service</em> (IaaS) provider.  IaaS virtualizes
hardware components, a key differentiation from other
<em>-as-a-service</em> products. IaaS &#8220;abstract[s] the user from the
details of infrasctructure like physical computing resources,
location, data partitioning, scaling, security, backup, etc.&#8221;
<a class="reference internal" href="#www-wikipedia-cloud" id="id100">[87]</a></p>
<p>However, Azure offers a host of closely-related tool and products
to enhance and improve the core product, such as raw block
storage, load balancers, and IP addresses
<a class="reference internal" href="#www-azure-msft" id="id101">[88]</a>. For instance, Azure users can access
predictive analytics, Bots and Blockchain-as-a-Service
<a class="reference internal" href="#www-azure-msft" id="id102">[88]</a> as well as more-basic computing,
networking, storage, database and management components
<a class="reference internal" href="#www-sec-edgar-msft" id="id103">[89]</a>.  The Azure website shows twelve major
categories under <em>Products</em> and twenty <em>Solution</em> categories,
e.g., e-commerce or Business SaaS apps.</p>
<p>Azure competes against Amazon&#8217;s <em>Amazon Web Service</em>,
<a class="reference internal" href="#www-aws-amzn" id="id104">[90]</a> even though IBM (<em>SoftLayer</em>
<a class="reference internal" href="#www-softlayer-ibm" id="id105">[91]</a> and <em>Bluemix</em> <a class="reference internal" href="#www-bluemix-ibm" id="id106">[92]</a>)
and Google (<em>Google Cloud Platform</em>) <a class="reference internal" href="#www-cloud-google" id="id107">[93]</a>
offer IaaS to the market.  As of January 2017, Azure&#8217;s datacenters
span 32 Microsoft-defined <em>regions</em>, or 38 <em>declared regions</em>,
throughout the world. <a class="reference internal" href="#www-azure-msft" id="id108">[88]</a></p>
</li>
<li><p class="first">Cloud Foundry</p>
</li>
<li><p class="first">Pivotal</p>
</li>
<li><p class="first">IBM BlueMix</p>
</li>
<li><p class="first">(Ninefold)</p>
<p>The Australian based cloud computing platform has shut down their
services since January 30, 2016. Refer <a class="reference internal" href="#www-ninefoldsite" id="id109">[94]</a></p>
</li>
<li><p class="first">Jelastic</p>
<p>Jelastic (acronym for Java Elastic) is an unlimited PaaS and Container based
IaaS within a single platform that provides high availability of
applications, automatic vertical and horizontal scaling via containerization
to software development clients, enterprise businesses, DevOps, System
Admins, Developers, OEMs and web hosting providers. <a class="reference internal" href="#www-jelastic-2" id="id110">[95]</a>
Jelastic is a Platform-as-Infrastructure provider of Java and PHP hosting.
It has international hosting partners and data centers. The company can add
memory, CPU and disk space to meet customer needs. The main competitors of
Jelastic are Google App Engine, Amazon Elastic Beanstalk, Heroku, and Cloud
Foundry.Jelastic is unique in that it does not have limitations or code
change requirements, and it offers automated vertical scaling, application
lifecycle management, and availability from multiple hosting providers
around the world. <a class="reference internal" href="#www-jelastic-1" id="id111">[96]</a></p>
</li>
<li><p class="first">Stackato</p>
</li>
<li><p class="first">appfog</p>
<p>According to <a class="reference internal" href="#wee" id="id112">[97]</a>, AppFog is a platform as a service (PaaS)
provider. Platform as a service provides a platform for the
development of web applications without the necessity of
purchasing the software and infrastructure that supports
it. <a class="reference internal" href="#kepes" id="id113">[98]</a> PaaS provides an environment for the creation of
software. <a class="reference internal" href="#kepes" id="id114">[98]</a> The underlying support infrastructure that AppFog
provides includes things such as runtime, middleware, o/s,
virtualization, servers, storage, and networking. <a class="reference internal" href="#appfog" id="id115">[99]</a> AppFog
is based on VMWares CloudFoundry project. <a class="reference internal" href="#wee" id="id116">[97]</a> It gets things
such as MySQL, Mongo, Reddis, memCache, etc. running and then
manages them. <a class="reference internal" href="#tweney" id="id117">[100]</a></p>
</li>
<li><p class="first">CloudBees</p>
</li>
<li><p class="first">Engine Yard</p>
</li>
<li><p class="first">(CloudControl)</p>
<p>No Longer active as of Feb. 2016 <a class="reference internal" href="#www-wiki" id="id118">[101]</a></p>
</li>
<li><p class="first">dotCloud</p>
<p>dotCloud services were shutdown on February 29,2016
<a class="reference internal" href="#www-dotcloud" id="id119">[102]</a></p>
</li>
<li><p class="first">Dokku</p>
</li>
<li><p class="first">OSGi</p>
</li>
<li><p class="first">HUBzero</p>
</li>
<li><p class="first">OODT</p>
</li>
<li><p class="first">Agave</p>
<p>Agave is an open source, application hosting framework and
provides a platform-as-a-service solution for hybrid
computing. <a class="reference internal" href="#agave-paper" id="id120">[103]</a> It provides everything ranging
from authentication and authorization to computational, data and
collaborative services. Agave manages end to end lifecycle of an
applications execution.  Agave provides an execution platform,
data management platform, or an application platform through
which users can execute applications, perform operations on their
data or simple build their web and mobile
applications. <a class="reference internal" href="#www-agaveapi-features" id="id121">[104]</a></p>
<p>Agaves APIs provide a catalog with existing technologies and
hence no additional appliances, servers or other software needs
to be installed. To deploy an application from the catalog, the
user needs to host it on a storage system registered with Agave,
and submit to agave, a JSON file that shall contain the path to
the executable file, the input parameters, and specify the
desired output location. <a class="reference internal" href="#agave-paper" id="id122">[103]</a> Agave shall read the
JSON file, formalize the parameters, execute the user program and
dump the output to the requested destination.</p>
</li>
<li><p class="first">Atmosphere</p>
<p>Atmosphere is developed by CyVerse (previously named as iPlant
Collaborative).
It is a cloud-computing platform. It allows one to launch his own
isolated virtual machine (VM) image <a class="reference internal" href="#www-at1" id="id123">[105]</a>.
It does not require any machine specification. It can be run on any device
(tablet/desktop/laptop) and any machine(Linux/Windows/Max/Unix).
User should have a CyVerse account and be granted permission to access to
Atmosphere before he can begin using Atmosphere. No subscription is needed.
Atmosphere is designed to execute data-intense bioinformatics tasks that
may include a)Infrastructure as a Service (IaaS) with advanced APIs;
b)Platform as a Service (PaaS), and c)Software as a Service (SaaS).
On Atmosphere one has several images of virtual machine and user can launch
any image or instance according to his requirements.
The images launched by users can be shared among different members as and
when required <a class="reference internal" href="#www-at2" id="id124">[106]</a>.</p>
</li>
</ol>
</div>
<div class="section" id="high-level-programming">
<h2>High level Programming<a class="headerlink" href="#high-level-programming" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="104">
<li><p class="first">Kite</p>
</li>
<li><p class="first">Hive</p>
</li>
<li><p class="first">HCatalog</p>
</li>
<li><p class="first">Tajo</p>
<p>Apache Tajo <a class="reference internal" href="#www-apache-tajo" id="id125">[107]</a> is a big data relational and
distributed data warehouse system for Apache&#8217;s Hadoop
framework. It uses the Hadoop Distributed File System (HDFS) as a
storage layer and has its own query execution engine instead of
the MapReduce framework. Tajo is designed to provide low-latency
and scalable ad-hoc queries, online aggregation, and ETL
(extraction-transformation-loading process) on large-data sets
which are stored on HDFS (Hadoop Distributed File System) and on
other data sources. <a class="reference internal" href="#www-tutorialspoint-tajo" id="id126">[108]</a> Apart from HDFS,
it also supports other storage formats as Amazon S3, Apache
HBase, Elasticsearch etc. It provides distributed SQL query
processing engine and even has query optimization techniques and
provides interactive anaysis on large-data sets. Tajo is
compatible with ANSI/ISO SQL standard, JDBC standard. Tajo can
also store data from various file formats such as CSV,
JSON,RCFile, SequenceFile, ORC and Parquet. It provides a SQL
shell which allows users to submit the SQL queries. It also
offers user defined functions to work with it which can be
created in python. A Tajo cluster has one master node and a
number of worker nodes. <a class="reference internal" href="#www-tutorialspoint-tajo" id="id127">[108]</a> The master
node is responsible for performing the query planning and
maintaining a coordination among the worker nodes. It does this
by dividing a query in small task which are assigned to the
workers who have a local query engine for executing the queries
assigned to them.</p>
</li>
<li><p class="first">Shark</p>
<p>Data Scientists when working on huge data sets try to extract
meaning and interpret the data to enhance insight about the
various patterns, oppurtunities and possiblities that the dataset
has to offer. :cite: &#8216;shark-paper-2012&#8217; At a traditional
EDW(Enterprrise Data Warehouse) a simple data manipulation can be
perfpormed using SQL queries but we have to rely on other systems
to apply the machine learning on thoese data.Apache Shark is a
distributed query engine developed by the open source community
whoese goal is to provide a a unified system for easy data
manipulation using SQL and pushing sophisticated analysis towards
the data.</p>
<p>:cite:&#8217;shark-paper-2012&#8217; Shark is a data Warehouse system built
on top of Apache Spark which does the parallel data execution and
is capable of deep data analysis using the Resilient Distributed
Datasets(RDD) memory abstraction which unifies the SQL query
processing engine with analytical algorithms based on this common
abstraction allowing the two to run in the same set of workers
and share intermediate data. Since RDDs are designed to scale
horizontally, it is easy to add or remove nodes to accommodate
more data or faster query processing thus it can be scaled to
thoushands o nodes in a fault-toleranat manner</p>
<p>:cite:&#8217;shark-paper-2012&#8217; &#8220;Shark is built on Hive Codebase and it
has the ability to execute HIVE QL queries up to 100 times faster
than Hive without making any change in the existing
queries&#8221;. Shark can run both on the StandAlone Mode and Cluster
Mode.:cite:&#8217;shark-paper-2012&#8217; Shark can answer the queries 40X
faster than Apache Hive and can machine learning programs 25X
faster than MapReduce programmes. in Apache hadoop on large data
sets.Thus, this new data analysis system performs query
processing and complex analytics(iterative Machine learning) at
scale and efficiently recovers form the failures midway</p>
</li>
<li><p class="first">Phoenix</p>
<p>In the first quarter of 2013, Salesforce.com released its
proprietary SQL-like interface and query engine for HBase,
<em>Phoenix</em>, to the open source community.  The company appears to
have been motivated to develop Phoenix as a way to 1) increase
accessiblity to HBase by using the industry-standard query
language (SQL); 2) save users time by abstracting away the
complexities of coding native HBase queries; and, 3) implementing
query best practices by implementing them automatically via
Phoenix. <a class="reference internal" href="#www-phoenix-cloudera" id="id128">[109]</a> Although Salesforce.com
initially <em>open-sourced</em> it via Github, by May of 2014 it had
become a top-level Apache project. <a class="reference internal" href="#www-phoenix-wikipedia" id="id129">[110]</a></p>
<p>Phoenix, written in Java, &#8220;compiles [SQL queries] into a series
of HBase scans, and orchestrates the running of those scans to
produce regular JDBC result sets.&#8221; <a class="reference internal" href="#www-apachephoenix-org" id="id130">[111]</a>
In addition, the program directs compute intense portions of the
calls to the server.  For instance, if a user queried for the top
ten records across numerous regions from an HBase database
consisting of a billion records, the program would first select
the top ten records for each region using server-side compute
resources.  After that, the client would be tasked with selecting
the overall top ten. <a class="reference internal" href="#www-phoenix-salesforcedev" id="id131">[112]</a></p>
<p>Despite adding an abstraction layer, Phoenix can actually speed
up queries because it optimizes the query during the translation
process. <a class="reference internal" href="#www-phoenix-cloudera" id="id132">[109]</a> For example, &#8220;Phoenix
beats Hive for a simple query spanning 10M-100M rows.&#8221;
<a class="reference internal" href="#www-phoenix-infoq" id="id133">[113]</a></p>
<p>Finally, another program can enhance HBase&#8217;s accessibility for
those inclined towards graphical interfaces.  SQuirell only
requires the user to set up the JDBC driver and specify the
appropriate connection string. <a class="reference internal" href="#www-phoenix-bighadoop" id="id134">[114]</a></p>
</li>
<li><p class="first">Impala</p>
</li>
<li><p class="first">MRQL</p>
<p>MapReduce Query Language (MRQL, pronounced miracle) &#8220;is a query
processing and optimization system for large-scale, distributed
data analysis&#8221;. <a class="reference internal" href="#www-apachemrql" id="id135">[115]</a> MRQL provides a SQL
like language for use on Apache Hadoop, Hama, Spark, and Flink.
MRQL allows users to perform complex data analysis using only SQL
like queries, which are translated by MRQL to efficient Java
code. <a class="reference internal" href="#www-apachemrql" id="id136">[115]</a></p>
<p>MRQL was created in 2011 by Leaonids
Fegaras <a class="reference internal" href="#www-mrqlhadoop" id="id137">[116]</a> and is currently in the Apache
Incubator.  All projects accepted by the Apache Software
Foundation (ASF) undergo an incubation period until a review
indicates that the project meets the standards of other ASF
projects. <a class="reference internal" href="#www-apacheincubator" id="id138">[117]</a></p>
</li>
<li><p class="first">SAP HANA</p>
<p>As noted in <a class="reference internal" href="#www-sap-hana" id="id139">[118]</a>, SAP HANA is in-memory massively
distributed platform that consists of three components:
analytics, relational ACID compliant database and
application. Predictive analytics and machine learning
capabilities are dynamically allocated for searching and
processing of spatial, graphical, and text data.
SAP HANA accommodates flexible development and deployment of
data on premises, cloud and hybrid configurations.  In a
nutshell, SAP HANA acts as a warehouse that integrates live
transactional data from various data sources on a single
platform <a class="reference internal" href="#olofson-2014" id="id140">[119]</a>. It provides extensive
administrative, security features and data access that ensures
high data availability, data protection and data quality.</p>
</li>
<li><p class="first">HadoopDB</p>
</li>
<li><p class="first">PolyBase</p>
</li>
<li><p class="first">Pivotal HD/Hawq</p>
</li>
<li><p class="first">Presto</p>
<p>Presto <a class="reference internal" href="#www-presto" id="id141">[120]</a> is an open-source distributed SQL query
engine that supports interactive analytics on large datasets. It
allows interfacing with a variety of data sources such as Hive,
Cassandra, RDBMSs and proprietary data source. Presto is used at a
number of big-data companies such as Facebook, Airbnb and
Dropbox. Presto&#8217;s performance compares favorably to similar systems
such as Hive and Stinger <a class="reference internal" href="#presto-paper-2014" id="id142">[121]</a>.</p>
</li>
<li><p class="first">Google Dremel</p>
</li>
<li><p class="first">Google BigQuery</p>
</li>
<li><p class="first">Amazon Redshift</p>
</li>
<li><p class="first">Drill</p>
<p>Apache Drill <a class="reference internal" href="#www-apachedrill" id="id143">[122]</a> is an open source framework
that provides schema free SQL query engine for distributed
large-scale datasets. Drill has an extensible architecture at
its different layers. It does not require any centralized
metadata and does not have any requirement for schema
specification. Drill is highly useful for short and interactive
ad-hoc queries on very large scale data sets. It is scalable to
several thousands of nodes. Drill is also capable to query
nested data in various formats like JSON and Parquet. It can
query large amount of data at very high speed. It is also
capable of performing discovery of dynamic schema.
A service called Drillbit  is at the core of Apache Drill
responsible for accepting requests from the client, processing
the required queries, and returning all the results to the client.
Drill is primarily focused on non-relational datastores,
including Hadoop and NoSQL</p>
</li>
<li><p class="first">Kyoto Cabinet</p>
<p>Kyoto Cabinet as specified in <a class="reference internal" href="#www-kyotocabinet" id="id144">[123]</a> is a
library of routines for managing a database which is a simple
data file containing records. Each record in the database is a
pair of a key and a value. Every key and value is serial bytes
with variable length. Both binary data and character string can
be used as a key and a value. Each key must be unique within a
database.  There is neither concept of data tables nor data
types. Records are organized in hash table or B+ tree. Kyoto
Cabinet runs very fast. The elapsed time to store one million
records is 0.9 seconds for hash database, and 1.1 seconds for B+
tree database. Moreover, the size of database is very small. The,
overhead for a record is 16 bytes for hash database, and 4 bytes
for B+ tree database. Furthermore, scalability of Kyoto Cabinet
is great. The database size can be up to 8EB (9.22e18 bytes).</p>
</li>
<li><p class="first">Pig</p>
</li>
<li><p class="first">Sawzall</p>
<p>Google engineers created the domain-specific programming language
(DSL) <em>Sawzall</em> as a productivity enhancement tool for Google
employees.  They targeted the analysis of large data sets with
flat, but regular, structures spread across numerous servers.
The authors designed it to handle &#8220;simple, easily distributed
computations: filtering, aggregation, extraction of statistics,&#8221;
etc. from the aforementioned data sets.
<a class="reference internal" href="#google-sawzall" id="id145">[124]</a></p>
<p>In general terms, a Sawzall job works as follows: multiple
computers each create a Sawzall instance, perform some operation
on a single record out of (potentially) petabytes of data, return
the result to an aggregator function on a different computer and
then shut down the Sawzall instance.</p>
<p>The engineer&#8217;s focus on simplicity and parallelization led to
unconventional design choices.  For instance, in contrast to most
programming languages Sawzall operates on one data record at a
time; it does not even preserve state between records.
<a class="reference internal" href="#www-bytemining-sawzall" id="id146">[125]</a> Addtionally, the language provides
just a single primitive result function, the <em>emit</em> statement.
The emitter returns a value from the Sawzall program to a
designated virtual receptacle, generally some type of aggregator.
In another example of pursuing language simplicity and
parallelization, the aggregators remain separate from the formal
Sawzall language (they are written in C++) because &#8220;some of the
aggregation algorithms are sophisticated and best implemented in
a native language [and] [m]ore important[ly] drawing an explicit
line between filtering and aggregation enables a high degree of
parallelism, even though it hides the parallelism from the
language itself&#8221;.  <a class="reference internal" href="#google-sawzall" id="id147">[124]</a></p>
<p>Important components of the Sawzall language include: <em>szl</em>, the
binary containing the code compiler and byte-code interpreter
that executes the program; the <em>libszl</em> library, which compiles
and executes Sawzall programs &#8220;[w]hen szl is used as part of
another program, e.g. in a [map-reduce] program&#8221;; the Sawzall
language plugin, designated <em>protoc_gen_szl</em>, which generates
Sawzall code when run in conjunction with Google&#8217;s own <em>protoc</em>
protocol compiler; and libraries for intrinsic functions as well
as Sawzall&#8217;s associated aggregation functionality.
<a class="reference internal" href="#www-google-code-wiki-sawzall" id="id148">[126]</a></p>
</li>
<li><p class="first">Google Cloud DataFlow</p>
<p>Google Cloud DataFlow <a class="reference internal" href="#data-flow1" id="id149">[127]</a> is a unified programming
model that manages the deployment, maintenance and optimization
of data processes such as batch processing, ETL etc. It creates a
pipeline of tasks and dynamically allocates resources thereby
maintaining high efficiency and low latency. According to
<a class="reference internal" href="#data-flow1" id="id150">[127]</a>, these capabilities make it suitable for
solving challenging big data problems. Also, google DataFlow
overcomes the performance issues faced by Hadoops Mapreduce while
building pipelines. As stated in <a class="reference internal" href="#dataconomy" id="id151">[128]</a> the
performance of MapReduce started deteriorating while facing
multiplepetabytes of data whereas Google Cloud Dataflow is
apparently better at handling enormous
datasets. <a class="reference internal" href="#data-flow1" id="id152">[127]</a> Additionally Google Dataflow can be
integrated with Cloud Storage, Cloud Pub/Sub, Cloud Datastore,
Cloud Bigtable, and BigQuery. The unified programming ability is
another noteworthy feature which uses Apache Beam SDKs to support
powerful operations like windowing and allows correctness control
to be applied to batch and stream  data processes.</p>
</li>
<li><p class="first">Summingbird</p>
</li>
<li><p class="first">Lumberyard</p>
</li>
</ol>
</div>
<div class="section" id="streams">
<h2>Streams<a class="headerlink" href="#streams" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="127">
<li><p class="first">Storm</p>
<p>Apache Storm is an open source distributed computing framework for
analyzing big data in real time. <a class="reference internal" href="#storm-paper-ijctt" id="id153">[129]</a> refers
storm as the Hadoop of real time data. Storm operates by reading real
time input data from one end and passes it through a sequence of
processing units delivering output at the other end. The basic element
of Storm is called topology. A topology consists of many other
elements interconnected in a sequential fashion. Storm allows us to
define and submit topologies written in any programming language.</p>
<p>Once under execution, a storm topology runs indefinitely unless killed
explicitly. The key elements in a topology are the spout and the
bolt. A spout is a source of input which can read data from various
datasources and passes it to a bolt. A bolt is the actual processing
unit that processes data and produces a new output stream. An output
stream from a bolt can be given as an input to another
bolt. <a class="reference internal" href="#www-storm-home-concepts" id="id154">[130]</a></p>
</li>
<li><p class="first">S4</p>
</li>
<li><p class="first">Samza</p>
<p>Apache Samza is an open-source near-realtime, asynchronous computational
framework for stream processing developed by the Apache Software
Foundation in Scala and Java. <a class="reference internal" href="#www-samza-3" id="id155">[131]</a>
Apache Samza is a distributed stream processing framework. It uses Apache
Kafka for messaging, and Apache Hadoop YARN to provide fault tolerance,
processor isolation, security, and resource management. Samza processes
streams. A stream is composed of immutable messages of a similar type or
category. Messages can be appended to a stream or read from a stream.
Samza supports pluggable systems that implement the stream abstraction:
in Kafka a stream is a topic, in a database we might read a stream by
consuming updates from a table, in Hadoop we might tail a directory of
files in HDFS. Samza is a stream processing framework. Samza provides a
very simple callback-based process message API comparable to MapReduce.
Samza manages snapshotting and restoration of a stream processors state.
Samza is built to handle large amounts of state (many gigabytes per
partition). <a class="reference internal" href="#www-samza-1" id="id156">[132]</a> Whenever a machine in the cluster fails,
Samza works with YARN to transparently migrate your tasks to another
machine. Samza uses Kafka to guarantee that messages are processed in the
order they were written to a partition, and that no messages are ever lost.
Samza is partitioned and distributed at every level. Kafka provides
ordered, partitioned, replayable, fault-tolerant streams. YARN provides a
distributed environment for Samza containers to run in. Samza works with
Apache YARN, which supports Hadoops security model, and resource isolation
through Linux CGroups <a class="reference internal" href="#www-samza-4" id="id157">[133]</a> <a class="reference internal" href="#www-samza-3" id="id158">[131]</a>.</p>
</li>
<li><p class="first">Granules</p>
<p>Granules in used for execution or processing of data streams in
distributed environment.
When applications are running concurrently on multiple computational
resources, granules manage their parallel execution.
The MapReduce implementation in Granules is responsible for providing
better performance.It has the capability of expressing computations like
graphs.
Computations can be scheduled based on periodicity or other activity.
Computations can be developed in C, C++, Java, Python, C#, R
It also provides support for extending basic Map reduce framework.
Its application domains include hand writing recognition, bio informatics
and computer brain interface <a class="reference internal" href="#www-granules" id="id159">[134]</a>.</p>
</li>
<li><p class="first">Neptune</p>
</li>
<li><p class="first">Google MillWheel</p>
</li>
<li><p class="first">Amazon Kinesis</p>
<p>Kinesis is Amazons <a class="reference internal" href="#www-kinesis" id="id160">[135]</a> real time data processing
engine. It is designed to provide scalable, durable and reliable
data processing platform with low latency. The data to Kinesis
can be ingested from multiple sources in different format. This
data is further made available by Kinesis to multiple
applications or consumers interested in the data. Kinesis
provides robust and fault tolerant system to handle this high
volume of data. Data sharding mechanism is Kinesis makes it
horizontally scalable. Each of these shards in Kinesis process a
group of records which are partitioned by the shard key. Each
record processed by Kinesis is identified by sequence number,
partition key and data blob. Sequence number to records is
assigned by the stream. Partition keys are used by partitioner(a
hash function) to map the records to the shards i.e. which
records should go to which shard. Producers like web servers,
client applications, logs push the data to Kinesis whereas
Kinesis applications act as consumers of the data from Kinesis
engine. It also provides data retention for certain time for
example 24 hours default. This data retention window is a sliding
window. Kinesis collects lot of metrics which can used to
understand the amount of data being processed by Kinesis.  User
can use this metrics to do some analytics and visualize the
metrics data.  Kinesis is one of the tools part of AWS
infrastructure and provides its users a complete
software-as-a-service. Kinesis <a class="reference internal" href="#big-data-analytics-book" id="id161">[136]</a> in
the area of real-time processing provides following key benefits:
ease of use, parellel processing, scalable, cost effective, fault
tolerant and highly available.</p>
</li>
<li><p class="first">LinkedIn</p>
</li>
<li><p class="first">Twitter Heron</p>
<p>Heron is a real-time analytics platform that was developed at
Twitter for distributed streaming processing. Heron was
introduced at SIGMOD 2015 to overcome the shortcomings of Twitter
Storm as the scale and diversity of Twitter data increased. As
mentioned in <a class="reference internal" href="#twitterheronopen" id="id162">[137]</a> The primary advantages of
Heron were: API compatible with Storm: Back compatibility with
Twitter Storm reduced migration time. Task-Isolation: Every task
runs in process-level isolation, making it easy to debug/
profile. Use of main stream languages: C++, Java, Python for
efficiency, maintainability, and easier community
adoption. Support for backpressure: dynamically adjusts the rate
of data flow in a topology during run-time, to ensure data
accuracy. Batching of tuples: Amortizing the cost of transferring
tuples. Efficiency: Reduce resource consumption by 2-5x and Heron
latency is 5-15x lower than Storms latency. The architecture of
Heron (as shown in <a class="reference internal" href="#twitterheron" id="id163">[138]</a>)uses the Storm API to
submit topologies to a scheduler. The scheduler runs each
topology as a job consisting of several containers. The
containers run the topology master, stream manager, metrics
manager and Heron instances. These containers are managed by the
scheduler depending on resource availability.</p>
</li>
<li><p class="first">Databus</p>
</li>
<li><p class="first">Facebook Puma/Ptail/Scribe/ODS</p>
<p>The real time data Processing at Fcabook is carried out using the
technologies like Scibe,PTail, Puma and ODS. While designing the
system, facebook primarily focused on the five key decissions
that the system should incorporate and that included Ease of Use,
Performance , Fault-tolerance , Scalability and
Correctness.:cite: &#8216;www-facebook&#8217; &#8220;The real time data analytics
ecosystem at facebook is designed to handle hundreds of Gigabytes
of data per second via hundreds of data pipelines and this system
handles over 200,000 events per second with a maximum latency of
30 seconds&#8221;. :cite:&#8217;www-facebook&#8217;Fcabook focused on the Seconds
of latency while designing the system and not milliseconds as
seconds are fast enough to for all the use case that needs to be
supported, and it allowed facebook to use persistent message bus
for data transport and this made the system more fault toleranat
and scalable. :cite:&#8217;facebook-paper-2017&#8217; The large
infrastructure of facebook comprises of hundreds of systems
distributed across multiple data centers that needs a continious
monitoring to track their health and performance.Which is done by
Operational Data Store(ODS).ODS comprises of a time series
database (TSDB),which is a query service, and a detection and
alerting system. ODSs TSDB is built atop the HBase storage
system.Time series data from services running on Facebook hosts
is collected by the ODS write service and written to HBase.</p>
<p>When the data is generated by the user from their devices, an
AJAX request is fired to facebook,and these requests are then
written to a log file using Scribe(distributed data transport
system), this messaging system collect, aggregate and delivers
high volume of log data with few seconds of latency and high
throughput.Scribe stores the data in the HDFS(Hadoop Distributed
File System) in a tailing fashion, where the new events are
stored in log files and the files are tailed below the current
events.The events are then written into the storage HBase on
distributed machines. This makes the data avalible for both batch
and real-time processing. Ptail is an internal tool built to
aggregate data from multiple Scribe stores and It then tails the
log files and pulls data out for processing. Puma is a stream
processing system which is the real-time aggregation/storage of
data. Puma provides filtering and processing of Scribe streams
(with a few seconds delay), usually Puma batches the storage per
1.5 seconds on average and when the last flush completes, then
only a new batch starts to avoid the contention issues, which
makes i fairly real time</p>
</li>
<li><p class="first">Azure Stream Analytics</p>
<p>Azure Stream Analytics is a platform that manages data streaming
from devices, web sites, infrastructure systems, social media,
internet of things analytics, and other sources usings real-time
event processing engine. <a class="reference internal" href="#www-azurestreamanalytics" id="id164">[139]</a> Jobs
are authored by &#8220;specifying the input source of the streaming
data, the output sink for the results of your job, and a data
tranformation expressed in a SQL-like language.&#8221;  Some key
capabilities and benefits include ease of use, scalability,
reliability, repeatability, quick recovery, low cost, reference
data use, user defined functions capability, and
connectivity. <a class="reference internal" href="#www-docs-microsoft" id="id165">[140]</a> Available documentation
to get started with Azure Stream
Analytics. <a class="reference internal" href="#www-github-azure" id="id166">[141]</a> Azure Stream Analytics has a
development project available on github.</p>
</li>
<li><p class="first">Floe</p>
</li>
<li><p class="first">Spark Streaming</p>
</li>
<li><p class="first">Flink Streaming</p>
</li>
<li><p class="first">DataTurbine</p>
</li>
</ol>
</div>
<div class="section" id="basic-programming-model-and-runtime-spmd-mapreduce">
<h2>Basic Programming model and runtime, SPMD, MapReduce<a class="headerlink" href="#basic-programming-model-and-runtime-spmd-mapreduce" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="143">
<li><p class="first">Hadoop</p>
</li>
<li><p class="first">Spark <a class="reference internal" href="#www-spark" id="id167">[142]</a></p>
<p>Apache Spark which is an open source cluster computing framework
has emerged as the next generation big data processing engine
surpassing Hadoop MapReduce. &#8220;Spark engine is developed for
in-memory processing as well a disk based processing. This system
also provides large number of impressive high level tools such as
machine learning tool M Lib, structured data processing, Spark
SQL, graph processing took Graph X, stream processing engine
called Spark Streaming, and Shark for fast interactive question
device.&#8221; The ability of spark to join datasets across various
heterogeneous data sources is one of its prized
attributes. Apache Spark is not the most suitable data analysis
engine when it comes to processing (1) data streams where latency
is the most crucial aspect and (2) when the available memory for
processing is restricted. &#8220;When available memory is very limited,
Apache Hadoop Map Reduce may help better, considering huge
performance gap.&#8221; In cases where latency is the most crucial
aspect we can get better results using Apache Storm.</p>
</li>
<li><p class="first">Twister</p>
</li>
<li><p class="first">MR-MPI</p>
<p><a class="reference internal" href="#www-mapreducempi" id="id168">[143]</a> MR-MPI stands for Map Reduce-Message
Passing Interface is open source library build on top of standard
MPI. It basically implements mapReduce operation providing a
interface for user to simplify writing mapReduce program.  It is
written in C++ and needs to be linked to MPI library in order to
make the basic map reduce functionality to be executed in
parallel on distributed memory architecture.  It provides
interface for c, c++ and python. Using C interface the library
can also be called from Fortrain.</p>
</li>
<li><p class="first">Stratosphere (Apache Flink)</p>
</li>
<li><p class="first">Reef</p>
<p>REEF (Retainable Evaluator Execution Framework) <a class="reference internal" href="#www-reef" id="id169">[144]</a>
is a scale-out computing fabric that eases the development of Big
Data applications on top of resource managers such as Apache YARN
and Mesos. It is a Big Data system that makes it easy to
implement scalable, fault-tolerant runtime environments for a
range of data processing models on top of resource managers. REEF
provides capabilities to run multiple heterogeneous frameworks
and workflows of those efficiently. REEF contains two libraries,
Wake and Tang where Wake is an event-based-programming framework
inspired by Rx and SEDA and Tang is a dependency injection
framework inspired by Google Guice, but designed specifically for
configuring distributed systems.</p>
</li>
<li><p class="first">Disco</p>
</li>
<li><p class="first">Hama</p>
<p>Apache Hama is a framework for Big Data analytics which uses the
Bulk Synchronous Parallel (BSP) computing model, which was
established in 2012 as a Top-Level Project of The Apache Software
Foundation.It provides not only pure BSP programming model but
also vertex and neuron centric programming models, inspired by
Google&#8217;s Pregel and DistBelief <a class="reference internal" href="#apache-hama" id="id170">[145]</a>. It avoids the
processing overhead of MapReduce approach such as sorting,
shuffling, reducing the vertices etc. Hama provides a message
passing interface and each superstep in BSP is faster than a full
job execution in MApReduce framework, such as Hadoop
<a class="reference internal" href="#book-hama" id="id171">[146]</a>.</p>
</li>
<li><p class="first">Giraph</p>
</li>
<li><p class="first">Pregel</p>
</li>
<li><p class="first">Pegasus</p>
</li>
<li><p class="first">Ligra</p>
<p>Ligra is a Light Weight Graph Processing Framework for the graph
manipulation and analysis in shared memory system. It is
particularly suited for implementing on parallel graph traversal
algorithms where only a subset of the vertices are processed in an
iteration The interface is lightweight in that it supplies only a
few functions. The Ligra framework has two very simple routines,
one for mapping over edges and one for mapping over vertices.</p>
<p>:cite:&#8217;ligra-paper-2013 &#8216;The implementations of several graph
algorithms like BFS, breadth-first search, betweenness centrality,
graph radii estimation, graph-connectivity, PageRank and
Bellman-Ford single-source shortest paths efficient and scalable,
and often achieve better running times than ones reported by other
graph libraries/systems</p>
<p>:cite:&#8217;ligra-paper-2&#8217; Although the shared memory machines cannot
be scaled to the same size as distributed memory clusters but the
current commodity single unit servers can easily fit graphs with
well over a hundred billion edges in the shared memory systems
that is large enough for any of the graphs reported in the papers
mentioned above.</p>
</li>
<li><p class="first">GraphChi</p>
</li>
<li><p class="first">Galois</p>
<p>Galois system was built by intelligent software systems team at
University of Texas, Austin. As explained in
<a class="reference internal" href="#www-galoissite" id="id172">[147]</a>, Galois is a system that automatically
executes &#8216;Galoized&#8217; serial C++ or Java code&nbsp;in parallel&nbsp;on
shared-memory machines. It works by exploiting amorphous
data-parallelism, which is present even in irregular codes that
are organized around pointer-based data structures such as graphs
and trees. By using Galois provided data structures programmers
can write serial programs that gives the performance of parallel
execution. Galois employs annotations at loop levels to
understand correct context during concurrent execution and
executes the code that could be run in parallel. The key idea
behind Galois is Tao-analysis, in which parallelism is exploited
at compile time rather than at run time by creating operators
equivalent of the code by employing data driven local computation
algorithm <a class="reference internal" href="#taoparallelismpaper" id="id173">[148]</a>. Galois currently supports
C++ and Java.</p>
</li>
<li><p class="first">Medusa-GPU</p>
</li>
<li><p class="first">MapGraph</p>
</li>
<li><p class="first">Totem</p>
</li>
</ol>
</div>
<div class="section" id="inter-process-communication-collectives">
<h2>Inter process communication Collectives<a class="headerlink" href="#inter-process-communication-collectives" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="160">
<li><p class="first">point-to-point</p>
</li>
<li><p class="first">publish-subscribe: MPI</p>
</li>
<li><p class="first">HPX-5</p>
<p>Based on <a class="reference internal" href="#www-hpx-5" id="id174">[149]</a>, High Performance ParallelX (HPX-5)
is an open source, distributed model that provides opportunity
for operations to run unmodified on one-to-many nodes. The
dynamic nature of the model accommodates effective computing
resource management and task scheduling. It is portable and
performance-oriented. HPX-5 was developed by IU Center for
Research in Extreme Scale Technologies (CREST). Concurrency is
provided by lightweight control object (LCO) synchronization and
asynchronous remote procedure calls. ParallelX component allows
for termination detection and supplies per-process
collectives. It addresses the challenges of starvation, latency,
overhead, waiting, energy and reliability. Finally, it supports
OpenCL to use distributed GPU and coprocessors. HPX-5 could be
compiled on various OS platforms , however it was only tested on
several Linux and Darwin (10.11) platforms. Required
configurations and environments could be accessed via
<a class="reference internal" href="#www-hpx-5-user-guide" id="id175">[150]</a>.</p>
</li>
<li><p class="first">Argo BEAST HPX-5 BEAST PULSAR</p>
<p>Search on the internet was not successsful.</p>
</li>
<li><p class="first">Harp</p>
<p>Harp <a class="reference internal" href="#www-harp" id="id176">[151]</a> is a simple, easy to maintain, low risk and
easy to scale static web server that also serves Jade, Markdown,
EJS, Less, Stylus, Sass, and CoffeeScript as HTML, CSS, and
JavaScript without any configuration and requires low cognitive
overhead. It supports the beloved layout/partial paradigm and it
has flexible metadata and global objects for traversing the file
system and injecting custom data into templates. It acts like a
lightweight web server that was powerful enough for me to abandon
web frameworks for dead simple front-end publishing. Harp can
also compile your project down to static assets for hosting
behind any valid HTTP server.</p>
</li>
<li><p class="first">Netty</p>
<p>Netty <a class="reference internal" href="#www-netty" id="id177">[152]</a> &#8220;is an asynchronous event-driven network
application framework for rapid development of maintainable high
performance protocol servers &amp; clients&#8221;. Netty <a class="reference internal" href="#netty-book" id="id178">[153]</a>
&#8220;is more than a collection of interfaces and classes; it also
defines an architectural model and a rich set of design
patterns&#8221;. It is protocol agnostic, supports both connection
oriented protocols using TCP and connection less protocols built
using UDP. Netty offers performance superior to standard Java NIO
API thanks to optimized resource management, pooling and reuse
and low memory copying.</p>
</li>
<li><p class="first">ZeroMQ</p>
<p>In <a class="reference internal" href="#www-zeromq" id="id179">[154]</a>, ZeroMQ is introduced as a software product
that can &#8220;connect your code in any language, on any platform&#8221; by
leveraging &#8220;smart patterns like pub-sub, push-pull, and
router-dealer&#8221; to carry &#8220;messages across inproc, IPC, TCP, TIPC,
[and] multicast.&#8221; In <a class="reference internal" href="#www-zeromq2" id="id180">[155]</a>, it is explained that
ZeroMQ&#8217;s &#8220;asynchronous I/O model&#8221; causes this &#8220;tiny library&#8221; to
be &#8220;fast enough to be the fabric for clustered products.&#8221; In
<a class="reference internal" href="#www-zeromq" id="id181">[154]</a>, it is made clear that ZeroMQ is &#8220;backed by a
large and open source community&#8221; with &#8220;full commercial support.&#8221;
In contrast to Message Passing Interface (i.e. MPI), which is
popular among parallel scientific applications, ZeroMQ is
designed as a fault tolerant method to communicate across highly
distributed systems.</p>
</li>
<li><p class="first">ActiveMQ</p>
</li>
<li><p class="first">RabbitMQ</p>
<p>RabbitMQ is a message broker <a class="reference internal" href="#www-rabbitmq" id="id182">[156]</a> which allows
services to exchange messages in a fault tolerant manner. It
provides variety of features which enables software applications
to connect and scale. Features are: reliability, flexible
routing, clustering, federation, highly available queues,
multi-protocol, many clients, management UI, tracing, plugin
system, commercial support, large community and user
base. RabbitMQ can work in multiple scenarios:</p>
<ol class="arabic">
<li><p class="first">Simple messaging: producers write messages to the queue and
consumers read messages from the the queue. This is synonymous
to a simple message queue.</p>
</li>
<li><p class="first">Producer-consumer: Producers produce messages and consumers
receive messages from the queue. The messages are delivered to
multiple consumers in round robin manner.</p>
</li>
<li><p class="first">Publish-subscribe: Producers publish messages to exchanges
and consumers subscribe to these exchanges. Consumers receive
those messages when the messages are available in those
exchanges.</p>
</li>
<li><p class="first">Routing: In this mode consumers can subscribe to a subset
of messages instead of receiving all messages from the queue.</p>
</li>
<li><p class="first">Topics: Producers can produce messages to a topic multiple
consumers registered to receive messages from those topics get
those messages. These topics can be handled by a single
exchange or multiple exchanges.</p>
</li>
<li><p class="first">RPC:In this mode the client sends messages as well as
registers a callback message queue. The consumers consume the
message and post the response message to the callback queue.</p>
<p>RabbitMQ is based on AMPQ <a class="reference internal" href="#ampq-article" id="id183">[157]</a> (Advanced
Message Queuing Protocol) messaging model. AMPQ is described
as follows messages are published to exchanges, which are
often compared to post offices or mailboxes. Exchanges then
distribute message copies to queues using rules called
bindings. Then AMQP brokers either deliver messages to
consumers subscribed to queues, or consumers fetch/pull
messages from queues on demand</p>
</li>
</ol>
</li>
<li><p class="first">NaradaBrokering</p>
</li>
<li><p class="first">QPid</p>
</li>
<li><p class="first">Kafka</p>
<p>Apache Kafka is a streaming platform, which works based on
publish-subscribe messaging system and supports distributed environment.</p>
<p><em>Kafka lets you publish and subscribe to the messages.</em> Kafka maintains
message feeds based on topic. A topic is a category or feed name to
which records are published. Kafkas Connector APIs are used to publish
the messages to one or more topics, whereas, Consumer APIs are used to
subscribe to the topics.</p>
<p><em>Kafka lets you process the stream of data at real time.</em> Kafkas stream
processor takes continual stream of data from input topics, processes the
data in real time and produces streams of data to output topics. Kafkas
Streams API are used for data transformation.</p>
<p><em>Kafka lets you store the stream of data in distributed clusters.</em> Kafka
acts as a storage system for incoming data stream. As Kafka is a distributed
system, data streams are partitioned and replicated across nodes.</p>
<p>Thus, a combination of messaging, storage and processing data stream makes
Kafka a streaming platform. It can be used for building data pipelines
where data is transferred between systems or applications. Kafka can also be
used by applications that transform real time incoming data. :cite:&#8217;www-kafka&#8217;</p>
</li>
<li><p class="first">Kestrel</p>
</li>
<li><p class="first">JMS</p>
<p>JMS (Java Messaging Service) is a java oriented messaging standard
that defines a set of interfaces and semantics which allows
applications to send, receive, create, and read messages.  It allows
the communication between different components of a distributed
application to be loosely coupled, reliable, and
asynchronous. <a class="reference internal" href="#www-jms-wiki" id="id184">[158]</a> JMS overcomes the drawbacks of RMI
(Remote Method Invocation) where the sender needs to know the method
signature of the remote object to invoke it and RPC(Remote Procedure
Call), which is tightly coupled i.e it cannot function unless the
sender has important information about the receiver.</p>
<p>JMS establishes a standard that provides loosely coupled communication
i.e the sender and receiver need not be present at the same time or
know anything about each other before initiating the communication.
JMS provides two communication domains.A point-to-point messaging
domain where there is one producer and one consumer. On generating
message, a producer simple pushes the message to a message queue which
is known to the consumer. The other communication domain is
publish/subscribe model, where one message can have multiple
receivers. <a class="reference internal" href="#www-jms-oracle-docs" id="id185">[159]</a></p>
</li>
<li><p class="first">AMQP</p>
<p><a class="reference internal" href="#www-amqp" id="id186">[160]</a> AMQP stands for Advanced Message Queueing
Protocol. AMQP is open interenet protocol that allows secure and
reliable communication between applications in different
orginization and different applications which are on diffferent
platforms. AMQP allows businesses to implement middleware
applications interoperability by allowing secure message transfer
bewteen the applications on timly manner. AMQP is mainly used by
financial and banking business. Other sectors that aslo use AMQP
are Defence, Telecommunication, cloud Computing and so on.
Apache Qpid, StormMQ, RabbitMQ, MQlight, Microsoft&#8217;s Windows
Azure Service Bus, IIT Software&#8217;s SwiftMQ and JORAM are some of
the products that implement AMQP protocol.</p>
</li>
<li><p class="first">Stomp</p>
</li>
<li><p class="first">MQTT</p>
<p>According to <a class="reference internal" href="#www-mqtt" id="id187">[161]</a>, Message Queueing Telemetry
Transport (MQTT) protocol is an Interprocess communication
protocol that could serve as better alternative to HTTP in
certain cases. It is based on a publish-subscribe messaging
pattern. Any sensor or remote machine can publish it&#8217;s data and
any registered client can subscribe the data. A broker takes care
of the message being published by the remote machine and updates
the subscriber in case of new message from the remote
machine. The data is sent in binary format which makes it use
less bandwidth. It is designed mainly to cater to the needs to
devices that has access to minimal network bandwidth and device
resources without affecting reliability and quality assurance of
delivery. MQTT protocol has been in use since 1999. One of the
notable work is project Floodnet <a class="reference internal" href="#www-floodnet" id="id188">[162]</a>, which
monitors river and floodplains through a set of sensors.</p>
</li>
<li><p class="first">Marionette Collective</p>
</li>
<li><p class="first">Public Cloud: Amazon SNS</p>
<p>Amazon SNS is an Inter process communication service which gives
the user simple, end-to-end push messaging service allowing them
to send messages, alerts, or notifications. According to
<a class="reference internal" href="#www-sns" id="id189">[163]</a>, it can be used to send a directed message
intended for an entity or to broadcast messages to list of
selected entities. It is an easy to use and cost effective
mechanism to send push messages. Amazon SNS is compatible to send
push notifications to iOS, Windows, Fire OS and Android OS
devices.</p>
<p>According to <a class="reference internal" href="#sns-blog" id="id190">[164]</a>,Topics are named groups of events or
access points, each identifying a specific subject, content, or event
type. Each topic has a unique identifier (URI) that identifies the SNS
endpoint for publishing and subscribing.Owners create topics and
control all access to the topic. The owner can define the permissions
for all of the topics that they own.Subscribers are clients
(applications, end-users, servers, or other devices) that want to
receive notifications on specific topics of interest to
them.Publishers send messages to topics. SNS matches the topic with
the list of subscribers interested in the topic, and delivers the
message to each and every one of them.</p>
<p>According to <a class="reference internal" href="#sns-faq" id="id191">[165]</a>, Amazon SNS follows pay as per usage. In
general it is $0.50 per 1 million Amazon SNS Requests.Amazon SNS
supports notifications over multiple transport protocols such as
HTTP/HTTPS, Email/Email-JSON, SQS(Message queue) and SMS.Amazon SNS
can be used with other AWS services such as Amazon SQS, Amazon EC2 and
Amazon S3.</p>
</li>
<li><p class="first">Lambda</p>
</li>
<li><p class="first">Google Pub Sub</p>
<p><a class="reference internal" href="#www-google-pub-sub" id="id192">[166]</a> Google Pub/Sub provides an asynchronous
messaging facility which assists the communication between independent
applications. It works in real time and helps keep the two interacting
systems independent. It is the same technology used by many of the
Google apps like GMail, Ads, etc. and so integration with them becomes
very easy. <a class="reference internal" href="#www-google-pub-sub-features" id="id193">[167]</a> Some of the typical
features it provides are: (1) Push and Pull - Google Pub/Sub integrates
quickly and easily with the systems hosted on the Google Cloud Platform
thereby supporting one-to-many, one-to-one and many-to-many
communication, using the push and pull requests. (2) Scalability - It
provides high scalability and availability even under heavy load without
any degradation of latency. This is done by using a global and highly
scalable design. (3) Encryption - It provides security by encryption of
the stored data as well as that in transit. Other than these important
features, it provides some others as well, like the usage of RESTful
APIs, end-to-end acknowledgement, replicated storage, etc.</p>
</li>
<li><p class="first">Azure Queues</p>
<p>Azure Queues storage is a Microsoft Azure service, providing inter
-process communication by message passing <a class="reference internal" href="#silberschatz1998operating" id="id194">[168]</a>.
A sender sends the message and a client receives and processes them.
The messages are stored in a queue which can contain millions of
messages, up to the total capacity limit of a storage account <a class="reference internal" href="#www-azurequeue-web" id="id195">[169]</a>.
Each message can be up to 64 KB in size. These messages can then be
accessed from anywhere in the world via authenticated calls using HTTP or
HTTPS. Similar to the other message queue services, Azure Queues enables
decoupling of the components <a class="reference internal" href="#www-tutorialspoint" id="id196">[170]</a>. It runs in an
asynchronous environment where messages can be sent among the different
components of an application. Thus, it provides an efficient solution for
managing workflows and tasks. The messages can remain in the queue up to 7
days, and afterwards, they will be deleted automatically.</p>
</li>
<li><p class="first">Event Hubs</p>
</li>
</ol>
</div>
<div class="section" id="in-memory-databases-caches">
<h2>In-memory databases/caches<a class="headerlink" href="#in-memory-databases-caches" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="183">
<li><p class="first">Gora (general object from NoSQL)</p>
<p>Gora is a in-memory data model <a class="reference internal" href="#www-gora" id="id197">[171]</a> which also
provides persistence to the big data. Gora provides persistence
to different types of data stores. Primary goals of Gora are:</p>
<ol class="arabic simple">
<li>data persistence</li>
<li>indexing</li>
<li>data access</li>
<li>analysis</li>
<li>map reduce support</li>
</ol>
<p>Unlike ORM models which mostly work with relational databases for
example hibernate gora works for most type of data stores like
documents, columnar, key value as well as relational. Gora uses
beans to maintain the data in-memory and persist it on
disk. Beans are defined using apache avro schema. Gora provides
modules for each type of data store it supports.  The mapping
between bean definition and datastore is done in a mapping file
which is specific to a data store.  Type Gora workflow will be:</p>
<ol class="arabic simple">
<li>define  the bean used as model for persistence</li>
<li>use gora compiler to compile the bean</li>
<li>create a mapping file to map bean definition to datastore</li>
<li>update gora.properties to specify the datastore to use</li>
<li>get an instance of corresponding data store using datastore factory.</li>
</ol>
<p>Gora has a query interface to query the underlying data
store. Its configuration is stored in gora.properties which
should be present in classpath. In the file you can specify
default data store used by Gora engine. Gora also has a CI/CD
library call GoraCI which is used to write integration tests.</p>
</li>
<li><p class="first">Memcached</p>
<p>Memcached is a free and open-source, high performance, distributed memory
object caching system. <a class="reference internal" href="#www-memcached" id="id198">[172]</a> Although, generic in nature,it
is intended for se in speeding up dynamic web applications by reducing
the database load.</p>
<p>It can be thought of as a short term memory for your applications.
Memcached is an in-memory key-value store for small chunks of arbitrary
data from the results of database calls, API calls and page rendering. Its
API is available in most of the popular languages. In simple terms, it
allows you to take memory from parts of your system where you have more
memory than you need and allocate it to parts of your system where you
have less memory than you need.</p>
</li>
<li><p class="first">Redis</p>
<p>Redis (Remote Dictionary Server) is an open source ,in-memory,
key-value database which is commonly referred as a data structure
server.  :cite:&#8217;redis-book-2011&#8217; &#8220;It is called a data structure
server and not simply a key-value store because Redis implements
datastructure which allows keys to contain binary safe strings
,hashes,sets and sortedsets, as well as lists&#8221; .Rediss
exceptional performance, simplicity to use and implement, and
atomic manipulation of data structures lends itself to solving
problems that are difficult or perform poorly when implemented
with traditional relational databases.  :cite:&#8217;redis-book-2016&#8217;
&#8220;Salivator Sanfilippo(Creator of open-sorce database Redis) makes
a strong case that Redis does not need to replace the existing
database but is an excellent addition to an enterprise for new
functionalities or to solve sometimes intyractable problems.&#8221;</p>
<p>:cite:&#8217;redis-book-2016&#8217; A very popular use pattern for Redis is
an in-memory cache for web-applications. The second popular use
pattern for REDIS is for metric storage of such quantitative data
such as web page usage and user behaviour on gamer leaderboards
where using a bit operations on strings, Redis very efficently
stores binary information on a particular characteristics.The
third popular Redis use pattern is a communication layer between
different systems through a publish/subscribe(pub/sub for short),
where one can post message to one or more channels that can be
acted upon by other systems that are subscribed to or listening
to that channel for incoming message. The Comapnies using REDIS
includes Twitter to store the timelines of all the user ,
Pinterest stores the user follower graph, Github, popular web
frameworks like Node.js ,Django,Ruby-on-Rails etc.</p>
</li>
<li><p class="first">LMDB (key value)</p>
<p>LMDB (Lighting memory-mapped Database) is a high performance embedded
transactional database in form of a key-value store
<a class="reference internal" href="#www-keyvalue" id="id199">[173]</a>. LMDB is designed around
virtual memory facilities found in modern operating
systems, multi-version concurrency control (MVCC)
and single-level store (SLS) concepts. LMDB stores
arbitrary key/data pairs as byte arrays, provides a
range-based search capability, supports multiple
data items for a single key and has a special mode
for appending records at the end of the database
(MDB_APPEND) which significantly increases its write
performance compared to other similar databases.</p>
<p>LMDB is not a relational database <a class="reference internal" href="#www-relationaldb" id="id200">[174]</a> and
strictly uses key-value store. Key-value databases
allows one write at a time, the difference that LMDB
highlights is that write transactions do not block
readers nor do readers block writes. Also, it does
allow multiple applications on the same system to
open and use the store simultaneously which helps in
scaling up performance <a class="reference internal" href="#www-lmdb" id="id201">[175]</a>.</p>
</li>
<li><p class="first">Hazelcast</p>
<p>Hazelcast is a java based, in memory data grid. <a class="reference internal" href="#www-wikihazel" id="id202">[176]</a>
It is open source software, released under the Apache 2.0 License.
<a class="reference internal" href="#www-githubhazel" id="id203">[177]</a> Hazelcast enables predictable scaling for
applications by providing in memory access to data.
<a class="reference internal" href="#www-wikihazel" id="id204">[176]</a> Hazelcast uses a grid to distribute data evenly
across a cluster. Clusters allow processing and storage to scale
horizontally. Hazelcast can run locally, in the cloud, in virtual
machines, or in Docker containers. <a class="reference internal" href="#www-wikihazel" id="id205">[176]</a></p>
</li>
<li><p class="first">Ehcache</p>
<p>EHCACHE is an open-source Java-based cache. It supports distributed
caching and could scale to hundred of caches. It comes with REST APIs
and could be integrated with popular frameworks like Hibernate
<a class="reference internal" href="#www-ehcache-features" id="id206">[178]</a>. It offers storage tires such that less
frequently data could be moved to slower tires
<a class="reference internal" href="#www-ehcache-documentation" id="id207">[179]</a>. It&#8217;s XA compliant and supports two-
phase commit and recovery for transactions. It&#8217;s developed and
maintained by Terracotta and is available under Apache 2.0 license.
It conforms to Java caching standard JSR 107.</p>
</li>
<li><p class="first">Infinispan</p>
</li>
<li><p class="first">VoltDB</p>
</li>
<li><p class="first">H-Store</p>
<p>H-Store is an in memory and parallel database management system
for on-line transaction processing (OLTP). Specifically ,
<a class="reference internal" href="#www-hstore" id="id208">[180]</a> illustrates that H-Store is a highly
distributed, row-store-based relational database that runs on a
cluster on shared-nothing, main memory executor nodes.As Noted in
<a class="reference internal" href="#kallman2008" id="id209">[181]</a> &#8220;the architectural and application shifts
have resulted in modern OLTP databases increasingly falling short
of optimal performance.In particular, the availability of
multiple-cores, the abundance of main memory, the lack of user
stalls, and the dominant use of stored procedures are factors
that portend a clean-slate redesign of RDBMSs&#8221;.The H-store which
is a complete redesign has the potential to outperform legacy
OLTP databases by a significant factor.  As detailed in
<a class="reference internal" href="#www-hstorewiki" id="id210">[182]</a> H-Store is the first implementation of a
new class of parallel DBMS, called NewSQL, that provides the
high-throughput and high-availability of NoSQL systems, but
without giving up the transactional guarantees of a traditional
DBMS.  The H-Store system is able to scale out horizontally
across multiple machines to improve throughput, as opposed to
moving to a more powerful , more expensive machine for a
single-node system.</p>
</li>
</ol>
</div>
<div class="section" id="object-relational-mapping">
<h2>Object-relational mapping<a class="headerlink" href="#object-relational-mapping" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="192">
<li><p class="first">Hibernate</p>
</li>
<li><p class="first">OpenJPA</p>
</li>
<li><p class="first">EclipseLink</p>
<p>EclipseLink is an open source persistence Services project from Eclipse
foundation. It is a framework which provide developers to
interact with data services including database and web services,
Object XML mapping etc. <a class="reference internal" href="#www-eclipselink" id="id211">[183]</a>. This is the project
which was developed out of Oracle&#8217;s Toplink product. The main
difference is EclipseLink does not have some key enterprise
feature. Eclipselink support a number of persistence standard
model like JPA, JAXB, JCA and Service Data Object. Like Toplink,
the ORM (Object relational model) is the technique to convert
incompatible type system in Object Oriented programming
language. It is a framework for storing java object into
relational database.</p>
</li>
<li><p class="first">DataNucleus</p>
<p>DataNucleus (available under Apache 2 open source license) is a
data management framework in Java. Formerly known as Java
Persistent Objects (JPOX) this was relaunched in 2008 as
DataNucleus. According to <a class="reference internal" href="#datanucleuswiki" id="id212">[184]</a> DataNucleus
Access Platform is a fully compliant implementation of the Java
Persistent API (JPA) and Java Data Objects (JDO)
specifications. It provides persistence and retrieval of data to
a number of datastores using a number of APIs, with a number of
query languages. In addition to object-relational mapping (ORM)
it can also map and manage data from sources other than RDBMS
(PostgreSQL, MySQL, Oracle, SQLServer, DB2, H2 etc.) such as
Map-based (Cassandra, HBase), Graph-based (Neo4j), Documents
(XLS, OOXML, XML, ODF), Web-based (Amazon S3, Google Storage,
JSON), Doc-based (MongoDB) and Others (NeoDatis, LDAP). It
supports the JPA (Uses JPQL Query language), JDO (Uses JDOQL
Query language) and REST APIs <a class="reference internal" href="#datanucleus" id="id213">[185]</a>.DataNucleus
products are built from a sequence of plugins where each of it is
an OSGi bundle and can be used in an OSGi environment. Google App
Engine uses DataNucleus as the Java persistence layer
<a class="reference internal" href="#datanucleusperformance" id="id214">[186]</a>.</p>
</li>
<li><p class="first">ODBC/JDBC</p>
</li>
</ol>
</div>
<div class="section" id="extraction-tools">
<h2>Extraction Tools<a class="headerlink" href="#extraction-tools" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="197">
<li><p class="first">UIMA</p>
<p>Unstructured Information Management applications (UIMA) provides
a framework for content analytics. It searches unstructured data
to retrieve specific targets for the user. For example, when a
text document is given as input to the system, it identifies
targets such as persons, places, objects and even
associations. According to , <a class="reference internal" href="#uima-wiki" id="id215">[187]</a> theUIMA
architecture can be thought of as four dimensions: 1. Specifies
component interfaces in analytics pipeline.  2. Describes a set
of Design patterns. 3. Suggests two data representations: an
in-memory representation of annotations for high-performance
analytics and an XML representation of annotations for
integration with remote web services. 4. Suggests development
roles allowing tools to be used by users with diverse skills.</p>
<p>UIMA uses different, possibly mixed, approaches which include
Natural Language Processing, Machine Learning, IR. UIMA supports
multimodal analytics <a class="reference internal" href="#uima-ss" id="id216">[188]</a> which enables the system to
process the resource fro various points of view. UIMA is used in
several software projects such as the IBM Research&#8217;s Watson uses
UIMA for analyzing unstructured data and Clinical Text Analysis
and Knowledge Extraction System (Apache cTAKES) which is a
UIMA-based system for information extraction from medical
records.</p>
</li>
</ol>
<ol class="arabic" start="381">
<li><p class="first">Tika</p>
<p>&#8220;The Apache Tika toolkit detects and extracts metadata and text
from over a thousand different file types (such as PPT, XLS, and
PDF). All of these file types can be parsed through a single
interface, making Tika useful for search engine indexing, content
analysis, translation, and much more. <a class="reference internal" href="#www-tika" id="id217">[189]</a>&#8220;</p>
</li>
</ol>
</div>
<div class="section" id="sql-newsql">
<h2>SQL(NewSQL)<a class="headerlink" href="#sql-newsql" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="198">
<li><p class="first">Oracle</p>
</li>
<li><p class="first">DB2</p>
</li>
<li><p class="first">SQL Server</p>
<p>SQL Server <a class="reference internal" href="#www-sqlserver-wiki" id="id218">[190]</a> is a relational database
management system from Microsoft. As of Jan 2017, SQL Server is
available in below editions</p>
<ol class="arabic simple">
<li>Standard - consists of core database engine</li>
<li>Web - low cost edition for web hosting</li>
<li>Business Intelligence - includes standard edition and business
intelligence tools like PowerPivot, PowerBI, Master Data Services</li>
<li>Enterprise - consists of core database engine and enterprise services
like cluster manager</li>
<li>SQL Server Azure - <a class="reference internal" href="#www-azuresql" id="id219">[191]</a> core database engine
integrated with Microsoft Azure cloud platform and available in
platform-as-a-service mode.</li>
</ol>
<p>In the book <a class="reference internal" href="#book-sqlserver" id="id220">[192]</a>, the technical architecture of SQL Server in
OLTP(online transaction processing), hybrid cloud and business
intelligence modes is explained in detail.</p>
</li>
<li><p class="first">SQLite</p>
</li>
<li><p class="first">MySQL</p>
<p>MySQL is a relational database management system. <a class="reference internal" href="#devmysql" id="id221">[193]</a> SQL
is an acronym for Structured Query Language and is a standardized
language used to interact with the databases. <a class="reference internal" href="#devmysql" id="id222">[193]</a>
Databases provide structure to a collection of data
while. <a class="reference internal" href="#devmysql" id="id223">[193]</a> A database management system allows for the
addition, accessing, and processing of the data stored in a
database. <a class="reference internal" href="#devmysql" id="id224">[193]</a> Relational databases utilize tables that are
broken down into columns, representing the various fields of the
table, and rows, which correspond to individual entries in the
table. <a class="reference internal" href="#howmysql" id="id225">[194]</a></p>
</li>
<li><p class="first">PostgreSQL</p>
</li>
<li><p class="first">CUBRID</p>
<p>CUBRID name is deduced from the combination of word CUBE(security
within box) and BRIDGE(data bridge).  It is an open source
Relational DataBase Management System designed in C programming
language with high performance, scalability and availability
features. During its development by NCL, korean IT service
provider the goal was to optimize database performance for
web-applications. <a class="reference internal" href="#www-cubrid" id="id226">[195]</a> Importantly most of the SQL
syntax from MYSQL and ORACLE can work on cubrid.CUBRID also
provides manager tool for database administration and migration
tool for migrating the data from DBMS to CUBRID bridging the dbs.
CUBRID enterprise version and all the tools are free and suitable
database candidate for web-application development.</p>
</li>
<li><p class="first">Galera Cluster</p>
<p>Galera cluster <a class="reference internal" href="#www-galera-cluster" id="id227">[196]</a> is a type of database
clustering which has all multiple masters and works on
synchronous replication. At a deeper level, it was created by
extending MySql replication API to provide all support for true
multi master synchronous replication.  This extended api is
called as Write-Set Replication API and is the core of the
clustering logic.  Each transaction of wsrep API not only
contains the record but also other meta-info to requires to
commit each node separately or asynchronously. So though it seems
synchronous logically but works independently on each node.  The
approach is also called virtually synchronous replication. This
helps in directly read-write on a specific node and can lose a
node without handling any complex failover scenarios (zero
downtime).</p>
</li>
<li><p class="first">SciDB</p>
</li>
<li><p class="first">Rasdaman</p>
</li>
<li><p class="first">Apache Derby</p>
</li>
<li><p class="first">Pivotal Greenplum</p>
</li>
<li><p class="first">Google Cloud SQL</p>
</li>
<li><p class="first">Azure SQL</p>
</li>
<li><p class="first">Amazon RDS</p>
<p>According to Amazon Web Services, Amazon Relation Database
Service (Amazon RDS) is a web service which makes it easy to
setup, operate and scale relational databases in the cloud. As
mentioned in <a class="reference internal" href="#amazonrds" id="id228">[197]</a> It allows to create and use
MySQL, Oracle, SQL Server, and PostgreSQL databases in the
cloud. Thus, codes, applications and tools used with existing
databases can be used with Amazon RDS. The basic components of
Amazon(As listed in <a class="reference internal" href="#amazonrdscomponents" id="id229">[198]</a>) RDS include: DB
Instances: DB instance is an isolated database environment in the
cloud. Regions and availability zones: Region is a data center
location which contains Availability Zones. Availability Zone is
isolated from failures in other Availability Zones. Security
groups: controls access to DB instance by allowing access to IP
address ranges or Amazon EC2 instances that is specified. DB
parameter groups: manage configuration of DB engine by specifying
engine configuration values that are applied to one or more DB
instances of the same instance type. DB option groups: Simplifies
data management through Oracle Application Express (APEX), SQL
Server Transparent Data Encryption, and MySQL memcached support.</p>
</li>
<li><p class="first">Google F1</p>
</li>
<li><p class="first">IBM dashDB</p>
<p>IBM dashDB is a data warehousing service hosted in cloud ,
This aims at integrating the data from various sources into a
cloud data base. Since the data base is hosted in cloud it
would have the benifits of a cloud like scalability and less
maintainance. This data base can be configured as &#8216;transaction
based&#8217; or &#8216;Analytics based&#8217; depending on the work load
<a class="reference internal" href="#www-ibm-dash-db-com" id="id230">[199]</a> .This is available through ibm blue mix
cloud platform.</p>
<p>dash DB has build in analytics based on IBM Netezza Analytics
in the PureData System for Analytics. Because of the build in
analytics and support of
in memory optimization promises better performance efficieny.
This can be run alone as a standalone or can be connected to
variousBI or analytic tools. <a class="reference internal" href="#www-ibm-analytics-com" id="id231">[200]</a></p>
</li>
<li><p class="first">N1QL</p>
</li>
<li><p class="first">BlinkDB</p>
</li>
<li><p class="first">Spark SQL</p>
</li>
</ol>
</div>
<div class="section" id="nosql">
<h2>NoSQL<a class="headerlink" href="#nosql" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="218">
<li><p class="first">Lucene</p>
<p>Apache Lucene <a class="reference internal" href="#www-lucene" id="id232">[201]</a> is a high-performance,
full-featured text search engine library.  It is originally
written in pure Java but also has been ported to few other
languages chiefly python.  It is suitable for applications that
requires full-text search.  One of the key implementation of
Lucene is Internet search engines and local, single-site
searching.  Another important implementation usage is its
recomendation system. The core idea of Lucene is to extract text
from any document that contains text (not image) field, making it
format idependent.</p>
</li>
<li><p class="first">Solr</p>
</li>
<li><p class="first">Solandra</p>
<p>Solandra is a highly scalable real-time search engine built on
Apache Solr and Apache Cassandra. Solandra simplifies maintaining
a large scale search engine, something that more and more
applications need. At its core, Solandra is a tight integration
of Solr and Cassandra, meaning within a single JVM both Solr and
Cassandra are running, and documents are stored and disributed
using Cassandra&#8217;s data model. <a class="reference internal" href="#www-solandra" id="id233">[202]</a></p>
<p>Solandra supports most out-of-the-box Solr functionality (search,
faceting, highlights), multi-master (read/write to any node). It
features replication, sharding, caching, and compaction managed
by Cassandra. <a class="reference internal" href="#www-solandra2" id="id234">[203]</a></p>
</li>
<li><p class="first">Voldemort</p>
<p>According to <a class="reference internal" href="#www-voldemort" id="id235">[204]</a>, project Voldemort, developed
by LinkedIn, is a non-relational database of key-value type that
supports eventual consistency. The distributed nature of the
system allows pluggable data placement and provides horizontal
scalability and high consistency. Replication and partitioning of
data is automatic and performed on multiple servers. Independent
nodes that comprise the server support transparent handling of
server failure and ensure absence of a central point of
failure. Essentially, Voldemort is a hashtable. It uses APIs for
data replication. In memory caching allows for faster
operations. It allows cluster expansion with no data rebalancing.
When Voldemort performance was benchmarked with the other
key-value databases such as Cassandra, Redis and HBase as well as
MySQL relational database <a class="reference internal" href="#rabl-sadoghi-jacobsen-2012" id="id236">[205]</a>, the
Voldemart&#8217;s throughput was twice lower than MySQL and Cassandra
and six times higher than HBase. Voldemort was slightly
underperforming in comparison with Redis. At the same time, it
demonstrated consistent linear performance in maximum throughput
that supports high scalability. The read latency for Voldemort
was fairly consistent and only slightly underperformed
Redis. Similar tendency was observed with the read latency that
puts Voldermort in the cluster of databases that require good
read-write speed for workload operations. However, the same
authors noted that Voldemort required creation of the node
specific configuration and optimization in order to successfully
run a high throughput tests. The default options were not
sufficient and were quickly saturated that stall the database.</p>
</li>
<li><p class="first">Riak</p>
<p>Riak is a set of scalable distributed NoSQL databases developed by
Basho Technologies. Riak KV is a key-value <a class="reference internal" href="#www-riak-kv" id="id237">[206]</a> database
with time-to-live feature so that older data is deleted automatically.
It can be queried through secondary indexes, search via Apache Solr,
and MapReduce. Riak TS is designed for time-series data. It co-
locates related data on the same physical cluster for faster access
<a class="reference internal" href="#www-riak-ts" id="id238">[207]</a>. Riak S2 is designed to store large objects like media
files and software binaries <a class="reference internal" href="#www-riak-s2" id="id239">[208]</a>. The databases are available
in both open source and commercial versions with multicluster
replication provided only in later. REST APIs are available for these
databases.</p>
</li>
<li><p class="first">ZHT</p>
<p>According to <a class="reference internal" href="#datasys" id="id240">[209]</a>, ZHT is a zero-hop distributed hash
table. Distributed hash tables effectively break a hash table up
and assign different nodes responsibility for managing different
pieces of the larger hash table. <a class="reference internal" href="#wiley" id="id241">[210]</a> To retrieve a value in a
distributed hash table, one needs to find the node that is
responsible for the managing the key value pair of
interest. <a class="reference internal" href="#wiley" id="id242">[210]</a> In general, every node that is a part of the
distributed hash table has a reference to the closest two nodes
in the node list. <a class="reference internal" href="#wiley" id="id243">[210]</a> In a ZHT, however, every node contains
information concerning the location of every other node. <a class="reference internal" href="#li" id="id244">[211]</a>
Through this approach, ZHT aims to provide high availability,
good fault tolerance, high throughput, and low latencies, at
extreme scales of millions of nodes. <a class="reference internal" href="#li" id="id245">[211]</a> Some of the defining
characteristics of ZHT are that it is light-weight, allows nodes
to join and leave dynamically, and utilizes replication to obtain
fault tolerance among others. <a class="reference internal" href="#li" id="id246">[211]</a></p>
</li>
<li><p class="first">Berkeley DB</p>
<p>Berkeley DB is a family of open source, NoSQL key-value database libraries.
<a class="reference internal" href="#www-bdb-wiki" id="id247">[212]</a> It provides a simple function-call API for data access
and management over a number of programming languages, including C, C++,
Java, Perl, Tcl, Python, and PHP. Berkeley DB is embedded because it links
directly into the application and runs in the same address space as the
application. <a class="reference internal" href="#www-bdb-stanford" id="id248">[213]</a> As a result, no inter-process
communication, either over the network or between processes on the same
machine, is required for database operations. It is also extremely portable
and scalable, it can manage databases up to 256 terabytes in size.</p>
<p><a class="reference internal" href="#www-bdb" id="id249">[214]</a> For data management, Berkeley DB offers advanced services,
such as concurrency for many users, ACID transactions, and recovery.</p>
<p>Berkeley DB is used in a wide variety of products and a large number of
projects, including gateways from Cisco, Web applications at Amazon.com
and open-source projects such as Apache and Linux.</p>
</li>
<li><p class="first">Kyoto/Tokyo Cabinet</p>
<p>Tokyo Cabinet <a class="reference internal" href="#www-tokyo-cabinet" id="id250">[215]</a> and Kyoto Cabinet
<a class="reference internal" href="#www-kyoto-cabinet" id="id251">[216]</a> are libraries of routines for managing a
database. The database normally is a simple data file containing
records having a key value pair structure. Every key and value is
serial bytes with variable length. Both binary data and character
string can be used as a key and a value. There is no concept of
data tables nor data types like RDBMS or DBMS. Records are
organized in hash table, B+ tree, or fixed-length array.Tokyo and
Kyoto cabinets both are developed as a successor of GDBM and QDBM
which are library routines for managing database as well. Tokyo
Cabinet is written in the C language, and is provided as API of
C, Perl, Ruby, Java, and Lua. Tokyo Cabinet is available on
platforms which have API conforming to C99 and POSIX. Whereas
Kyoto Cabinet is written in the C++ language, and is provided as
API of C++, C, Java, Python, Ruby, Perl, and Lua. Kyoto Cabinet
is available on platforms which have API conforming to C++03 with
the TR1 library extensions. Both are free software licenced under
GNU (General Public Licence). <a class="reference internal" href="#www-tokyo-cabinet" id="id252">[215]</a> actually mentions
that Kyoto Cabinet is more powerful and has convenient library
structure than Tokyo and recommends people to use Kyoto. Since
they use key-value pair concept, you can store a record with a
key and a value, delete a record using the key and even retrive a
record using the key. Both have smaller size of database file,
faster processing speed and provide effective backup procedures.</p>
</li>
<li><p class="first">Tycoon</p>
<p>Tycoon/ Kyoto Tycoon <a class="reference internal" href="#tycoon-fl" id="id253">[217]</a> is a lightweight database
server developed by FLL labs and is a distributed Key-value store
<a class="reference internal" href="#tycoon-cf" id="id254">[218]</a>. It is very useful in handling cache data
persistent data of various applications. Kyoto Tycoon is also a
package of network interface to the DBM called Kyoto Cabinet
<a class="reference internal" href="#tycoon-fl2" id="id255">[219]</a> which contains a library of routines for
managing a database. Tycoon is composed of a sever process that
manger multiple databases. This renders high concurrency enabling
it to handle more than 10 thousand connections at the same time.</p>
</li>
<li><p class="first">Tyrant</p>
<p>Tyrant provides network interfaces to the database management
system called Tokyo Cabinet. Tyrant is also called as Tokyo
Tyrant. Tyrant is implemented in C and it provides APIs for Perl,
Ruby and C. Tyrant provides high performance and concurrent
access to Tokyo Cabinet. The blog <a class="reference internal" href="#www-tyrant-blog" id="id256">[220]</a>
explains the results of performance experiments between Tyrant and
Memcached + MySQL.</p>
<p>Tyrant was written and maintained by FAL Labs
<a class="reference internal" href="#www-tyrant-fal-labs" id="id257">[221]</a>.  However, according to FAL Labs,
their latest product <a class="reference internal" href="#www-kyoto-tycoon" id="id258">[222]</a> Kyoto Tycoon is
more powerful and convenient server than Tokyo Tyrant.</p>
</li>
<li><p class="first">MongoDB</p>
<p>MongoDB is a NoSQL database which uses collections and documents
to store data as opposed to the relational database where data is
stored in tables and rows. In MongoDB a collection is a container
for documents, whereas a document contains key-value pairs for storing
data. As MongoDB is a NoSQL database, it supports dynamic schema design
allowing documents to have different fields. The database uses a document
storage and data interchange format called BSON, which provides a binary
representation of JSON-like documents.</p>
<p>MongoDB provides high data availability by way of replication and
sharding. High cost involved in data replication can be reduced by
horizontal data
scaling by way of shards where data is scattered across multiple
servers. It reduces query cost as the query load is distributed
across servers. This means that both read and write performance
can be increased by adding more shards to a cluster. Which document
resides on which shard is determined by the shard key of each collection.</p>
<p>As far as data backup and restore is concerned the default MongoDB
storage engines natively support backup of complete data. For incremental
backups one can use MongoRocks that is a third party tool developed by Facebook.</p>
</li>
<li><p class="first">Espresso</p>
</li>
<li><p class="first">CouchDB</p>
</li>
<li><p class="first">Couchbase</p>
<p>Couchbase, Inc. offers Couchbase Server (CBS) to the marketplace
as a NoSQL, document-oriented database alternative to traditional
relationship- oriented database managgement systems as well as
other NoSQL competitors.  The basic storage unit, a <em>document</em>,
is a &#8220;data structure defined as a collection of named fields&#8221;.
The document utilizes JSON, thereby allowing each document to
have its own individual schema. <a class="reference internal" href="#www-infoworld-cbs" id="id259">[223]</a></p>
<p>CBS combines the in-memory capabilities of Membase with CouchDB&#8217;s
inherent data store reliability and data persistency.  Membase
functions in RAM only, providing the highest-possible speed
capabilities to end users.  However, Membase&#8217;s in-ram existence
limits the amount of data it can use.  More importantly, it
provides no mechanism for data recovery if the server crashes.
Combining Membase with CouchDB provides a persistent data source,
mitigating the disadvantages of either product.  In addition,
CouchDB + membase allows the data size &#8220;to grow beyond the size
of RAM&#8221;.  <a class="reference internal" href="#www-safaribooks-cbs" id="id260">[224]</a></p>
<p>CBS is written in Erlang/OTP, but generally shortened to just
Erlang.  In actuality, t is written in &#8220;Erlang using components
of OTP alongside some C/C++&#8221;<a class="reference internal" href="#www-erlangcentral-cbs" id="id261">[225]</a>, It
runs on an Erlang virtual machine known as
BEAM. <a class="reference internal" href="#www-wikipedia-erlang-cbs" id="id262">[226]</a></p>
<p>Out-of-the-box benefits of Erlang/OTP include dynamic type
setting, pattern matching and, most importantly, actor-model
concurrency.  As a result, Erlang code virtually eliminates the
possibility of inadvertent deadlock scenarios.  In addition,
Erlang/OTP processes are lightweight, spawning new processes does
not consume many resources and message passing between processes
is fast since they run in the same memory space.  Finally, OTP&#8217;s
process supervision tree makes Erlang/OTP extremely
fault-tolerant.  Error handling is indistinguishable from a
process startup, easing testing and bug detection.
<a class="reference internal" href="#www-couchbase-blog-cbs" id="id263">[227]</a></p>
<p>CouchDB&#8217;s design adds another layer of reliability to CBS.
CouchDB operates in <em>append-only</em> mode, so it adds user changes
to the tail of database.  This setup resists data corruption
while taking a snapshot, even if the server continues to run
during the procedure.  <a class="reference internal" href="#www-hightower-cbs" id="id264">[228]</a></p>
<p>Finally, CB uses the Apache 2.0 License, one of several
open-source license alternatives. <a class="reference internal" href="#www-quora-cbs" id="id265">[229]</a></p>
</li>
<li><p class="first">IBM Cloudant</p>
</li>
<li><p class="first">Pivotal Gemfire</p>
<p>According to <a class="reference internal" href="#www-gemfire" id="id266">[230]</a>, a real-time, consistent access
to data-intensive applications is provided by a open source, data
management platform named Pivotal Gemfire. &#8220;GemFire pools memory,
CPU, network resources, and optionally local disk across multiple
processes to manage application objects and behavior&#8221;. The main
features of Gemfire are high scalability, continuous
availability, shared nothing disk persistence, heterogeneous data
sharing and parallelized application behavior on data stores to
name a few.  In Gemfire, clients can subscribe to receive
notifications to execute their task based on a specific change in
data. This is achieved through the continuous querying feature
which enables event-driven architecture. The shared nothing
architecture of Gemfire suggests that each node is
self-sufficient and independent, which means that if the disk or
caches in one node fail the remaining nodes remaining
untouched. Additionally, the support for multi-site
configurations enable the user to scale horizontally between
different distributed systems spread over a wide geographical
network.</p>
</li>
<li><p class="first">HBase</p>
<p>Apache Hbase is a distributed column-oriented database
which is built on top of HDFS (Hadoop Distributed File
System).According to <a class="reference internal" href="#www-hbase" id="id267">[231]</a>, It is a open source,
versioned, distributed, non-relational database modelled after
Googles Bigtable. Similar to Bigtable providing harnessing
distributed file storage system offered by Google file system,
Apache Hbase provides similar capabilities on top of Hadoop and
HDFS. Moreover, Hbase supports random, real-time CRUD
(Create/Read/Update/Delete) operations.</p>
<p>Hbase is a type of NoSQL database and is classified as a key value
store.In HBase, value is identied with a key where both of them are
stored as byte arrays. Values are stored in the order of keys. HBase
is a database system where the tables have no schema. Some of the
companies that use HBase as their core program are Facebook, Twitter,
Adobe, Netflix etc.</p>
</li>
<li><p class="first">Google Bigtable</p>
<p>Google Bigtable is a NoSQL database service, built upon several Google
technologies, including Google File System, Chubby Lock Service, and
SSTable. <a class="reference internal" href="#www-cloudbigtable" id="id268">[232]</a>  Designed for Big Data, Bigtable
provides high performance and low latency and scales to hundreds of
petabytes. <a class="reference internal" href="#www-cloudbigtable" id="id269">[232]</a> Bigtable powers many core
Google products, such as Search, Analytics, Maps, Earth, Gmail,
and YouTube. <a class="reference internal" href="#www-wikibigtable" id="id270">[233]</a> Since May 6, 2015, a
version of Bigtable has been available to the public.  Bigtable
also drives Google Cloud Datastore <a class="reference internal" href="#www-wikibigtable" id="id271">[233]</a> and
Spanner, a distributed NewSQL database also developed by
Google. <a class="reference internal" href="#www-wikispanner" id="id272">[234]</a></p>
</li>
<li><p class="first">LevelDB</p>
</li>
<li><p class="first">Megastore and Spanner</p>
<p>Spanner <a class="reference internal" href="#corbett-spanner" id="id273">[235]</a> is Google&#8217;s distributed database
which is used for managing all google services like play, gmail,
photos, picasa, app engine etc Spanner is distributed database
which spans across multiple clusters, datacenters and geo
locations.  Spanner is structured in such a way so as to provide
non blocking reads, lock free transactions and atomic schema
modification. This is unlike other noSql databases which follow
the CAP theory i.e. you can choose any two of the three:
Consistency, Availability and Partition-tolerance. However,
spanner gives an edge by satisfying all three of these. It gives
you atomicity and consistency along with availability, partition
tolerance and synchronized replication.  Megastore bridges the
gaps found in google&#8217;s bigtable. As google realized that it is
difficult to use bigtable where the application requires
constantly changing schema. Megastore offers a solution in terms
of semi-relational data model.  Megastore
<a class="reference internal" href="#www-magastore-spanner" id="id274">[236]</a> also provides a transactional
database which can scale unlike relational data stores and
synchronous replication.  Replication in megastore is supported
using Paxos. Megastore also provides versioning. However,
megastore has a poor write performance and lack of a SQL like
query language. Spanners basically adds what was missing in
Bigtable and megastore. As a global distributed database spanner
provides replication and globally consistent reads and
writes. Spanner deployment is called universe which is a
collections of zones. These zones are managed by singleton
universe master and placement driver. Replication in spanner is
supported by Paxos state machine. Spanner was put into evaluation
in early 2011 as F1 backend(F1 is Google&#8217;s advertisement system)
which was replacement to mysql. Overall spanner fulfils the needs
of relational database along with scaling of noSQL database.  All
these features make google run all their apps seamlessly on
spanner infrastructure.</p>
</li>
<li><p class="first">Accumulo</p>
</li>
<li><p class="first">Cassandra</p>
<p>Apache Cassandra <a class="reference internal" href="#www-cassandra" id="id275">[237]</a> is an open-source
distributed database managemment for handling large volume of
data accross comodity servers. It works on asynchronous
masterless replication technique leading to low latency and high
availability. It is a hybrid between a key-value and column
oriented database. A table in cassandra can be viewed as a multi
dimensional map indexed by a key. It has its own &#8220;Cassandra Query
language (CQL)&#8221; query language for data extraction and
mining. One of the demerits of such structure is it does not
support joins or subqueries. It is a java based system which can
be administered by any JMX compliant tools.</p>
</li>
<li><p class="first">RYA</p>
<p>Rya is a scalable system for storing and retrieving RDF data in
a cluster of nodes. <a class="reference internal" href="#punnoose" id="id276">[238]</a> RDF stands for Resource
Description Framework. <a class="reference internal" href="#punnoose" id="id277">[238]</a> RDF is a model that facilitates
the exchange of data on a network. <a class="reference internal" href="#w3" id="id278">[239]</a> RDF utilizes a form
commonly referred to as a triple, an object that consists of a
subject, predicate, and object. <a class="reference internal" href="#punnoose" id="id279">[238]</a> These triples are used
to describe resources on the Internet. <a class="reference internal" href="#punnoose" id="id280">[238]</a> Through new
storage and querying techniques, Rya aims to make accessing RDF
data fast and easy. <a class="reference internal" href="#apacherya" id="id281">[240]</a></p>
</li>
<li><p class="first">Sqrrl</p>
</li>
<li><p class="first">Neo4J</p>
</li>
<li><p class="first">graphdb</p>
<p>A Graph Database is a database that uses graph structures for semantic
queries with nodes, edges and properties to represent and store data.
<a class="reference internal" href="#www-graphdb" id="id282">[241]</a>
The Graph is a concept which directly relates the data items in the store.
The data which is present in the store is linked together directly with the
help of relationships. It can be retrieved with a single operation.
Graph database allow simple and rapid retrieval of complex hierarchical
structures that are difficult to model in relational systems.</p>
<p>There are different underlying storage mechanisms used by graph databases.
Some graphdb depend on a relational engine and store the graph data in a
table, while others use a key-value store or document-oriented database for
storage. Thus, they are inherently caled as NoSQL structures.
Data retrieval in a graph database requires a different query language
other than SQL. Some of the query languages used to retrieve data from a
graph database are Gremlin, SPARQL, and Cypher.
Graph databases are based on graph theory. They employ the concepts of
nodes, edges and properties.</p>
</li>
<li><p class="first">Yarcdata</p>
</li>
<li><p class="first">AllegroGraph</p>
<p>AllegroGraph is a database technology that enables businesses to
extract sophisticated decision insights and predictive analytics from
their&nbsp;highly complex, distributed data&nbsp;that cant be answered with
conventional databases, i.e., it turns complex data into actionable
business insights. <a class="reference internal" href="#www-allegro" id="id283">[242]</a>
It can be viewed as a closed source database that is used for storage
and retrieval of data in the form of triples (triple is a data entity
composed of subject-predicate-object like Professor teaches students).
Information in a triplestore is retrieved using a query language. Query
languages can be classified into database query languages or information
retrieval query languages. The difference is that a database query language
gives exact answers to exact questions, while an information retrieval
query language finds documents containing requested information.
Triple format&nbsp;represents information&nbsp;in a machine-readable format.
Every part of the triple is individually addressable via unique&nbsp;URLs&nbsp;
for example, the statement Professor teaches students might be
represented in RDF(Resource Description Framework&nbsp;) as
<a class="reference external" href="http://example.name#Professor12">http://example.name#Professor12</a> <a class="reference external" href="http://xmlns.com/foaf/0.1/teacheshttp">http://xmlns.com/foaf/0.1/teacheshttp</a>:
//example.name#students. Using this representation, semantic data can
be queried.  <a class="reference internal" href="#www-allegrow" id="id284">[243]</a></p>
</li>
<li><p class="first">Blazegraph</p>
</li>
<li><p class="first">Facebook Tao</p>
<p>In the paper published in USENIX annual technical conference,
Facebook Inc describes TAO (The Association and Objects) as
:cite book-tao a geographically distributed data store that
provides timely access to the social graph for Facebooks demanding
workload using a fixed set of queries. It is deployed at Facebook for
many data types that fit its model. The system runs on thousands of
machines, is widely distributed, and provides access to many petabytes
of data. TAO represents social data items as Objects (user) and
relationship between them as Associations (liked by, friend of).
TAO cleanly separates the caching tiers from the persistent data
store allowing each of them to be scaled independently. To any user
of the system it presents a single unified API that makes the entire
system appear like 1 giant graph database. :cite:&#8217;www-tao&#8217;.</p>
</li>
<li><p class="first">Titan:db</p>
<p>Titan:db <a class="reference internal" href="#www-titan" id="id285">[244]</a> is a distributed graph database that
can support of thousands of concurrent users interacting with a
single massive graph database that is distributed over the
clusters. It is open source with liberal Apache 2 license.
Its main components are storage backend, search backend, and
TinkerPop graph stack. Titan provides support for various
storage backends and also linear scalability for a growing data
and user base. It inherits features such as Gremlin query
language  and Rexter graph server from TinkerPop <a class="reference internal" href="#www-tinkerpop" id="id286">[245]</a>.
For huge graphs, Titan uses a component called Titan-hadoop which
compiles Gremlin queries to Hadoop MapReduce jobs and runs them
on the clusters. Titan is basically optimal for smaller graphs.</p>
</li>
<li><p class="first">Jena</p>
<p>Jena is an open source Java Framework provided by Apache for
semantic web applications. (<a class="reference internal" href="#jena-wiki" id="id287">[246]</a>) It provides a
programmatic environment for RDF, RDFS and OWL, SPARQL, GRDDL,
and includes a rule-based inference engine. Semantic web data
differs from conventional web applications in that it supports a
web of data instead of the classic web of documents format. The
presence of a rule based inference engine enable Jena to perform
a reasoning based on OWL and RDFS ontologies.  <a class="reference internal" href="#jena-blog" id="id288">[247]</a>
The architecture of Jena contains three layers : Graph layer,
model layer and Ontology layer. The graph layer forms the base
for the architecture. It does not have an extensive RDF
implementation and serves more as a Service provider
Interface. According to <a class="reference internal" href="#jena-blog" id="id289">[247]</a> It provides
classes/methods that could be further extended. The model layer
extends the graph layer and provides objects of type resource
instead of node to work with.  The ontology layer enables one
to work with triples.</p>
</li>
<li><p class="first">Sesame</p>
<p>Sesame is framework which can be used for the analysis of RDF
(Resource Description Framework) data.  Resource Description
Framework (RDF) <a class="reference internal" href="#www-rdf" id="id290">[248]</a> is a model that facilitates the
interchange of data on the Web.  Using RFD enables us to merge
data even if the underlying schemas differ.  <a class="reference internal" href="#www-sesame" id="id291">[249]</a>
Sesame has now officially been integrated into RDF4J Eclipse
project.  Sesame takes in the natively written code as the input
and then performs a series of transformations, generating kernels
for various platforms.  <a class="reference internal" href="#sesame-paper-2013" id="id292">[250]</a> In order to
achieve this, it makes use of the feature identifier, impact
predictor, source-to-source translator and the auto-tuner.  The
feature identifier is concerned with the extraction and detection
of the architectural features that are important for application
performance.  The impact predictor determines the performance
impact of the core features extracted above.  A source-to-source
translator transforms the input code into a parametrized one;
while the auto-tuner helps find the optimal solution for the
processor.</p>
</li>
<li><p class="first">Public Cloud: Azure Table</p>
<p>Microsoft offers its NoSQL Azure Table product to the market as a
low-cost, fast and scalable data storage
option. <a class="reference internal" href="#www-what-to-use" id="id293">[251]</a> Table stores data as collections
of key-value combinations, which it terms <em>properties</em>.  Table
refers to a collection of properties as an <em>entity</em>.  Each entity
can contain a mix of properties.  The mix of properties can vary
between each entity, although each entity may consist of no more
than 255 properties. <a class="reference internal" href="#www-blobqueuetable" id="id294">[252]</a></p>
<p>Although data in Azure Table will be structured via key-value
pairs, Table provides just one mechanism for the user to define
relationships between entities: the entity&#8217;s <em>primary key</em>.  The
primary key, which Microsoft sometimes calls a <em>clustered index</em>,
consists of a PartitionKey and a RowKey.  The PartitionKey
indicates the group, a.k.a partition, to which the user assigned
the entity.  The RowKey indicates the entity&#8217;s relative position
in the group.  Table sorts in ascending order by the PartitionKey
first, then by the RowKey using lexical comparisons.  As a
result, numeric sorting requires fixed-length, zero-padded
strings.  For instance, Table sorts <em>111</em> before <em>2</em>, but will
sort <em>111</em> after <em>002</em>. <a class="reference internal" href="#www-scalable-partitioning" id="id295">[253]</a></p>
<p>Azure Table is considered best-suited for infrequently accessed
data storage.</p>
</li>
<li><p class="first">Amazon Dynamo</p>
<p>Amazon explains DynamoDB as :cite:&#8217;www.dyndb&#8217; a fast and flexible
NoSQL database service for all applications that need consistent,
single-digit millisecond latency at any scale. It is a fully managed
cloud database and supports both document and key-value store models.
Its flexible data model and reliable performance make it a great fit
for mobile, web, gaming, ad tech, IoT, and many other applications.
DynamoDB can be easily integrated with big-data processing tools like
Hadoop. It can also be integrated with AWS Lambda, an event driven platform,
which enables creating applications that can automatically react to data
changes. At present there are certain limits to DynamoDB. Amazon has listed
all the limits in a web page titled <a class="reference external" href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html">Limits in DynamoDB</a></p>
</li>
</ol>
<p></p>
<ol class="arabic simple" start="253">
<li>Google DataStore</li>
</ol>
</div>
<div class="section" id="file-management">
<h2>File management<a class="headerlink" href="#file-management" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="254">
<li><p class="first">iRODS</p>
</li>
<li><p class="first">NetCDF</p>
</li>
<li><p class="first">CDF</p>
</li>
<li><p class="first">HDF</p>
</li>
<li><p class="first">OPeNDAP</p>
</li>
<li><p class="first">FITS</p>
<p>FITS stand for &#8216;Flexible Image Trasnport System&#8217;. It is a
standard data format used in astronomy. FITS data format is
endorsed by NASA and International Astronomical Union. According
to <a class="reference internal" href="#www-fits-nasa" id="id296">[254]</a>, FITS can be used for transport,
analysis and archival storage of scientific datasets and support
multi-dimensional arrays, tables and headers sections.  FITS is
actively used and developed - according to
<a class="reference internal" href="#www-news-fits-2016" id="id297">[255]</a> newer version of FITS standard
document was released in July 2016. FITS can be used for
digitization of contents like books and
magzines. Vatican Library <a class="reference internal" href="#www-fits-vatican-library" id="id298">[256]</a> used FITS
for long term preservation of their book, manuscripts and other
collection. Matlab, a language used for technical computing
supports fits <a class="reference internal" href="#www-fits-matlab" id="id299">[257]</a>. The 2011 paper
<a class="reference internal" href="#paper-fits-2011" id="id300">[258]</a> explains how to perform
processing of astronomical images on Hadoop using FITS.</p>
</li>
<li><p class="first">RCFile</p>
<p>RCFile (Record Columnar File) <a class="reference internal" href="#www-rcfile" id="id301">[259]</a> is a big
data placement data structure that supports fast data loading and
query processing coupled with efficient storage space utilization
and adaptive to dynamic workload environments. It is designed for
data warehousing systems that uses map-reduce. The data is stored
as a flat file comprising of binary key/value pairs. The rows are
partitioned first and then the columns are partitioned in each
row and the respective meta-data for each row is stored in the
key part for that row and the values comprises of the data part
of the row. Storing the data in this format enables RCFile to
accomplish fast loading and query processing.A shell utility is
available for reading RCFile data and metadata
<a class="reference internal" href="#www-rcfile" id="id302">[259]</a>. According to <a class="reference internal" href="#he2011rcfile" id="id303">[260]</a>, RCFile has
been chosen in Facebook data warehouse system as the default
option. It has also been adopted by Hive and Pig, the two most
widely used data analysis systems developed in Facebook and
Yahoo!</p>
</li>
<li><p class="first">ORC</p>
<p>ORC files were created as part of the initiative to massively
speed up Apache Hive and improve the storage efficiency of data
stored in Apache Hadoop. ORC is a self-describing type-aware
columnar file format designed for Hadoop workloads. It is
optimized for large streaming reads, but with integrated support
for finding required rows quickly. Storing data in a columnar
format lets the reader read, decompress, and process only the
values that are required for the current query. Because ORC files
are type-aware, the writer chooses the most appropriate encoding
for the type and builds an internal index as the file is
written.ORC files are divided in to stripes that are roughly 64MB
by default. The stripes in a file are independent of each other
and form the natural unit of distributed work. Within each
stripe, the columns are separated from each other so the reader
can read just the columns that are required <a class="reference internal" href="#www-orc-docs" id="id304">[261]</a>.</p>
</li>
<li><p class="first">Parquet</p>
<p>Apache parquet is the column Oriented data store for Apache
Hadoop ecosystem and available in any data processing framework,
data model or programming language <a class="reference internal" href="#www-parquet" id="id305">[262]</a>. It
stores data such that the values in each column are physically
stored in contiguous memory locations. As it has the columnar
storage, it provides efficient data compression and encoding
schemes which saves storage space as the queries that fetch
specific column values need not read the entire row data and thus
improving performance.It can be implemented using the Apache
Thrift framework which increases its flexibility to work with a
number of programming languages like C++, Java, Python, PHP, etc.</p>
</li>
</ol>
</div>
<div class="section" id="data-transport">
<h2>Data Transport<a class="headerlink" href="#data-transport" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="263">
<li><p class="first">BitTorrent</p>
<p>Bittorrent is P2P communication protocol commonly used for
sending and receiving the large digital files like movies and
audioclips.In order to upload and download file, user have to
download bittorrent client which implement the bittorrent
protocol. Bittorrent uses the principle of swarning and
tracking. <a class="reference internal" href="#www-bittorrent" id="id306">[263]</a> It divides the files in large
number of chunck and as soon as file is received it can be server
to the other users for downloading.  So rather than downloading
one entire large file from one source, user can download small
chunk from the different sources of linked users in
swarn. Bittorrent trackers keeps list of files available for
transfer and helps the swarn user find each other.</p>
<p>Using the protocol, machine with less configuration can serve as
server for distributing the files. It result in increase in the
downloading speed and reduction in origin server configuration.</p>
<p>Few popular bittorrent client in Torrent, qBittorrent.</p>
</li>
<li><p class="first">HTTP</p>
</li>
<li><p class="first">FTP</p>
<p>According to <a class="reference internal" href="#ftp-wiki" id="id307">[264]</a> FTP is an acronym for File Transfer
Protocol. It is network protocol standard used for transferring
files between two computer systems or between a client and a
server. It is part of the Application layer of the Internet
Protocol Suite and works along with HTTP/SSH. It follows a
client-server model architecture. Secure systems asks the client
to authenticate themselves using a Username and Password
registered with the server to access the files via FTP. The
specification for FTP was first written by Abhay Bhushan
<a class="reference internal" href="#www-rfc114" id="id308">[265]</a> in 1971 and is termed as RFC114. The current
specification, RFC959 in use was written in 1985. Several other
versions of the specification are available which provides
firewall friendly FTP access, additional security extensions,
support for IPV6 and passive mode file access respectively. FTP
can be used in command line in most of the operating systems to
transfer files. There are FTP clients such as WinSCP, FileZilla
etc. which provides a graphical user interface to the clients to
authenticate themselves (sign on) and access the files from the
server.</p>
</li>
<li><p class="first">SSH</p>
<p>SSH is a cryptographic network protocol <a class="reference internal" href="#www-ssh-wiki" id="id309">[266]</a> to
provide a secure channel between two clients over an unsecured
network. It uses public-key cryptography for authenticating the
remote machine and the user. The public-private key pairs could
be generated automatically to encrypt the network connection.
ssh-keygen utility could be used to generate the keys manually.
The public key then could be placed on the all the computers to
which the access is required by the owner of the private key.
SSH runs on the client-server model where a server listens for
incoming ssh connection requests. It&#8217;s generally used for remote
login and command execution. It&#8217;s other important uses include
tunneling(required in cloud computing) and file transfer(SFTP).
OpenSSH is an open source implementation of network utilities
based on SSH <a class="reference internal" href="#www-openssh-wiki" id="id310">[267]</a>.</p>
</li>
<li><p class="first">Globus Online (GridFTP)</p>
<p>GridFTP is a enhancement on the File Tranfer Protocol (FTP) which
provides high-performance , secure and reliable data transfer for
high-bandwidth wide-area networks. As noted in
<a class="reference internal" href="#www-globusonline" id="id311">[268]</a> the most widely used implementation of
GridFTP is Globus Online. GridFTP achieves efficient use of
bandwidth by using multiple simultaneous TCP streams.  Files can
be downloaded in pieces simultaneously from multiple sources; or
even in separate parallel streams from the same source. GridFTP
allows transfers to be restarted automatically and handles
network unavailability with a fault tolerant implementation of
FTP.The underlying TCP connection in FTP has numerous settings
such as window size and buffer size. GridFTP allows automatic (or
manual) negotiation of these settings to provide optimal transfer
speeds and reliability .</p>
</li>
<li><p class="first">Flume</p>
<p>Flume is distributed, reliable and available service for
efficiently collecting, aggregating and moving large amounts of
log data [apche-flume]. Flume was created to allow you to
flow data from a source into your Hadoop environment.  In Flume,
the entities you work with are called sources, decorators, and
sinks. A source can be any data source, and Flume has many
predefined source adapters. A sink is the target of a specific
operation. A decorator is an operation on the stream that can
transform the stream in some manner, which could be to compress
or uncompress data, modify data by adding or removing pieces of
information, and more <a class="reference internal" href="#ibm-flume" id="id313">[269]</a>.</p>
</li>
<li><p class="first">Sqoop</p>
<p>Apache Sqoop is a tool to transfer large amounts of data between Apache Hadoop
and sql databases <a class="reference internal" href="#www-sqoop" id="id314">[270]</a>. The name is a Portmanteau of
SQL + Hadoop. It is a command line interface application which
supports incremental loads of complete tables, free form (custom)
SQL Queries and allows the use of saved and scheduled jobs to import
latest updates made since the last import. The imports can also be
used to populate tables in Hive or Hbase. Sqoop has the option of
export, which allows data to be transferred from Hadoop into a
relational database. Sqoop is supported in many different business
integration suits like Informatica Big Data Management, Pentaho
Data Integration, Microsoft BI Suite and Couchbase <a class="reference internal" href="#sqoop-wiki" id="id315">[271]</a>.</p>
</li>
<li><p class="first">Pivotal GPLOAD/GPFDIST</p>
</li>
</ol>
</div>
<div class="section" id="cluster-resource-management">
<h2>Cluster Resource Management<a class="headerlink" href="#cluster-resource-management" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="271">
<li><p class="first">Mesos</p>
<p>Apache Mesos <a class="reference internal" href="#www-mesos" id="id316">[272]</a> abstracts CPU, memory,
storage, and other compute resources away from machines (physical
or virtual), enabling fault-tolerant and elastic distributed
systems to easily be built and run effectively. The Mesos kernel
runs on every machine and provides applications (e.g., Hadoop,
Spark, Kafka, Elasticsearch) with APIs for resource management
and scheduling across entire datacenter and cloud environments.</p>
<p>The resource scheduler of Mesos supports a generalization of
max-min fairness <a class="reference internal" href="#paper-mesos-abu-dbai-2016" id="id317">[273]</a>, termed Dominant
Resource Fairness (DRF) <a class="reference internal" href="#paper-mesos-ghodsi2011dominant" id="id318">[274]</a>
scheduling discipline, which allows to harmonize execution of
heterogeneous workloads (in terms of resource demand) by
maximizing the share of any resource allocated to a specific
framework.</p>
<p>Mesos uses containers for resource isolation between
processes. In the context of Mesos, the two most important
resource-isolation methods to know about are the control groups
(cgroups) built into the Linux kernel,and Docker. The difference
between using hyper-V, Docker containers, cgroup is described in
detail in the book &#8220;Mesos in action&#8221; <a class="reference internal" href="#book-mesos-ignazio-2016" id="id319">[275]</a></p>
</li>
<li><p class="first">Yarn</p>
<p>Yarn (Yet Another Resource Negotiator) is Apache Hadoops cluster
management project <a class="reference internal" href="#www-cloudera" id="id320">[276]</a> . Its a resource
management technology which make a pace between, the way
applications use Hadoop system resources &amp; node manager
agents. Yarn, split up the functionalities of resource
management and job scheduling/monitoring. The NodeManager watch
the resource (cpu, memory, disk,network) usage the container and
report the same to ResourceManager. Resource manager will take a
decision on allocation of resources to the
applications. ApplicationMaster is a library specific to
application, which requests/negotiate resources from
ResourceManager and launch and monitoring the task with
NodeManager(s) <a class="reference internal" href="#www-architecture" id="id321">[277]</a>.  ResourceManager have
two majors: Scheduler and ApplicationManager. Scheduler have a
task to schedule the resources required by the
application. ApplicationManger holds the record of application
who require resource. It validates (whether to allocate the
resource or not) the applications resource requirement and
ensure that no other application already have register for the
same resource requirement. Also it keeps the track of release of
resource. <a class="reference internal" href="#www-hadoopapache" id="id322">[278]</a></p>
</li>
<li><p class="first">Helix</p>
<p>Helix is a data management system getting developed by IBM which
helps the users to do explitory analysis of the data received
from various sources following different formats. This system
would help orgnaize the data by providing links between data
collected across various sources dispite of the knowledge of the
data sources schemas.It also aims at providing  the data really
required for the user by extracting the important information
from the data. This would plan to target the issue by
mainataining the &#8220;knowledge base of schemas&#8221; and
&#8220;context-dependent dynamic linkage&#8221;, The system can get the
schema details either from the  knowledge base being maintained
or can even get the schema from the data being received. As the
number of users for helix increases the linkages gets stronger
and would provide better data
quality. <a class="reference internal" href="#www-ibm-helix-paper" id="id323">[279]</a></p>
</li>
<li><p class="first">Llama</p>
</li>
<li><p class="first">Google Omega</p>
</li>
<li><p class="first">Facebook Corona</p>
</li>
<li><p class="first">Celery</p>
</li>
<li><p class="first">HTCondor</p>
</li>
<li><p class="first">SGE</p>
</li>
<li><p class="first">OpenPBS</p>
</li>
<li><p class="first">Moab</p>
</li>
<li><p class="first">Slurm <a class="reference internal" href="#www-slurm" id="id324">[280]</a></p>
<p>Simple Linux Utility for Resource Management (SLURM) workload
manager is an open source, scalable cluster resource management
tool used for job scheduling in small to large Linux cluster
using multi-core architecture. As per,
<a class="reference internal" href="#www-slurmschedmdsite" id="id325">[281]</a> SLURM has three key
functions. First, it allocates resources to users for some
duration with exclusive and/or non-exclusive access. Second, it
enables users to start, execute and monitor jobs on the resources
allocated to them. Finally, it intermediates to resolve conflicts
on resources for pending work by maintaining them in a queue. The
slurm architecture has following components: a centralized
manager to monitor resources and work, may have a backup manager,
daemon on each server to provide fault-tolerant communications,
an optional daemon for clusters with multiple mangers and tools
to initiate, terminate and report about jobs in a graphical view
with network topology. It also provides around twenty additional
plugins that could be used for functionalities like accounting,
advanced reservation, gang scheduling, back fill scheduling and
multifactor job prioritization. Though originally developed for
Linux, SLURM also provides full support on platforms like AIX,
FreeBSD, NetBSD and Solaris <a class="reference internal" href="#www-slurmplatformssite" id="id326">[282]</a>.</p>
</li>
<li><p class="first">Torque</p>
</li>
<li><p class="first">Globus Tools</p>
</li>
<li><p class="first">Pilot Jobs</p>
<p>In pilot job, an application acquires a resource so that it can
be delegated some work directly by the application; instead of
requiring some job scheduler. The issue of using a job scheduler
is that a waiting queue is required. Few examples of Pilot Jobs
are the <a class="reference internal" href="#pilot-job-falkon-paper-2007" id="id327">[283]</a> Falkon lightweight
framework and <a class="reference internal" href="#pilot-job-htcaas-paper-2007" id="id328">[284]</a> HTCaaS. Pilot
jobs are typically associated with both Parallel computing as
well as Distributed computing. Their main aim is to reduce the
dependency on queues and the associated multiple wait times.</p>
<p><a class="reference internal" href="#www-pilot-job-paper-2016" id="id329">[285]</a> Using pilot jobs enables us to have a
multilevel technique for the execution of various workloads. This is so
because the jobs are typically acquired by a placeholder job and they
relayed to the workloads.</p>
</li>
</ol>
</div>
<div class="section" id="file-systems">
<h2>File systems<a class="headerlink" href="#file-systems" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="286">
<li><p class="first">HDFS</p>
<p>Hadoop provides distributed file system framework that uses Map
reduce (Distributed computation framework) for transformation and
analyses of large dataset.  Its main work is to partition the
data and other computational tasks to be performed on that data
across several clusters.  HDFS is the component for distributed
file system in Hadoop.An HDFS cluster primarily consists of a
Name Node and Data Nodes. Name Node manages the file system
metadata such as access permission, modification time, location
of data and Data Nodes store the actual data.&nbsp;  When user
applications or Hadoop frameworks request access to a file in
HDFS, Name Node service responds with the Data Node locations for
the respective individual data blocks that constitute the whole
of the requested <a class="reference external" href="file:cite">file:cite</a>:<cite>www-hdfs</cite>.</p>
</li>
<li><p class="first">Swift</p>
</li>
<li><p class="first">Haystack</p>
</li>
<li><p class="first">f4</p>
</li>
<li><p class="first">Cinder</p>
<p>&#8220;Cinder is a block storage service for Openstack&#8221;
<a class="reference internal" href="#wiki-cinder" id="id330">[286]</a>. According to <a class="reference internal" href="#book-cinder" id="id331">[287]</a> Openstack
Compute uses ephemeral disks meaning that they exist only for the
life of the Openstack instance i.e. when the instance is
terminated the disks disappear. Block storage system is a type of
persistent storage that can be used to persist data beyond the
life of the instance. Cinder provides users with access to
persistent block-level storage devices. It is designed such that
users can create block storage devices on demand and attach them
to any running instances of OpenStack
Compute. <a class="reference internal" href="#wiki-cinder" id="id332">[286]</a> This is achieved through the use of
either a reference implementation(LVM) or plugin drivers for
other storage. Cinder virtualizes the management of block storage
devices and provides end users with a self-service API to request
and consume those resources without requiring any knowledge of
where their storage is actually deployed or on what type of
device.</p>
</li>
<li><p class="first">Ceph</p>
</li>
<li><p class="first">FUSE</p>
<p>FUSE (Filesystem in Userspace) <a class="reference internal" href="#www-fuse" id="id333">[288]</a> &#8220;is an interface
for userspace programs to export a filesystem to the Linux
kernel&#8221;. The FUSE project consists of two components: the fuse
kernel module and the libfuse userspace library. libfuse provides
the reference implementation for communicating with the FUSE
kernel module.The code for FUSE itself is in the kernel, but the
filesystem is in userspace.  As per the 2006 paper
<a class="reference internal" href="#fuse-paper-hptfs" id="id334">[289]</a> on HPTFS which has been built on top of
FUSE. It mounts a tape as normal file system based data storage
and provides file system interfaces directly to the application.
Another implementation of FUSE FS is CloudBB
<a class="reference internal" href="#fuse-paper-cloudbb" id="id335">[290]</a>. Unlike conventional filesystems
CloudBB creates an on-demand two-level hierarchical storage
system and caches popular files to accelerate I/O performance. On
evaluating performance of real data-intensive HPC applications in
Amazon EC2/S3, results show CloudBB improves performance by up to
28.7 times while reducing cost by up to 94.7% compared to the
ones without CloudBB.</p>
<p>Some more implementation examples of FUSE are - mp3fs (A VFS to
convert FLAC files to MP3 files instantly), Copy-FUSE(To access
cloud storage on Copy.com), mtpfs(To mount MTP devices) etc.</p>
</li>
<li><p class="first">Gluster</p>
</li>
<li><p class="first">Lustre</p>
<p>The Lustre file system <a class="reference internal" href="#www-lustre" id="id336">[291]</a> is an open-source,
parallel file system that supports many requirements of
leadership class HPC simulation environments and Enterprise
environments worldwide. Because Lustre file systems have high
performance capabilities and open licensing, it is often used in
supercomputers.Lustre file systems are scalable and can be part
of multiple computer clusters with tens of thousands of client
nodes, tens of petabytes of storage on hundreds of servers, and
more than a terabyte per second of aggregate I/O
throughput. Lustre file systems a popular choice for businesses
with large data centers, including those in industries such as
meteorology, simulation, oil and gas, life science, rich media,
and finance. Lustre provides a POSIX compliant interface and many
of the largest and most powerful supercomputers on Earth today
are powered by the Lustre file system.</p>
</li>
<li><p class="first">GPFS</p>
<p>IBM General Parallel File System (GPFS) was rebranded to IBM
Spectrum Scale on February 17, 2015.  <a class="reference internal" href="#www-wikigpfs" id="id337">[292]</a>
See 380.</p>
</li>
</ol>
<ol class="arabic" start="380">
<li><p class="first">IBM Spectrum Scale</p>
<p>General Parallel File System (GPFS) was rebranded as IBM Spectrum
Scale on February 17, 2015. <a class="reference internal" href="#www-wikigpfs" id="id338">[292]</a></p>
<p>Spectrum Scale is a clustered file system, developed by IBM, designed
for high performance. It &#8220;provides concurrent high-speed file access
to applications executing on multiple nodes of clusters&#8221;
<a class="reference internal" href="#www-wikigpfs" id="id339">[292]</a> and can be deployed in either shared-nothing
or shared disk modes. Spectrum Scale is available on AIX, Linux,
Windows Server, and IBM System Cluster 1350. <a class="reference internal" href="#www-wikigpfs" id="id340">[292]</a>
Due to its focus on performance and scalability, Spectrum Scale has
been utilized in compute clusters, big data and analytics (including
support for Hadoop Distributed File System (HDFS), backups and
restores, and private clouds. <a class="reference internal" href="#www-spectrumscale" id="id341">[293]</a></p>
</li>
</ol>
<ol class="arabic" start="296">
<li><p class="first">GFFS</p>
<p>The Global Federated File System (GFFS) <a class="reference internal" href="#www-gffs" id="id342">[294]</a> is a
computing technology that allows linking of data from Windows,
Mac OS X, Linux, AFS, and Lustre file systems into a global
namespace, making them available to multiple systems. It is a
federated, secure, standardized, scalable, and transparent
mechanism to access and share resources across organizational
boundaries It is useful when, for data resources, boundaries do
not require application modification and do not disrupt existing
data access patterns. It uses FUSE to handle access control and
allows research collaborators on remote systems to access a
shared file system. Existing applications can access resources
anywhere in the GFFS without modification. It helps in rapid
development of code, which can then be exported via GFFS and
implemented in-place on a given computational resource or Science
Gateway.</p>
</li>
<li><p class="first">Public Cloud: Amazon S3</p>
<p>Amazon Simple Storage Service (Amazon S3) <a class="reference internal" href="#www-amazon-s3" id="id343">[295]</a> is
storage object which provides a simple web service interface to
store and retrieve any amount of data from anywhere on the
web. With Amazon S3, users can store as much data as they want
and can scale it up and down based on the requirements.For
developers Amazon S3 provides full REST API&#8217;s and SDK&#8217;s which can
be integrated with third-party technologies. Amazon S3 is also
deeply integrated with other AWS services to make it easier to
build solutions that use a range of AWS services which include
Amazon CloudFront, Amazon CloudWatch, Amazon Kinesis, Amazon RDS,
Amazon Glacier etc. Amazon S3 provides auotmatic encryption of
data once the data is uploaded in the cloud. Amazon S3 uses the
concept of Buckets and Objects for storing data wherein Buckets
are used to store objects. Amazon S3 services can be used using
the Amazon Console Management. <a class="reference internal" href="#www-amazon-s3-docs" id="id344">[296]</a> The steps
for using the Amazon S3 are as follows: (1) Sign up for Amazon S3
(2) After sign up, create a Bucket in your account, (3) Create
and object which might be an file or folder, and (4) Perform
operations on the object which is stored in the cloud.</p>
</li>
<li><p class="first">Azure Blob</p>
<p>Azure Blob storage is a service that stores unstructured data in the cloud
as objects/blobs. Blob storage can store any type of text or binary data,
such as a document, media file, or application installer <a class="reference internal" href="#www-azure-3" id="id345">[297]</a>
Blob storage is also referred to as object storage. The word Blob expands
to Binary Large OBject. There are three types of blobs in the service offe-
red by Windows Azure namely block, append and page blobs. <a class="reference internal" href="#www-azure-2" id="id346">[298]</a>
1. Block blobs are collection of individual blocks with unique block ID.
The block blobs allow the users to upload large amount of data.
2. Append blobs are optimized blocks that helps in making the operations
efficient.
3. Page blobs are compilation of pages. They allow random read and write
operations. While creating a blob, if the type is not specified they are
set to block type by default. All the blobs must be inside a container in
your storage.
Azure Blob storage is a service for storing large amounts of unstructured
object data, such as text or binary data, that can be accessed from
anywhere in the world via HTTP or HTTPS. You can use Blob storage to expose
data publicly to the world, or to store application data privately. Common
uses of Blob storage include serving images or documents directly to a
browser, storing files for distributed access, streaming video and audio,
storing data for backup and restore, disaster recovery, and archiving and
storing data for analysis by an on-premises or Azure-hosted service.
Azure Storage is massively scalable and elastic with an auto-partitioning
system that automatically load-balances your data. Blob storage is a
specialized storage account for storing your unstructured data as blobs
(objects) in Azure Storage. Blob storage is similar to existing
general-purpose storage accounts and shares all the great durability,
availability, scalability, and performance features. Blob storage has two
types of access tiers that can be specified, hot access tier, which will be
accessed more frequently, and a cool access tier, which will be less
frequently accessed. There are many reasons why you should consider using
BLOB storage. Perhaps you want to share files with clients, or off-load
some of the static content from your web servers to reduce the load on
them. <a class="reference internal" href="#www-azure-3" id="id347">[297]</a></p>
</li>
<li><p class="first">Google Cloud Storage</p>
<p>Google Cloud Storage is the cloud enabled storage offered by
Google. <a class="reference internal" href="#www-google-cloud-storage" id="id348">[299]</a> It is unified object
storage. To have high availability and performance among
different regions in the geo-redundant storage offering. If you
want high availability and redundancy with a single region one
can go for Regional storage. Nearline and Coldline are the
different archival storage techniques. Nearline storage
offering is for the archived data which the user access less than
once a month . Coldline storage is the storage which is used
for the data which is touched less than once a year.</p>
<p>All the data in Google Cloud storage belongs inside a project. A
project will contains different buckets. Each bucket has
different objects. We need to make sure that the name of the
bucket is unique across all Google cloud name space . And the
name of the objects should unique in a bucket.</p>
</li>
</ol>
</div>
<div class="section" id="interoperability">
<h2>Interoperability<a class="headerlink" href="#interoperability" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="300">
<li><p class="first">Libvirt</p>
</li>
<li><p class="first">Libcloud</p>
<p>:cite::<cite>www-libcloudwiki</cite> Libcloud is a python library that
allows to interact with several popular cloud service
providers. It is primarily designed to ease development of
software products that work with one or more cloud services
supported by Libcloud. It provides a unified API to interact with
these different cloud services. Current API includes methods for
list, reboot, create, destroy, list images and list
sizes. :cite::<cite>www-libclouddoc</cite> lists Libcloud key component APIs
Compute, Storage, Load Balancers, DNS, Container and
Backup. Compute API allows users to manage cloud servers. Storage
API allows users to manage cloud object storage and also provides
CDN management functionality. Load balancer, DNS and Backup APIs
allows users to manage their respective functionalities, as
services, and related products of different cloud service
providers. Container API allows users to deploy containers on to
container virtualization platforms. Libcloud supports Python 2,
Python 3 and PyPy.</p>
</li>
<li><p class="first">JClouds</p>
<p><a class="reference internal" href="#cloud-portability-book" id="id349">[300]</a> Primary goals of cross-platform
cloud APIs is that application built using these APIs can be
seamlessly ported to different cloud providers. The APIs also
bring interoperability such that cloud platforms can communicate
and exchange information using these common or shared interfaces.
Jclouds or apache jclouds <a class="reference internal" href="#www-jclouds" id="id350">[301]</a> is a java based
library to provide seamless access to cloud platforms. Jclouds
library provides interfaces for most of cloud providers like
docker, openstack, amazon web services, microsoft azure, google
cloud engine etc. It will allow users build applications which
can be portable across different cloud environments.  Key
components of jcloud are:</p>
<ol class="arabic">
<li><p class="first">Views: abstracts functionality from a specific vendor and
allow user to write more generic code. For example odbc
abstracts the underlying relational data source. However, odbc
driver converts to native format. In this case user can switch
databases without rewriting the application. Jcloud provide
following views: blob store, compute service, loadBalancer
service</p>
</li>
<li><p class="first">API: APIs are requests to execute a particular
functionality. Jcloud provide a single set of APIs for all
cloud vendors which is also location aware. If a cloud vendor
doesnt support customers from a particular region the API
will not work from that region.</p>
</li>
<li><p class="first">Provider: a particular cloud vendor is a provider. Jcloud uses
provider information to initialize its context.</p>
</li>
<li><p class="first">Context: it can be termed as a handle to a particular
provider. Its like a ODBC connection object. Once connection
is initialized for a particular database, it can used to make
any api call.</p>
<p>Jclouds provides test library to mock context, APIs etc to
different providers so that user can write unit test for his
implementation rather than waiting to test with the cloud
provider. Jcloud library certifies support after testing the
interfaces with live cloud provider. These features make
jclouds robust and adoptable, hiding most of the complexity of
cloud providers.</p>
</li>
</ol>
</li>
<li><p class="first">TOSCA</p>
</li>
<li><p class="first">OCCI</p>
<p>The Open Cloud Computing Interface (OCCI) is a RESTful
Protocol and API that provides specifications  and remote
management for the development of interoperable tools
<a class="reference internal" href="#www-occi" id="id351">[302]</a>. It supports IaaS, PaaS and SaaS and
focuses on integration, portability, interoperability,
innovation and extensibility. It provides a set of documents
that describe an OCCI Core model, contain best practices
of interaction with the model, combined into OCCI Protocols,
explain methods of communication between components via
HTTP protocol introduced in the OCCI Renderings, and
define infrastructure for IaaS presented in the OCCI
Extensions.</p>
<p>The current version 1.2 OCCI consists of seven documents that
identify require and optional components. Of the Core Model.  In
particular, the following components are required to implement:
a)Core Model, b)HTTP protocol, c)Text rendering and d)JSON
rendering. Meanwhile, Infrastructure, Platform and SLA models are
optional.  The OCCI Core model defines instance types and</p>
<p>provides a layer of abstraction that allows the OCCI client
to interact with the model without knowing of its potential
structural changes. The model supports extensibility via
inheritance and using mixin types that represent ability to
add new components and capabilities at run-time.
<a class="reference internal" href="#nyren-edmonds-papaspyrou-2016" id="id352">[303]</a></p>
<p>The OCCI Protocol defines the common set of names provided
for the IaaS cloud services user that specify requested
system requirements. It is often denoted as resource
templates or flavours   <a class="reference internal" href="#drescher-parak-wallom-2015" id="id353">[304]</a>.</p>
<p>OCCI RESTful HTTP Protocol describes communications between
server and client on OCCI platform via HTTP protocol
<a class="reference internal" href="#nyren-edmonds-metsch-2016" id="id354">[305]</a>. It defines a minimum set of HTTP
headers and status codes to ensure compliance with the
OCCI Protocol. Separate requirements for Server and Client
for versioning need to be implemented using HTTP &#8216;Server&#8217;
header and &#8216;User-Agent&#8217; header respectively.</p>
<p>JSON rendering  <a class="reference internal" href="#nyren-feldhaus-parak-2016" id="id355">[306]</a> protocol provides
JSON specifications to allow &#8220;render OCCI instances
independently of the protocol being used.&#8221; In addition, it
provides details of the JSON object declaration, OCCI Action
Invocation, object members required for OCCI Link Instance
Rendering, &#8220;location maps to OCCI Core&#8217;s source and target
model attributes and kind maps to OCCI Core&#8217;s target&#8221; to
satisfy OCCI Link Instance Source/Target Rendering requirements.
Finally, it specifies various attributes and collection
rendering requirements.
The text rendering process is depricated and will be
removed from the next major version  <a class="reference internal" href="#edmonds-metsch-2016" id="id356">[307]</a>.</p>
</li>
<li><p class="first">CDMI</p>
</li>
<li><p class="first">Whirr</p>
</li>
<li><p class="first">Saga</p>
<p>SAGA(Simple API for Grid Applications) provides an abstraction layer
to make it easier for applications to utilize and exploit infra
effectively. With infrastructure being changed continuously its
becoming difficult for most applications to utilize the advances in
hardware. SAGA API provides a high level abstraction of the most
common Grid functions so as to be independent of the diverse and
dynamic Grid environments. <a class="reference internal" href="#saga-paper" id="id357">[308]</a> This shall address the
problem of applications developers developing an application tailored
to a specific set of infrastructure.  SAGA allows computer scientists
to write their applications at high level just once and not to worry
about low level hardware changes. SAGA provides this high level
interface which has the underlying mechanisms and adapters to make the
appropriate calls in an intelligent fashion so that it can work on any
underlying grid system. SAGA was built to provide a standardized,
common interface across various grid middleware systems and their
versions.  <a class="reference internal" href="#www-saga-ogf-document" id="id358">[309]</a></p>
<p>As SAGA is to be implemented on different types
of middleware it does not specify a single security model but provides
hooks to interfaces of various security models. The SAGA API provides
a set of packages to implement its objectivity : SAGA supports data
management, resource discovery, asynchronous notification, event
generation, event delivery etc. It does so by providing set of
functional packages namely SAGA file package, replica package, stream
package, RPC package, etc. SAGA provides interoperability by allowing
the same application code to run on multiple grids and also
communicate with applications running on others. <a class="reference internal" href="#saga-paper" id="id359">[308]</a></p>
</li>
<li><p class="first">Genesis</p>
</li>
</ol>
</div>
<div class="section" id="devops">
<h2>DevOps<a class="headerlink" href="#devops" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="309">
<li><p class="first">Docker (Machine, Swarm)</p>
</li>
<li><p class="first">Puppet</p>
<p>Puppet is an open source software configuration management
tool <a class="reference internal" href="#www-puppet-wiki-puppet" id="id360">[310]</a>.This aims at automatic
configuration of the software
applications and infrastructure. This configuration is done
using the easy to use languge.
Puppet works on major linux distributions and also on
microsoft windows ,
it is also cross-platform application making it easy to manage
and portable. <a class="reference internal" href="#www-puppet-puppet-site" id="id361">[311]</a></p>
<p>Puppet works with a client server model. All the clients (
nodes)  which needs to be managed will have &#8216;Puppet Agent&#8217;
installed and &#8216;Puppet Master&#8217; contains the configuration for
different hosts this demon process rund on master server. The
connection between &#8216;Puppet Master&#8217; and &#8216;Puppet agent&#8217; will be
established using thesecured SSL connection. The configiration
at client will be validated as per the set up in Puppet master
at a predefined interval. If configration at client is not
matching with the master puppet agent fetches the equired
changes from master. <a class="reference internal" href="#www-puppet-slashroot" id="id362">[312]</a></p>
<p>Puppet is developed by Puppet Labs
using ruby language and released as GNU General Public License
(GPL) until version 2.7.0 and the Apache License 2.0 after
that. <a class="reference internal" href="#www-puppet-wiki-puppet" id="id363">[310]</a></p>
</li>
<li><p class="first">Chef</p>
<p>Chef is a configuration management tool. It is implemented in
Ruby and Erlang. Chef can be used to configure and maintain
servers on-premise as well as cloud platforms like Amazon EC2,
Google Cloud Platform and Open Stack. The book
<a class="reference internal" href="#chef-book" id="id364">[313]</a> explains the use of concept called &#8216;recipes&#8217; in
Chef to manage server applications and utilities such as database
servers like MySQL, or HTTP servers like Apache HTPP and systems
like Apache Hadoop.</p>
<p>Chef is available in open source version and it also has
commercial products for the companies which need it
<a class="reference internal" href="#www-chef-commercial" id="id365">[314]</a></p>
</li>
<li><p class="first">Ansible</p>
<p>Ansible is an IT automation tool that automates cloud
provisioning, configuration management, and application
deployment. <a class="reference internal" href="#www-ansible" id="id366">[315]</a> Once Ansible gets installed on a
control node, which is an agentless architecture, it connects to
a managed node through the default OpenSSH connection
type. <a class="reference internal" href="#www-ansible2" id="id367">[316]</a></p>
<p>As with most configuration management softwares, Ansible
distinguishes two types of servers: controlling machines and
nodes. First, there is a single controlling machine which is
where orchestration begins. Nodes are managed by a controlling
machine over SSH. The controlling machine describes the location
of nodes through its inventory.</p>
<p>Ansible manages machines in an agent-less manner. Ansible is
decentralized, if needed, Ansible can easily connect with
Kerberos, LDAP, and other centralized authentication management
systems.</p>
</li>
<li><p class="first">SaltStack</p>
</li>
<li><p class="first">Boto</p>
<p><a class="reference internal" href="#www-boto" id="id368">[317]</a> The latest version of Boto is Boto3.
<a class="reference internal" href="#www-boto-github" id="id369">[318]</a> Boto3 is the Amazon Web Services (AWS) Software
Development Kit (SDK) for Python. It enables the Python developers to
make use of services like Amazon S3 and Amazon EC2.
<a class="reference internal" href="#www-boto3-documentation" id="id370">[319]</a> It provides object oriented APIs along
with low-level direct service access. It provides simple in-built
functions and interfaces to work with Amazon S3 and EC2.</p>
<p><a class="reference internal" href="#www-boto-amazon-python-sdk" id="id371">[320]</a> Boto3 has two distinct levels of APIs
- client and resource. One-to-one mappings to underlying HTTP API is
provided by the client APIs. Resource APIs provide resource objects and
collections to perform various actions by accessing the attributes.
Boto3 also comes with &#8216;waiters&#8217;. Waiters are used for polling status
changes in AWS, automatically. Boto3 has these waiters for both the APIs
- client as well as resource.</p>
</li>
<li><p class="first">Cobbler</p>
<p>Cobbler is a Linux provisioning system that facilitates and
automates the network based system installation of multiple computer
operating systems from a central point using services such as DHCP,
TFTP and DNS <a class="reference internal" href="#www-cobbler" id="id372">[321]</a>.It is a nifty piece of code that
assemble s all the usual
setup bits required for a large network installation like TFTP, DNS,
PXE installation trees. and automates the process[1].It can be
configured for PXE, reinstallations and virtualized guests using Xen,
KVM or VMware.  Cobbler interacts with the koan program for
re-installation and virtualization support.  Cobbler builds the
Kickstart mechanism and offers installation profiles that can be
applied to one or many machines.  Cobbler has features to dynamically
change the information contained in a kickstart template (definition),
either by passing variables called ksmeta or by using so-called
snippets.</p>
</li>
<li><p class="first">Xcat</p>
</li>
<li><p class="first">Razor</p>
<p>Razor is a hardware provisioning application, developed by Puppet
Labs and EMC. Razor was introduced as open, pluggable, and
programmable since most of the provisioning tools that existed
were vendor-specific, monolithic, and closed. According to
<a class="reference internal" href="#razorwiki" id="id373">[322]</a> it can deploy both bare-metal and virtual
systems. During boot the Razor client automatically discovers the
inventory of the server hardware  CPUs, disk, memory, etc.,
feeds this to the Razor server in real-time and the latest state
of every server is updated. It maintains a set of rules to
dynamically match the appropriate operating system images with
server capabilities as expressed in metadata. User-created policy
rules are referred to choose the preconfigured model to be
applied to a new node. The node follows the model&#8217;s directions,
giving feedback to Razor as it completes various steps as
specified in <a class="reference internal" href="#razorpuppet" id="id374">[323]</a>. Models can include steps for
handoff to a DevOps system or to any other system capable of
controlling the node.</p>
</li>
<li><p class="first">CloudMesh</p>
</li>
<li><p class="first">Juju</p>
<p>Juju (formerly Ensemble) <a class="reference internal" href="#juju-paper" id="id375">[324]</a> is software from
Canonical that provides open source service orchestration. It is
used to easily and quickly deploy and manage services on cloud
and physical servers. Juju charms can be deployed on cloud
services such as Amazon Web Services (AWS), Microsoft Azure and
OpenStack. It can also be used on bare metal using MAAS.
Specifically <a class="reference internal" href="#www-juju" id="id376">[325]</a> lists around 300 charms available
for services available in the Juju store. Charms can be written
in any language. It also supports Bundles which are
pre-configured collection of Charms that helps in quick
deployment of whole infrastructure.</p>
</li>
<li><p class="first">Foreman</p>
</li>
<li><p class="first">OpenStack Heat</p>
<p>Openstack Heat, a template deployment service was the project
launched by Openstack, a cloud operating system similar to AWS
Cloud Formation. <a class="reference internal" href="#www-heat-blog-introduction" id="id377">[326]</a> states - Heat
is an orchestration service which allows us to define resources
over the cloud and connections amongst them using a simple text
file called referred as a template. &#8220;A Heat template describes
the infrastructure for a cloud application in a text file that is
readable and writable by humans, and can be checked into version
control&#8221; <a class="reference internal" href="#www-heat-wiki" id="id378">[327]</a></p>
<p>Once the execution enviroment has been setup and a user wants to
modify the architecture of resources in the future, a user needs
to simply change the template and check it in. Heat shall make
the necessary changes. Heat provides 2 types of template -
HOT(Heat Orchestration Template) and CFN (AWS Cloud Formation
Template). The HOT can be defined as YAML and is not compatible
with AWS. The CFN is expressed as JSON and follows the syntax of
AWS Cloud Formation and thus is AWS compatible. Further, heat
provides an additional &#64;parameters section in its template which
can be used to parameterize resources to make the template
generic.</p>
</li>
<li><p class="first">Sahara</p>
<p>The Sahara product provides users with the capability to
provision data processing frameworks (such as Hadoop, Spark and
Storm) on OpenStack <a class="reference internal" href="#www-openstack" id="id379">[328]</a> by specifying several
parameters such as the version,cluster topology and hardware node
details.As specified in <a class="reference internal" href="#www-sahara" id="id380">[329]</a> the solution allows
for fast provisioning of data processing clusters on OpenStack
for development and quality assurance and utilisation of unused
computer power from a general purpose OpenStack Iaas Cloud.Sahara
is managed via a REST API with a User Interface available as part
of OpenStack Dashboard.</p>
</li>
<li><p class="first">Rocks</p>
</li>
<li><p class="first">Cisco Intelligent Automation for Cloud</p>
<p>Cisco Intelligent automation for cloud desires to help different
service providers and software professionals in delivering highly
secure infrastructure as a service on demand. It provides a
foundation for organizational transformation by expanding the
uses of cloud technology beyond its infrastructure
<a class="reference internal" href="#cis1" id="id381">[330]</a>. From a single self-service portal, it automates
standard business processes and sophisticated data center which
is beyond the provision of virtual machines. Cisco Intelligent
automation for cloud is a unified cloud platform that can deliver
any type of service across mixed environments <a class="reference internal" href="#cis2" id="id382">[331]</a>. This
leads to an increase in cloud penetration across different
business and IT holdings. Its services range from underlying
infrastructure to anything-as-a-service by allowing its users to
evaluate, transform and deploy the IT and business services in a
way they desire.</p>
</li>
<li><p class="first">Ubuntu MaaS</p>
</li>
<li><p class="first">Facebook Tupperware</p>
</li>
<li><p class="first">AWS OpsWorks</p>
<p>AWS Opsworks is a configuration service provided by Amazon Web
Services that uses Chef, a Ruby and Erlang based configuration
management tool <a class="reference internal" href="#www-wikichef" id="id383">[332]</a>, to automate the
configuration, deployment, and management of servers and
applications. There are two versions of AWS Opsworks.
The first, a fee based offering called AWS OpsWorks for Chef
Automate, provides a Chef Server and suite of tools to enable
full stack automation. The second, AWS OpsWorks Stacks, is a
free offering in which applications are modeled as stacks
containing various layers. Amazon Elastic Cloud Compute (EC2)
instances or other resources can be deployed and configured
in each layer of AWS OpsWorks Stacks. <a class="reference internal" href="#www-awsopsworks" id="id384">[333]</a></p>
</li>
<li><p class="first">OpenStack Ironic</p>
</li>
<li><p class="first">Google Kubernetes</p>
<p>Google Kubernetes is a cluster management platform developed by
Google. According to <a class="reference internal" href="#www-kubernetesdoc" id="id385">[334]</a> is an open source
system for &#8220;automating deployment, scaling and management of
containerized applications&#8221;. It primarily manages clusters
through containers as they decouple applications from the
host operating system dependencies and allowing their quick and
seamless deployment, maintenance and scaling.</p>
<p>Kubernetes components are designed to extensible primarily
through Kubernetes API. Kubernetes follows a master-slave
architecture, according to <a class="reference internal" href="#www-kuberneteswiki" id="id386">[335]</a> Kubernetes
Master controls and manages the clusters workload and
communications of the system. Its main components are etcd, API
server, scheduler and controller manager. The individual
Kubernetes nodes are the workers where containers are
deployed. The components of a node are Kubelet, Kube-proxy and
cAdvisor. Kunernetes makes it easier to run application on public
and private clouds. It is also said to be self-healing due to
features like auto-restart and auto-scaling.</p>
</li>
<li><p class="first">Buildstep</p>
<p>Buildsteps is an open software developed under MIT license.
It is a base for Dockerfile and it activates Heroku-style
application. Heroku is a platform-as-service (PaaS) that
automates deployment of applications on the cloud. The
program is pushed to the PaaS using git push, and then
PaaS detects the programming language, builds, and runs
application on a cloud platform <a class="reference internal" href="#plassnig-2015" id="id387">[336]</a>.
Buildstep takes two parameters: a tar file that contains
the application and a new application container name to
create a new container for this application. Build script
is dependent on buildpacks that are pre-requisites for
buildstep to run. The builder script runs inside the new
container.  The resulting build app can be run with Docker
using docker build -t your_app_name command.
<a class="reference internal" href="#gonzalez-2015" id="id388">[337]</a>.</p>
</li>
<li><p class="first">Gitreceive</p>
</li>
<li><p class="first">OpenTOSCA</p>
</li>
<li><p class="first">Winery</p>
<p>Eclipse Winery <a class="reference internal" href="#www-winery" id="id389">[338]</a> is a &#8220;web-based environment to
graphically model [Topology and Orchestration Specification for
Cloud Applications] TOSCA topologies and plans managing these
topologies.&#8221; Winery <a class="reference internal" href="#winery-paper-2013" id="id390">[339]</a> is a &#8220;tool
offering an HTML5-based environment for graph-based modeling of
application topologies and defining reusable component and
relationship types.&#8221; This web-based <a class="reference internal" href="#winery-paper-2013" id="id391">[339]</a>
interface enables users to drag and drop icons to create
automated &#8220;provisioning, management, and termination of
applications in a portable and interoperable way.&#8221;
Essentially, this web-based interface <a class="reference internal" href="#winery-paper-2013" id="id392">[339]</a>
allows users to create an application topology, which
&#8220;describes software and hardware components involved and
relationships between them&#8221; as well a management plan, which
&#8220;captures knowledge [regarding how] to deploy and manage an
application.&#8221;</p>
</li>
<li><p class="first">CloudML</p>
</li>
<li><p class="first">Blueprints</p>
<p>In <a class="reference internal" href="#www-blueprints" id="id393">[340]</a>, it is explained that &#8220;IBM Blueprint
has been replaced by IBM Blueworks Live.&#8221; In
<a class="reference internal" href="#www-blueworks-live2" id="id394">[341]</a>, IBM Blueworks Live is described &#8220;as
a cloud-based business process modeller, belonging under the set
of IBM SmartCloud applications&#8221; that as
<a class="reference internal" href="#www-blueworks-live" id="id395">[342]</a> states &#8220;drive[s] out inefficiencies
and improve[s] business operations.&#8221; Similarly to Google Docs,
IBM Blueworks Live is &#8220;designed to help organizations discover
and document their business processes, business decisions and
policies in a collaborative manner.&#8221; While Google Docs and IBM
Blueworks Live are both simple to use in a collaborative manner,
<a class="reference internal" href="#www-blueworks-live2" id="id396">[341]</a> explains that IBM Blueworks Live
has the &#8220;capabilities to implement more complex models.&#8221;</p>
</li>
<li><p class="first">Terraform</p>
<p>Terraform, developed by HashiCorp, is an infrastructure
management tool, it has an open source platform as well as an
enterprise version and uses infrastructure as a code to increase
operator productivity. Its latest release is Terraform 0.8
According to the website <a class="reference internal" href="#www-terraform" id="id397">[343]</a> it enables users
to safely and predictably create, change and improve the
production infrastructure and codifies APIs into declarative
configuration files that can be shared amongst other users and
can be treated as a code, edited, reviewed and versioned at the
same time. The book <a class="reference internal" href="#www-terraform-book" id="id398">[344]</a> explains that it
can manage the existing and popular service it provides as well
as create customized in-house solutions. It builds an execution
plan that describes what it can do next after it reaches a
desired state to accomplish the goal state. It provides a
declarative executive plan which is used for creating
applications and implementing the infrastructures. Terraform is
mainly used to manage cloud based and SaaS infrastructure, it
also supports Docker and VMWare vSphere.</p>
</li>
<li><p class="first">DevOpSlang</p>
</li>
<li><p class="first">Any2Api</p>
<p>This framework <a class="reference internal" href="#wettinger-any2api" id="id399">[345]</a> allows user to wrap an
executable program or scripts, for example scripts, chef
cookbooks, ansible playbooks, juju charms, other compiled
programs etc. to generate APIs from your existing code.  These
APIs are also containerized so that they can be hosted on a
docker container, vagrant box etc Any2Api helps to deal with
problems like scale of application, technical expertise, large
codebase and different API formats. The generated API hide the
tool specific details simplifying the integration and
orchestration different kinds of artifacts. The APIfication
framework contains various modules:</p>
<ol class="arabic simple">
<li>Invokers, which are capable of running a given type of
executable for example cookbook invoker can be used to run Chef
cookbooks</li>
<li>Scanners, which are capable of scanning modules of certain type for
example cookbook scanner scans Chef cookbooks.</li>
<li>API impl generators, which are doingthe actual work to
generate the API implementation.</li>
</ol>
<p>The final API implementation <a class="reference internal" href="#www-any2api" id="id400">[346]</a> is is packages
with executable in container.  The module is packaged as npm
module. Currently any2api-cli provides a command line interface
and web based interface is planned for future
development. Any2Api is very useful for by devops to orchestrate
open source ecosystem without dealing with low level details of
chef cookbook or ansible playbook or puppet. It can also be very
useful in writing microservices where services talk to each other
using well defined APIs.</p>
</li>
</ol>
</div>
<div class="section" id="iaas-management-from-hpc-to-hypervisors">
<h2>IaaS Management from HPC to hypervisors<a class="headerlink" href="#iaas-management-from-hpc-to-hypervisors" title="Permalink to this headline"></a></h2>
<ol class="arabic" start="339">
<li><p class="first">Xen</p>
<p>Xen is the only open-source bare-metal hypervisor based on
microkernel design <a class="reference internal" href="#www-xen-wikipedia" id="id401">[347]</a>. The hypervisor
runs at the highest privilege among all the processes on the
host. It&#8217;s responsibility is to manage CPU and memory and
handle interrupts <a class="reference internal" href="#www-xen-overview" id="id402">[348]</a>. Virtual
machines are deployed in the guest domain called DomU which
has no access privilege to hardware. A special virtual machine
is deployed in the control domain called Domain 0. It contains
hardware drivers and the toolstack to control the VMs and is
the first VM to be deployed. Xen supports both Paravirtualization
and hardware assisted virtualization. The hypervisor itself has
a very small footprint. It&#8217;s being actively maintained by Linux
Foundation under the trademark &#8220;XEN Project&#8221;. Some of the
features included in the latest releases include &#8220;Reboot-free
Live Patching&#8221; (to enable application of security patches without
rebooting the system) and KCONFIG support (compilation support to
create a lighter version for requirements such as embedded
systems) <a class="reference internal" href="#www-xen-fl" id="id403">[349]</a>.</p>
</li>
<li><p class="first">KVM</p>
</li>
<li><p class="first">QEMU</p>
<p>QEMU (Quick Emulator) is a generic open source hosted hypervisor
<a class="reference internal" href="#www-hypervisor" id="id404">[350]</a> that performs hardware virtualization
(virtualization of computers as complete hardware platform,
certain logical abstraction of their componentry or only the
certain functionality required to run various operating systems)
<a class="reference internal" href="#www-qemu" id="id405">[351]</a> and also emulates CPUs through dynamic binary
translations and provides a set of device models, enabling it to
run a variety of unmodified guest operating systems.</p>
<p>When used as an emulator, QEMU can run Operating Systems and programs
made for one machine (ARM board) on a different machine (e.g. a
personal computer) and achieve good performance by using dynamic
translations.  When used as a virtualizer, QEMU achieves near native
performance by executing the guest code directly on the host CPU. QEMU
supports virtualization when executing under the Xen hypervisor or
using KVM kernel module in Linux <a class="reference internal" href="#www-qemuwiki" id="id406">[352]</a>.</p>
<p>Compared to other virtualization programs like VMWare and VirtualBox,
QEMU does not provide a GUI interface to manage virtual machines nor
does it provide a way to create persistent virtual machine with saved
settings. All parameters to run virtual machine have to be specified
on a command line at every launch. Its worth noting that there are
several GUI front-ends for QEMU like virt-manager and gnome-box.</p>
</li>
<li><p class="first">Hyper-V</p>
</li>
<li><p class="first">VirtualBox</p>
</li>
<li><p class="first">OpenVZ</p>
<p>OpenVZ (Open Virtuozzo) is an operating system-level virtualization
technology for Linux. It allows a physical server to run multiple isolated
operating system instances, called containers, virtual private servers, or
virtual environments (VEs). OpenVZ is similar to Solaris Containers and
LXC. <a class="reference internal" href="#www-openvz-3" id="id407">[353]</a> While virtualization technologies like VMware and
Xen provide full virtualization and can run multiple operating systems and
different kernel versions, OpenVZ uses a single patched Linux kernel and
therefore can run only Linux. All OpenVZ containers share the same archite-
cture and kernel version. This can be a disadvantage in situations where
guests require different kernel versions than that of the host. However, as
it does not have the overhead of a true hypervisor, it is very fast and
efficient. Memory allocation with OpenVZ is soft in that memory not used in
one virtual environment can be used by others or for disk caching. <a class="reference internal" href="#www-openvz-2" id="id408">[354]</a>
While old versions of OpenVZ used a common file system (where each virtual
environment is just a directory of files that is isolated using chroot),
current versions of OpenVZ allow each container to have its own file system.
OpenVZ has four main features, <a class="reference internal" href="#www-openvz-1" id="id409">[355]</a>
1. OS virtualization: A container (CT) looks and behaves like a regular
Linux system. It has standard startup scripts; software from vendors can
run inside a container without OpenVZ-specific modifications or adjustment;
A user can change any configuration file and install additional software;
Containers are completely isolated from each other and are not bound to
only one CPU and can use all available CPU power.
2. Network virtualization: Each CT has its own IP address and CTs are
isolated from the other CTs meaning containers are protected from each
other in the way that makes traffic snooping impossible; Firewalling may
be used inside a CT
3. Resource management: All the CTs are use the same kernel. OpenVZ
resource management consists of four main components: two-level disk quota,
fair CPU scheduler, disk I/O scheduler, and user beancounters.
4. Checkpointing and live migration: Checkpointing allows to migrate a
container from one physical server to another without a need to
shutdown/restart a container. This feature makes possible scenarios such as
upgrading your server without any need to reboot it: if your database needs
more memory or CPU resources, you just buy a newer better server and live
migrate your container to it, then increase its limits.</p>
</li>
<li><p class="first">LXC</p>
<p>LXC (Linux Containers) is an operating-system-level
virtualization method for running multiple isolated
Linux systems (containers) on a control host using a single
Linux kernel <a class="reference internal" href="#www-wiki-lxc" id="id410">[356]</a>. LXC are similar to the treditional virtual
machines but instead of having seperate kernel process for the
guest operating system being run, containers would share the
kernal process with the host operating system. This is made
possible with the implementation of namespaces and cgroups. <a class="reference internal" href="#www-jpablo" id="id411">[357]</a></p>
<p>Containers are light weighed ( As guest operating system
loading and booting is eleminated ) and more customizable
compared to VM technologies.The basis for docker developement
is also LXC. <a class="reference internal" href="#www-infoworld" id="id412">[358]</a>. Linux containers would
work on the major distributions of linux this would not work
on Microsoft Windows.</p>
</li>
<li><p class="first">Linux-Vserver</p>
</li>
<li><p class="first">OpenStack</p>
</li>
<li><p class="first">OpenNebula</p>
</li>
<li><p class="first">Eucalyptus</p>
</li>
<li><p class="first">Nimbus</p>
<p>Nimbus Infrastructure <a class="reference internal" href="#www-nimbus-wiki" id="id413">[359]</a> is an open source
IaaS implementation. It allows deployment of self-configured
virtual clusters and it supports configuration of scheduling,
networking leases, and usage metering.</p>
<p>Nimbus Platform <a class="reference internal" href="#www-nimbus" id="id414">[360]</a> provides an integrated set of
tools which enable users to launch large virtual clusters as well
as launch and monitor the cloud apps. It also includes service
that provides auto-scaling and high availability of resources
deployed over multiple IaaS cloud.  The Nimubs Platform tools are
cloudinit.d, Phantom and Context Broker.  In this paper
<a class="reference internal" href="#nimbus-paper" id="id415">[361]</a>, the use of Nimbus Phantom
to deploy auto-scaling solution across multiple NSF FutureGrid
clouds is explained. In this implementation Phantom was responsible
for deploying instances across multiple clouds and monitoring those
instance.  Nimbus platform supports Nimbus, Open Stack, Amazon
and several other clouds.</p>
</li>
<li><p class="first">CloudStack</p>
<p>Apache CloudStack is open source software designed to deploy and
manage large networks of virtual machines, as a highly available,
highly scalable Infrastructure as a Service (IaaS) cloud
computing platform. It uses existing hypervisors such as KVM,
VMware vSphere, and XenServer/XCP for virtualization. In addition
to its own API, CloudStack also supports the Amazon Web Services
(AWS) API and the Open Cloud Computing Interface from the Open
Grid Forum. <a class="reference internal" href="#www-cloudstack" id="id416">[362]</a></p>
<p>ColudStack features like built-in high-availability for hosts
and VMs, AJAX web GUI for management, AWS API compatibility,
Hypervisor agnostic, snapshot management, usage metering, network
management (VLAN&#8217;s, security groups), virtual routers, firewalls,
load balancers and multi-role support. <a class="reference internal" href="#www-cloudstack2" id="id417">[363]</a></p>
</li>
<li><p class="first">CoreOS</p>
<p><a class="reference internal" href="#www-core" id="id418">[364]</a> states that CoreOS is a linux operating system
used for clustered deployments. CoreOS allows applications to
run on containers. CoreOS can be run on clouds, virtual or
physical servers. CoreOS allows the ability for automatic software
updates inorder to make sure containers in cluster are secure and
reliable. It also makes managing large cluster environements
easier. CoreOS provides open source tools like CoreOS Linux,
etcd,rkt and flannel. CoreOS also has commercial products
Kubernetes and CoreOS stack. Core OS. In CoreOS linux service
discovery is achieved by etcd, applications are run on Docker and
process management is achieved by fleet.</p>
</li>
<li><p class="first">rkt</p>
<p>rkt is an container manager developed by CoreOS <a class="reference internal" href="#www-coreos" id="id419">[365]</a>
designed for Linux clusters. It is an alternative for Docker
runtime and is designed for server environments with high
security and composibity requirement. It is the first
implementation of the open container standard called
&#8220;App Container&#8221; or &#8220;appc&#8221; specification but not the only one.
It is a standalone tool that lives outside of the core operating
system and can be used on variety of platforms such as Ubuntu,
RHEL, CentOS, etc. rkt implements the facilities specified by
the App Container as a command line tool. It allows execution
of App Containers with pluggable isolation and also varying
degrees of protection. Unlike Docker, rkt runs containers as
un-priviliged users making it impossible for attackers to break
out of the containers and take control of the entire physical
server. rkt&#8217;s primary interface comprises a single executable
allowing it easily integrate with existing init systems and
also advanced cluster environments. rkt is open source and is
written in the Go programming language <a class="reference internal" href="#www-github-rkt" id="id420">[366]</a>.</p>
</li>
<li><p class="first">VMware ESXi</p>
<p>VMware ESXi (formerly ESX) is an enterprise-class, type-1
hypervisor developed by VMware for deploying and serving virtual
computers <a class="reference internal" href="#wiki-vmwareesxi" id="id421">[367]</a>. The name ESX originated as an
abbreviation of Elastic Sky X. ESXi installs directly onto your
physical server enabling it to be partitioned into multiple
logical servers referred to as virtual machines.  Management of
VMware ESXi is done via APIs. This allows for an agent-less
approach to hardware monitoring and system management. VMware
also provides remote command lines, such as the vSphere Command
Line Interface (vCLI) and PowerCLI, to provide command and
scripting capabilities in a more controlled manner. These remote
command line sets include a variety of commands for
configuration, diagnostics and troubleshooting. For low-level
diagnostics and the initial configuration, menu-driven and
command line interfaces are available on the local console of the
server <a class="reference internal" href="#vmware-esxi" id="id422">[368]</a>.</p>
</li>
<li><p class="first">vSphere and vCloud</p>
</li>
<li><p class="first">Amazon</p>
<p>Amazons AWS (Amazon Web Services) is a provider of Infrastructure
as a Service (IaaS) on cloud. It provides a broad set of infrastructure
services, such as computing, data storage, networking and databases.
One can leverage AWS services by creating an account with AWS and then
creating a virtual server, called as an instance, on the AWS cloud.
In this instance you can select the hard disk volume, number of CPUs
and other hardware configuration based on your application needs.
You can also select operating system and other software required
to run your application. AWS lets you select from the countless services.
Some of them are mentioned below:</p>
<ul class="simple">
<li>Amazon Elastic Computer Cloud (EC2)</li>
<li>Amazon Simple Storage Service (Amazon S3)</li>
<li>Amazon CloudFront</li>
<li>Amazon Relational Database Service (Amazon RDS)</li>
<li>Amazon SimpleDB</li>
<li>Amazon Simple Notification Service (Amazon SNS)</li>
<li>Amazon Simple Queue Service (Amazon SQS)</li>
<li>Amazon Virtual Private Cloud (Amazon VPC)</li>
</ul>
<p>Amazon EC2 and Amazon S3 are the two core IaaS services, which are
used by cloud application solution developers worldwide. :cite:&#8217;www-aws&#8217;</p>
<p><strong>Improve: all of them need bibentries</strong></p>
</li>
<li><p class="first">Azure</p>
</li>
<li><p class="first">Google and other public Clouds</p>
</li>
<li><p class="first">Networking: Google Cloud DNS</p>
</li>
<li><p class="first">Amazon Route 53</p>
</li>
</ol>
</div>
<div class="section" id="cross-cutting-functions">
<h2>Cross-Cutting Functions<a class="headerlink" href="#cross-cutting-functions" title="Permalink to this headline"></a></h2>
<div class="section" id="monitoring">
<h3>Monitoring<a class="headerlink" href="#monitoring" title="Permalink to this headline"></a></h3>
<ol class="arabic" start="361">
<li><p class="first">Ambari</p>
<p>Apache Amabari is an open source platform that enables easy
management and maintenance of Hadoop clusters, regardless of
cluster size. Ambari has a simplified Web UI and robust REST API
for automating and controlling cluster operations.
<a class="reference internal" href="#www-hortonworks-ambari" id="id423">[369]</a> illustrates Ambari to provide key
benefits including easy installation, configuration, and
management with features such as Smart Configs and cluster
recommendations and Ambari Blueprints, to provide repeatable and
automated cluster creation. Ambari provides a centralized
security setup that automates security capabilities of
clusters. Ambari provides a holistic view for cluster monitoring
and provides visualizations for operation
metrics. <a class="reference internal" href="#www-ambari" id="id424">[370]</a> provides documentation about Ambari,
including a quick start guide for installing a cluster with
Ambari. <a class="reference internal" href="#www-github-ambari" id="id425">[371]</a> provides the project documents
for ambari on github.</p>
</li>
<li><p class="first">Ganglia</p>
</li>
<li><p class="first">Nagios <a class="reference internal" href="#www-nagios" id="id426">[372]</a></p>
<p>Nagios is a platform, which provides a set of software for
network infrastructure monitoring. It also offers administrative
tools to diagnose when failure events happen, and to notify
operators when hardware issues are detected. Specifically,
illustrates that Nagios is consist of modules including
<a class="reference internal" href="#nagios-book" id="id427">[373]</a>: a core and its dedicated tool for core
configuration, extensible plugins and its frontend. Nagios core
is designed with scalability in mind.  Nagios contains a
specification language allowing for building an extensible
monitoring systems.  Through the Nagios API components can
integrate with the Nagios core services. Plugins can be developed
via static languages like C or script languages. This mechanism
empowers Nagios to monitor a large set of various scenarios yet
being very flexible. <a class="reference internal" href="#nagios-paper-2012" id="id428">[374]</a> Besides its open
source components, Nagios also has commercial products to serve
needing clients.</p>
</li>
<li><p class="first">Inca</p>
<p>Inca is a grid monitoring <a class="reference internal" href="#inca-book" id="id429">[375]</a> software suite. It
provides grid monitoring features. These monitoring features
provide operators failure trends, debugging support, email
notifications, environmental issues etc. <a class="reference internal" href="#www-inca" id="id430">[376]</a>. It
enables users to automate the tests which can be executed on a
periodic basis. Tests can be added and configured as and when
needed. It helps users with different portfolios like system
administrators, grid operators, end users etc Inca provides
user-level grid monitoring. For each user it stores results as
well as allows users to deploy new tests as well as share the
results with other users. The incat web ui allows users to view
the status of test, manage test and results. The architectural
blocks of inca include report repository, agent, data consumers
and depot. Reporter is an executable program which is used to
collect the data from grid source. Reporters can be written in
perl and python. Inca repository is a collection of pre build
reporters.  These can be accessed using a web url. Inca
repository has 150+ reporters available. Reporters are versioned
and allow automatic updates. Inca agent does the configuration
management. Agent can be managed using the incat web ui. Inca
depot provides storage and archival of reports. Depot uses
relational database for this purpose. The database is accessed
using hibernate backend.  Inca web UI or incat provides real time
as well as historical view of inca data.  All communication
between inca components is secured using SSL certificates. It
requires user credentials for any access to the
system. Credentials are created at the time of the setup and
installation. Inca&#8217;s performance has been phenomenal in
production deployments. Some of the deployments are running for
more than a decade and has been very stable. Overall Inca
provides a solid monitoring system which not only monitors but
also detects problems very early on.</p>
</li>
</ol>
</div>
<div class="section" id="security-privacy">
<h3>Security &amp; Privacy<a class="headerlink" href="#security-privacy" title="Permalink to this headline"></a></h3>
<ol class="arabic" start="365">
<li><p class="first">InCommon</p>
</li>
<li><p class="first">Eduroam <a class="reference internal" href="#www-eduroam" id="id431">[377]</a></p>
<p>Eduroam is an initiative started in the year 2003 when the number
of personal computers with in the academia are growing
rapidly. The goal is to solve the problem of secure access to
WI-FI due to increasing number of students and reasearch teams
becoming mobile which was increasing the administrative problems
for provide access to WI-FI. Eduroam provides any user from an
eduroam participating site to get network access at any
instituion connected through eduroam. According to the
orgnizatioin it uses a combination of radius-based infrastructuor
with 802.1X standard techonology to provide roaming acess across
reasearch and educational networks. The role of the RADIUS
hierarchy is to forward user crednetials to the users home
instituion where they can be verified. This proved to be a
successful solution when compared to other traditonal ways like
using MAC-adress, SSID, WEP, 802.1x(EAP-TLS, EAP-TTLS), VPN
Clients, Mobile-IP etc which have their own short comings when
used for this purpose <a class="reference internal" href="#eduroam-paper-2005" id="id432">[378]</a>. Today by
enabling eduroam users get access to internet across 70 countries
and tens of thousands of access points worldwide.</p>
</li>
<li><p class="first">OpenStack Keystone</p>
<p><a class="reference internal" href="#www-keystone-wiki" id="id433">[379]</a> Keystone is the identity service used by
OpenStack for authentication (authN) and high-level authorization (authZ).
There are two authentication mechanisms in Keystone, UUID token, and PKI.
Universally unique identifier (UUID) is a 128-bit number used to identify
information (user). Each application&nbsp;after&nbsp;each request&nbsp;of the client
checks token validity online. PKI was introduced later and improved the
security of Keystone <a class="reference internal" href="#cui2015security" id="id434">[380]</a>. In PKI, each token has its
own&nbsp;digital signature&nbsp;that can be checked by any service and OpenStack
application with no necessity to ask for Keystone database <a class="reference internal" href="#www-cloudberrylab-kstn" id="id435">[381]</a>.</p>
<p>Thus, Keystone enables ensuring users identity with no need to transmit
its password to applications. It has recently been rearchitected to allow
for expansion to support proxying external services and AuthN/AuthZ
mechanisms such as oAuth, SAML and openID in future versions <a class="reference internal" href="#www-keystone" id="id436">[382]</a>.</p>
</li>
<li><p class="first">LDAP</p>
<p>LDAP stands for Lightweight Directory Access Protocol. It is a software
protocol for enabling anyone to locate organizations, individuals, and
other resources such as files and devices in a network, whether on the
Internet or on corporate internet.
<a class="reference internal" href="#www-ldap" id="id437">[383]</a></p>
<p>LDAP is a &#8220;lightweight&#8221; (smaller amount of code) version of
Directory Access Protocol (DAP), which is part of X.500, a
standard for directory services in a network.  In a network, a
directory tells you where in the network something is located. On
TCP/IP networks (including the Internet), the domain name system
(DNS) is the directory system used to relate the domain name to a
specific network address (a unique location on the
network). However, you may not know the domain name. LDAP allows
you to search for an individual without knowing where they&#8217;re
located (although additional information will help with the
search).An LDAP directory can be distributed among many
servers. Each server can have a replicated version of the total
directory that is synchronized periodically.  An LDAP server is
called a Directory System Agent (DSA). An LDAP server that
receives a request from a user takes responsibility for the
request, passing it to other DSAs as necessary, but ensuring a
single coordinated response for the user.</p>
</li>
<li><p class="first">Sentry</p>
<p><a class="reference internal" href="#www-sentry" id="id438">[384]</a> &#8220;Apache Sentry is a granular, role-based authorization
module for Hadoop. Sentry provides the ability to control and enforce
precise levels of privileges on data for authenticated users and
applications on a Hadoop cluster. Sentry currently works out of the box
with Apache Hive, Hive Metastore/HCatalog, Apache Solr, Impala and HDFS
(limited to Hive table data). Sentry is designed to be a pluggable
authorization engine for Hadoop components. It allows the client to define
authorization rules to validate a user or applications access requests
for Hadoop resources. Sentry is highly modular and can support authorization
for a wide variety of data models in Hadoop.&#8221;</p>
</li>
<li><p class="first">Sqrrl</p>
</li>
<li><p class="first">OpenID</p>
<p>OpenID is an authentication protocol that allows users to log in
to different websites, which are not related, using the same
login credentials for each, i.e. without having to create
separate id and password for all the websites. The login
credentials used are of the existing account. The password is
known only to the identity provider and nobody else which
relieves the users concern about identity being known to an
insecure website. <a class="reference internal" href="#ope1" id="id439">[385]</a> It provides a mechanism that makes
the users control the information that can be shared among
multiple websites. OpenID is being adopted all over the web. Most
of the leading organizations including Microsoft, Facebook,
Google, etc. are accepting the OpenIDs <a class="reference internal" href="#ope2" id="id440">[386]</a>. It is an
open source and not owned by anyone. Anyone can use OpenID or be
an OpenID provider and there is no need for an individual to be
approved.</p>
</li>
<li><p class="first">SAML OAuth</p>
<p>As explained in <a class="reference internal" href="#saml" id="id441">[387]</a>, Security Assertion Markup Language
(SAML) is a secured XML based communication mechanism for
communicating identities between organizations. The primary use
case of SAML is Internet SSO. It eliminates the need to maintain
multiple authentication credentials in multiple locations. This
enhances security by elimination opportunities for identity
theft/Phishing. It increases application access by eliminating
barriers to usage. It reduces administration time and cost by
excluding the effort to maintain duplicate credentials and
helpdesk calls to reset forgotten passwords. Three entities of
SAML are the users, Identity Provider (IdP-Organization that
maintains a directory of users and an authentication mechanism)
and Service Provider(SP-Hosts the application /service). User
tries to access the application by clicking on a link or through
an URL on the internet. The Federated identity software running
in the IdP validates the users identity and the user is then
authenticated. A specifically formatted message is then
communicated to the federated identity software running at SP. SP
creates a session for the user in the target application and
allows the user to get direct access once it receives the
authorization message from a known identity provider.</p>
</li>
</ol>
</div>
<div class="section" id="distributed-coordination">
<h3>Distributed Coordination<a class="headerlink" href="#distributed-coordination" title="Permalink to this headline"></a></h3>
<ol class="arabic" start="373">
<li><p class="first">Google Chubby</p>
<p>Chubby Distributed lock service <a class="reference internal" href="#www-chubby" id="id442">[388]</a>
is intended for use within a loosely-coupled distributed system
consisting of moderately large numbers of small machines
connected by a high-speed network. Asynchronous consensus is
solved by the Paxos protocol. The implementation in Chubby is
based on coarse grained lock server and a library that the client
applications link against.  As per the 2016 paper
<a class="reference internal" href="#chubby-paper-2016" id="id443">[389]</a>, an open-source implementation of the
Google Chubby lock service was provided by the Apache ZooKeeper
project. ZooKeeper used a Paxos-variant protocol Zab for solving
the distributed consensus problem.  Google stack and Facebook
stack both use versions of zookeeper.</p>
</li>
<li><p class="first">Zookeeper</p>
<p>Zookeeper provides coordination services to distributed applications.
It includes synchronization, configuration management and naming
services among others. The interfaces are available in Java and C
<a class="reference internal" href="#www-zoo-overiew" id="id444">[390]</a>. The services themselves can be distributed
across multiple Zookeeper servers to avoid single point of failure.
If the leader fails to answer, the clients can fall-back to other
nodes. The state of the cluster is maintained in an in-memory image
along with a persistent storage file called znode by each server. The
cluster namespace is maintained in a hierarchical order. The changes to the
data are totally ordered <a class="reference internal" href="#www-zoo-wiki" id="id445">[391]</a> by stamping each update
with a number. Clients can also set a watch on a znode to be notified
of any change <a class="reference internal" href="#www-zoo-ibm" id="id446">[392]</a>. The performance of the ZooKeeper
is optimum for &#8220;read-dominant&#8221; workloads. It&#8217;s maintained by Apache
and is open-source.</p>
</li>
<li><p class="first">Giraffe</p>
<p>Giraffe is a scalable distributed coordination
service. Distributed coordination is a media access technique
used in distributed systems to perform functions like providing
group membership, gaining lock over resources, publishing,
subscribing, granting ownership and synchronization together
among multiple servers without issues. Giraffe was proposed as
alternative to coordinating services like Zookeeper and Chubby
which were efficient only in read-intensive scenario and small
ensembles. To overcome this three important aspects were included
in the design of Giraffe <a class="reference internal" href="#giraffepaper" id="id447">[393]</a>. First feature is
Giraffe uses interior-node joint trees to organize coordination
servers for better scalability. Second, Giraffe uses Paxos
protocol for better consistency and to provide more
fault-tolerance. Finally, Giraffe also facilitates hierarchical
data organization and in-memory storage for high throughput and
low latency.</p>
</li>
<li><p class="first">JGroups</p>
</li>
</ol>
</div>
<div class="section" id="message-and-data-protocols">
<h3>Message and Data Protocols<a class="headerlink" href="#message-and-data-protocols" title="Permalink to this headline"></a></h3>
<ol class="arabic" start="377">
<li><p class="first">Avro</p>
</li>
<li><p class="first">Thrift</p>
</li>
<li><p class="first">Protobuf</p>
<p>Protocol Buffer <a class="reference internal" href="#www-protobuf" id="id448">[394]</a> is a way to serialize
structured data into binary form (stream of bytes) in order to
transfer it over wires or for storage. It is used for inter
apllication communication or for remote procedure call (RPC). It
involves a interface description that describes the structure of
some data and a program that can generate source code or parse it
back to the binary form. It emphasizes on simplicity and
performance over xml. Though xml is more readable but requires
more resources in parsing and storing.  This is developed by
Google and available under open source licensing. The parser
program is available in many languages including java and python.</p>
</li>
</ol>
</div>
</div>
<div class="section" id="new-technologies-to-be-integrated">
<h2>New Technologies to be integrated<a class="headerlink" href="#new-technologies-to-be-integrated" title="Permalink to this headline"></a></h2>
<ol class="arabic simple" start="382">
<li>TBD</li>
</ol>
</div>
<div class="section" id="excersise">
<span id="techs-exercise"></span><h2>Excersise<a class="headerlink" href="#excersise" title="Permalink to this headline"></a></h2>
<dl class="docutils">
<dt>TechList.1: In class you will be given an HID and you will be assigned</dt>
<dd><p class="first">a number of technologies that you need to research and create a
summary as well as one or more relevant references to be added to the
Web page. All technologies for TechList.1 are marked with a (1)
behind the technology.  An example text is given for Nagios in this
page.  Please create a pull request with your responses. You are
responsible for making sure the request shows up and each commit is
using gitchangelog in the commit message:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">new</span><span class="p">:</span><span class="n">usr</span><span class="p">:</span> <span class="n">added</span> <span class="n">paragraph</span> <span class="n">about</span> <span class="o">&lt;</span><span class="n">PUTTECHHERE</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>You can create one or more pull requests for the technology and the
references. We have created in the referens file a placeholder using
your HID to simplify the management of the references while avoiding
conflicts.  For the technologies you are responsible to invesitgate
them and write an academic summary of the technology. Make sure to
add your reference to refs.bib.  Many technologies may have
additional references than the Web page. Please add the most
important once while limiting it to three if you can. Avoid
plagearism and use proper quotations or better rewrite the text.</p>
<p>You must look at <a class="reference internal" href="technologies-hw.html"><span class="doc">Completing Techlist Assignments</span></a> to sucessfully complete the
homework</p>
<p>A video about this hoemwork is posted at
<a class="reference external" href="https://www.youtube.com/watch?v=roi7vezNmfo">https://www.youtube.com/watch?v=roi7vezNmfo</a> showing how to
do references in emacs and jabref, it shows you how to configure
git, it shows you how to do the fork request while asking you to add
&#8220;new:usr ....&#8221; to the commit messages). As this is a homework
realated video we put a lot of information in it that is not only
useful for beginners. We recommend you watch it.</p>
<p>This homework can be done in steps. First you can collect all the
content in an editor. Second you can create a fork. Third you can
add the new content to the fork. Fourth you can commit. Fith you
can push. Six if the TAs have commend improve. The commit message
must have new:usr: at the beginning.</p>
<p>While the Nagios entry is a good example (make sure grammer is ok
the Google app engine is an example for a bad entry.</p>
<p class="last">Do Techlist 1.a 1.b 1.c first. We  will assign Techlist 1.d and
TechList 2 in February.</p>
</dd>
<dt>TechList.1.a:</dt>
<dd>Complete the pull request with the technologies assigned to you.
Details for the assignment are posted in Piazza. Search for TechList.</dd>
<dt>TechList.1.b: Identify how to cite. We are using &#8220;scientific&#8221; citation</dt>
<dd>formats such as IEEEtran, and ACM. We are <strong>not</strong> using citation
formats such as Chicago, MLA, or ALP. The later are all for non
scientific publications and thus of no use to us. Also when writing
about a technology do not use the names of the person, simply say
something like. In [1] the definition of a turing machine is given
as follows, ...  and do not use elaborate sentences such as: In his
groundbraking work conducted in England, Allan Turing, introduced
the turing machine in the years 1936-37 [2]. Its definition is base
on ... The difference is clear, while the first focusses on results
and technological concepts, the second introduces a colorful
description that is more suitable for a magazine or a computer
history paper.</dd>
<dt>TechList 1.c:</dt>
<dd>Learn about Plagearism and how to avoid it.
Many Web pages will conduct self advertisement while adding
suspicious and subjective adjectives or phrases such as cheaper,
superior, best, most important, with no equal, and others that you
may not want to copy into your descriptions. Please focus on facts
not on what the author of the Web page claims.</dd>
<dt>TechList 1.d:</dt>
<dd>Identify technologies from the Apache project or other
Big Data related Web pages and projects that are not yet listed here
and add the name and descriptions as well as references and that you
find important.</dd>
<dt>TechList.2:</dt>
<dd>In this hopweork we provide you with additional technologies
that you need to compleate They are marked with (2) in the HID
assignment.</dd>
<dt>TechList.3:</dt>
<dd>Identify technologies that are not listed here and add
them. Provide a description and a refrence just as you did before.
Make sure duplicated entries will be merged. Before you start doing a
technology to avoid adding technologies that have already been done by
others.</dd>
</dl>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<p id="bibtex-bibliography-i524/technologies-0"><table class="docutils citation" frame="void" id="www-bpel-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Bpel wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Business_Process_Execution_Language">https://en.wikipedia.org/wiki/Business_Process_Execution_Language</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ode-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[2]</a></td><td>Ode wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Apache_ODE">https://en.wikipedia.org/wiki/Apache_ODE</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ode-web" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[3]</a></td><td>Ode website. Web Page. URL: <a class="reference external" href="http://ode.apache.org/">http://ode.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-pegasus" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[4]</a></td><td>Pegasus. Web Page. Accessed:2/3/2017. URL: <a class="reference external" href="https://pegasus.isi.edu/">https://pegasus.isi.edu/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-taverna" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[5]</a></td><td>Taverna. Web Page. URL: <a class="reference external" href="https://taverna.incubator.apache.org/introduction/">https://taverna.incubator.apache.org/introduction/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="taverna-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[6]</a></td><td><em>Taverna Workflows: Syntax and Semantics</em>, IEEE, December 2007. URL: <a class="reference external" href="http://ieeexplore.ieee.org/document/4426917/">http://ieeexplore.ieee.org/document/4426917/</a>, <a class="reference external" href="http://dx.doi.org/10.1109/E-SCIENCE.2007.71">doi:10.1109/E-SCIENCE.2007.71</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-trident-tutorial" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[7]</td><td><em>(<a class="fn-backref" href="#id7">1</a>, <a class="fn-backref" href="#id8">2</a>, <a class="fn-backref" href="#id10">3</a>)</em> Trident tutorial. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://storm.apache.org/releases/0.10.1/Trident-tutorial.html">http://storm.apache.org/releases/0.10.1/Trident-tutorial.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-trident-overview" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id9">[8]</a></td><td>Trident api overview. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://storm.apache.org/releases/1.0.0/Trident-API-Overview.html">http://storm.apache.org/releases/1.0.0/Trident-API-Overview.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-biokepler" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id11">[9]</a></td><td>What is biokepler. WebPage. URL: <a class="reference external" href="http://www.biokepler.org/faq#what-is-biokepler">http://www.biokepler.org/faq#what-is-biokepler</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-biokepler-demos" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id12">[10]</a></td><td>Demo workflow. WebPage. URL: <a class="reference external" href="http://www.biokepler.org/userguide#demos">http://www.biokepler.org/userguide#demos</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="bioactors" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id13">[11]</a></td><td>Weizhong Li, editor. <em>Introduction to bioActors</em>, number&nbsp;1st in 1st Workshop on bioKepler Tools and Its Applications, UCSD;SDSC, September 2012. URL: <a class="reference external" href="http://www.biokepler.org/sites/swat.sdsc.edu.biokepler/files/workshops/2012-09-05/slides/2012-09-05-02-Li.pdf">http://www.biokepler.org/sites/swat.sdsc.edu.biokepler/files/workshops/2012-09-05/slides/2012-09-05-02-Li.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-galaxy-ansible" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id14">[12]</a></td><td>About ansible galaxy. Web Page. Accessed: 2017-2-06. URL: <a class="reference external" href="https://galaxy.ansible.com/intro/">https://galaxy.ansible.com/intro/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ansible-book-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id15">[13]</a></td><td>Michael Heap. <em>Ansible From Beginner to Pro</em>. apress, 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-github-galaxy" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id16">[14]</a></td><td>GitHub ansible/ galaxy. Web Page. Accessed: 2017-2-06. URL: <a class="reference external" href="https://github.com/ansible/galaxy/">https://github.com/ansible/galaxy/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cascading" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id17">[15]</a></td><td>Cascading. Web Page, 2017. URL: <a class="reference external" href="http://www.cascading.org/projects/cascading/">http://www.cascading.org/projects/cascading/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="e-science-central-paper-2010" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[16]</td><td><em>(<a class="fn-backref" href="#id18">1</a>, <a class="fn-backref" href="#id19">2</a>, <a class="fn-backref" href="#id21">3</a>, <a class="fn-backref" href="#id22">4</a>)</em> Hugo Hiden, Paul Watson, Simon Woodman, and David Leahy. E-science central: cloud-based e-science and its application to chemical property modelling. In <em>University of Newcastle upon Tyne, Computing Science, Technical Report Series, No. CS-TR-1227</em>. University of Newcastle upon Tyne, November 2010.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-e-science-central" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id20">[17]</a></td><td>Escience central overview. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://bitbucket.org/digitalinstitute/esciencecentral/">https://bitbucket.org/digitalinstitute/esciencecentral/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="azure-df" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id23">[18]</a></td><td>Archive. Azure_df. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="http://www.jamesserra.com/archive/2014/11/what-is-azure-data-factory/">http://www.jamesserra.com/archive/2014/11/what-is-azure-data-factory/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="azure-ms" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id24">[19]</a></td><td>Microsoft. Azure_ms. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/data-factory/data-factory-introduction">https://docs.microsoft.com/en-us/azure/data-factory/data-factory-introduction</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-dataflow" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id25">[20]</a></td><td>Cloud dataflow. WebPage. URL: <a class="reference external" href="https://cloud.google.com/dataflow/">https://cloud.google.com/dataflow/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-googlelivestream" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id26">[21]</a></td><td>Jacob Jackson. Google service analyzes live streaming data. WebPage, June 2014. URL: <a class="reference external" href="http://www.infoworld.com/article/2607938/data-mining/google-service-analyzes-live-streaming-data.html">http://www.infoworld.com/article/2607938/data-mining/google-service-analyzes-live-streaming-data.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-nifi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id27">[22]</a></td><td>Apachi nifi. Web Page. URL: <a class="reference external" href="https://nifi.apache.org">https://nifi.apache.org</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hortanworks" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id28">[23]</a></td><td>Apache nifi (aka hdf) data flow across data center. Web Page. URL: <a class="reference external" href="https://community.hortonworks.com/articles/9933/apache-nifi-aka-hdf-data-flow-across-data-center.html">https://community.hortonworks.com/articles/9933/apache-nifi-aka-hdf-data-flow-across-data-center.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-forbes" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id29">[24]</a></td><td>Nsa &#8216;nifi&#8217; big data automation project out in the open. Web Page. URL: <a class="reference external" href="http://www.forbes.com/sites/adrianbridgwater/2015/07/21/nsa-nifi-big-data-automation-project-out-in-the-open/#6b37cac55d9a">http://www.forbes.com/sites/adrianbridgwater/2015/07/21/nsa-nifi-big-data-automation-project-out-in-the-open/#6b37cac55d9a</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pent1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id30">[25]</a></td><td>Pentaho. webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Pentaho">https://en.wikipedia.org/wiki/Pentaho</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pent2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id31">[26]</a></td><td>Any analytics, any data, simplified. webpage. URL: <a class="reference external" href="http://www.pentaho.com/product/product-overview">http://www.pentaho.com/product/product-overview</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mahout" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id32">[27]</a></td><td>Apache mahout. Web Page. URL: <a class="reference external" href="http://mahout.apache.org/">http://mahout.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-datafu" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id33">[28]</a></td><td>Datafu. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="https://datafu.incubator.apache.org/">https://datafu.incubator.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-r" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id34">[29]</a></td><td>R: what is r? Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://www.r-project.org/about.html">https://www.r-project.org/about.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-r" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id35">[30]</a></td><td>Vignesh Prajapati. <em>Big data analytics with R and Hadoop</em>. Packt Publishing, 2013. ISBN 9781782163282.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-pbdr" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id36">[31]</a></td><td>G.&nbsp;Ostrouchov, W.-C. Chen, D.&nbsp;Schmidt, and P.&nbsp;Patel. Web Page, 2012. URL: <a class="reference external" href="http://r-pbd.org/">http://r-pbd.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="bioconductor-article-2004" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id37">[32]</a></td><td>Robert&nbsp;C. Gentleman, Vincent&nbsp;J. Carey, Douglas&nbsp;M. Bates, Ben Bolstad, Marcel Dettling, Sandrine Dudoit, Byron Ellis, Laurent Gautier, Yongchao Ge, Jeff Gentry, Kurt Hornik, Torsten Hothorn, Wolfgang Huber, Stefano Iacus, Rafael Irizarry, Friedrich Leisch, Cheng Li, Martin Maechler, Anthony&nbsp;J. Rossini, Gunther Sawitzki, Colin Smith, Gordon Smyth, Luke Tierney, Jean&nbsp;YH Yang, and Jianhua Zhang. Bioconductor: open software development for computational biology and bioinformatics. <em>Genome Biology</em>, 5(10):R80, 2004. URL: <a class="reference external" href="http://dx.doi.org/10.1186/gb-2004-5-10-r80">http://dx.doi.org/10.1186/gb-2004-5-10-r80</a>, <a class="reference external" href="http://dx.doi.org/10.1186/gb-2004-5-10-r80">doi:10.1186/gb-2004-5-10-r80</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bioconductor-about" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id38">[33]</a></td><td>About bioconductor. Web Page. Accessed: 2017-02-10. URL: <a class="reference external" href="https://www.bioconductor.org/about/">https://www.bioconductor.org/about/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-opencv" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id39">[34]</a></td><td>web page. URL: <a class="reference external" href="http://opencv.org/">http://opencv.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="opencv-version" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id40">[35]</a></td><td>web page. URL: <a class="reference external" href="http://opencv.org/opencv-3-2.html">http://opencv.org/opencv-3-2.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id41">[36]</a></td><td>Alfredo Buttari, Julien Langou, Jakub Kurzak, and Jack Dongarra. A class of parallel tiled linear algebra algorithms for multicore architectures. <em>Parallel Computing</em>, 35(1):3853, 2009. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0167819108001117">http://www.sciencedirect.com/science/article/pii/S0167819108001117</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-plasma-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id42">[37]</a></td><td>PLASMA README. Web Page. URL: <a class="reference external" href="http://icl.cs.utk.edu/projectsfiles/plasma/html/README.html">http://icl.cs.utk.edu/projectsfiles/plasma/html/README.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id43">[38]</a></td><td>Jack Dongarra, Mark Gates, Azzam Haidar, Jakub Kurzak, Piotr Luszczek, Stanimire Tomov, and Ichitaro Yamazaki. Accelerating numerical dense linear algebra calculations with GPUs. In <em>Numerical Computations with GPUs</em>, pages 328. Springer, 2014. URL: <a class="reference external" href="http://link.springer.com/chapter/10.1007/978-3-319-06548-9_1">http://link.springer.com/chapter/10.1007/978-3-319-06548-9_1</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id44">[39]</a></td><td>Azzam Haidar, Jakub Kurzak, and Piotr Luszczek. An improved parallel singular value algorithm and its implementation for multicore hardware. In <em>Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis</em>, 90. ACM, 2013. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?id=2503292">http://dl.acm.org/citation.cfm?id=2503292</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id45">[40]</a></td><td>Jakub Kurzak, Hatem Ltaief, Jack Dongarra, and Rosa&nbsp;M. Badia. Scheduling dense linear algebra operations on multicore processors. <em>Concurrency and Computation: Practice and Experience</em>, 22(1):1544, 2010. URL: <a class="reference external" href="http://onlinelibrary.wiley.com/doi/10.1002/cpe.1467/full">http://onlinelibrary.wiley.com/doi/10.1002/cpe.1467/full</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id46">[41]</a></td><td>Stanimire Tomov, Jack Dongarra, and Marc Baboulin. Towards dense linear algebra for hybrid GPU accelerated manycore systems. <em>Parallel Computing</em>, 36(5):232240, 2010. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0167819109001276">http://www.sciencedirect.com/science/article/pii/S0167819109001276</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-plasma-magma-6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id47">[42]</a></td><td>Stanimire Tomov, Rajib Nath, Hatem Ltaief, and Jack Dongarra. Dense linear algebra solvers for multicore with GPU accelerators. In <em>Parallel &amp; Distributed Processing, Workshops and Phd Forum (IPDPSW), 2010 IEEE International Symposium on</em>, 18. IEEE, 2010. URL: <a class="reference external" href="http://ieeexplore.ieee.org/abstract/document/5470941/">http://ieeexplore.ieee.org/abstract/document/5470941/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azuremlsite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id48">[43]</a></td><td>Azure machine learning. Web Page. Accessed: 2017-1-28. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-what-is-machine-learning#what-is-machine-learning-in-the-microsoft-azure-cloud">https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-what-is-machine-learning#what-is-machine-learning-in-the-microsoft-azure-cloud</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-prediction" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id49">[44]</a></td><td>Google cloud prediction api documentation. WebPage. Accessed 2017-1-26. URL: <a class="reference external" href="https://cloud.google.com/prediction/docs/">https://cloud.google.com/prediction/docs/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-translation" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id50">[45]</a></td><td>Google cloud translation api documentation. WebPage. URL: <a class="reference external" href="https://cloud.google.com/translate/docs/">https://cloud.google.com/translate/docs/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="dblp-journals-corr-abs-1202-6548" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id51">[46]</a></td><td>Davide Albanese, Roberto Visintainer, Stefano Merler, Samantha Riccadonna, Giuseppe Jurman, and Cesare Furlanello. Mlpy: machine learning python. <em>CoRR</em>, 2012. URL: <a class="reference external" href="http://arxiv.org/abs/1202.6548">http://arxiv.org/abs/1202.6548</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mlpy" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id52">[47]</a></td><td>Mlpy site. Web Page. URL: <a class="reference external" href="http://mlpy.sourceforge.net/docs/3.5/">http://mlpy.sourceforge.net/docs/3.5/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="scik1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id53">[48]</a></td><td>Jason Brownlee. A gentle introduction to scikit-learn: a python machine learning library. webpage, 2014. URL: <a class="reference external" href="http://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/">http://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="scik2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id54">[49]</a></td><td>Scikit-learn. webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Scikit-learn">https://en.wikipedia.org/wiki/Scikit-learn</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="comp1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[50]</td><td><em>(<a class="fn-backref" href="#id55">1</a>, <a class="fn-backref" href="#id56">2</a>)</em> Rudi Cilibrasi, Anna&nbsp;Lissa Cruz, Steven de&nbsp;Rooij, and Maarten Keijzer. What is complearn. webpage. URL: <a class="reference external" href="http://complearn.org/">http://complearn.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-caffe" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id57">[51]</a></td><td>Caffe-deep learning. Accessed: 02-06-2017. URL: <a class="reference external" href="http://caffe.berkeleyvision.org/">http://caffe.berkeleyvision.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-torch" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id58">[52]</a></td><td>Torch-machine learning. Accessed: 02-06-2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Torch_(machine_learning)">https://en.wikipedia.org/wiki/Torch_(machine_learning)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-theano" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id59">[53]</a></td><td>Theano. Web Page. Accessed: 2017-1-21. URL: <a class="reference external" href="http://deeplearning.net/software/theano/introduction.html">http://deeplearning.net/software/theano/introduction.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-dl4j" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id60">[54]</a></td><td>DL4j. Webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Deeplearning4j">https://en.wikipedia.org/wiki/Deeplearning4j</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibmwatson-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id61">[55]</a></td><td>Ibm watson. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Watson_(computer)">https://en.wikipedia.org/wiki/Watson_(computer)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibmwatson" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[56]</td><td><em>(<a class="fn-backref" href="#id62">1</a>, <a class="fn-backref" href="#id63">2</a>)</em> Ibm watson product page. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.ibm.com/watson">https://www.ibm.com/watson</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-graphlab" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id64">[57]</a></td><td>Graphlab. Webpage. Accessed: 2017-01-28. URL: <a class="reference external" href="http://www.select.cs.cmu.edu/code/graphlab/">http://www.select.cs.cmu.edu/code/graphlab/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-graphx" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id65">[58]</a></td><td>GraphX. Web Page. Accessed: 2017-01-30. URL: <a class="reference external" href="http://spark.apache.org/graphx/">http://spark.apache.org/graphx/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-graphx1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id66">[59]</a></td><td>Reynold&nbsp;S. Xin, Joseph&nbsp;E. Gonzalez, Michael&nbsp;J. Franklin, and Ion Stoica. Graphx: a resilient distributed graph system on spark. In <em>First International Workshop on Graph Data Management Experiences and Systems</em>, GRADES &#8216;13, 2:12:6. New York, NY, USA, 2013. ACM. URL: <a class="reference external" href="http://doi.acm.org/10.1145/2484425.2484427">http://doi.acm.org/10.1145/2484425.2484427</a>, <a class="reference external" href="http://dx.doi.org/10.1145/2484425.2484427">doi:10.1145/2484425.2484427</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-news" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id67">[60]</a></td><td>Dylan Raithel. Apache tinkerpop graduates to top-level project. Web Page, 2016. URL: <a class="reference external" href="https://www.infoq.com/news/2016/06/tinkerpop-top-level-apache/">https://www.infoq.com/news/2016/06/tinkerpop-top-level-apache/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apachetinkerpophome" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id68">[61]</a></td><td>Apache Tinker Pop Home. Apache tinkerpop home. Web Page, 2016. URL: <a class="reference external" href="https://tinkerpop.apache.org/">https://tinkerpop.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="dream" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[62]</td><td><em>(<a class="fn-backref" href="#id69">1</a>, <a class="fn-backref" href="#id70">2</a>)</em> Overview. Online. The contact information listed on the webpage was for Yogesh Simmhan. URL: <a class="reference external" href="http://dream-lab.cds.iisc.ac.in/about/overview/">http://dream-lab.cds.iisc.ac.in/about/overview/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="rao" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[63]</td><td><em>(<a class="fn-backref" href="#id71">1</a>, <a class="fn-backref" href="#id72">2</a>)</em> Siddharth Rao. Dream:lab  democratising computing through the cloud. Online, March 2016. URL: <a class="reference external" href="http://iisc.researchmedia.center/article/dreamlab-">http://iisc.researchmedia.center/article/dreamlab-</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="denero" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[64]</td><td><em>(<a class="fn-backref" href="#id73">1</a>, <a class="fn-backref" href="#id74">2</a>)</em> John Denero. <em>CS61A: Online Textbook</em>. Berkeley, http://www-inst.eecs.berkeley.edu/~cs61a/sp12/book/, 2011. &#8220;This book is derived from the classic textbook Structure and Interpretation of Computer Programs by Abelson, Sussman, and Sussman. John Denero originally modified if for Python for the Fall 2011 semester.&#8221; http://www-inst.eecs.berkeley.edu/~cs61a/sp12/book/. URL: <a class="reference external" href="http://www-inst.eecs.berkeley.edu/~cs61a/sp12/book/communication.html">http://www-inst.eecs.berkeley.edu/~cs61a/sp12/book/communication.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fusiontablesupport" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id75">[65]</a></td><td>Google. Fusiontablesupporthelp. Web Page, 2017. URL: <a class="reference external" href="https://support.google.com/fusiontables/answer/171181?hl=en">https://support.google.com/fusiontables/answer/171181?hl=en</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wiki-fusiontable" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id76">[66]</a></td><td>Wikipedia. Fusion table support. Web Page, Last updated in 1 November 2016. URL: <a class="reference external" href="https://support.google.com/fusiontables/answer/171181?hl=en">https://support.google.com/fusiontables/answer/171181?hl=en</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="googlefusiontable2012" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id77">[67]</a></td><td>Google. Google fusion tables: data management, integration and collaboration in the cloud. 2012. URL: <a class="reference external" href="http://homes.cs.washington.edu/~alon/files/socc10.pdf">http://homes.cs.washington.edu/~alon/files/socc10.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-nwb-edu" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id78">[68]</a></td><td>Nwb. Web Page, 2017. URL: <a class="reference external" href="http://nwb.cns.iu.edu/about.html">http://nwb.cns.iu.edu/about.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-elasticsearch" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id79">[69]</a></td><td>Elastic search. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.elastic.co/products/elasticsearch">https://www.elastic.co/products/elasticsearch</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-elasticsearch-intro" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id80">[70]</a></td><td>Elastic search getting started. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="elasticsearch-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id81">[71]</a></td><td>Clinton Gormley and Zachary Tong. <em>Elasticsearch - The Definitive Guide : A Distributed REAL-TIME SEARCH AND ANALYTICS ENGINE</em>. O&#8217;Reilly Media,Inc, 1005 Gravenstein Highway North, Sebastopol, CA 95472, 1st edition, 2015. ISBN 9781449358549.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-elasticsearch-hadoop" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id82">[72]</a></td><td>Elastic search on hadoop. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.elastic.co/products/hadoop">https://www.elastic.co/products/hadoop</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikipedia-elasticsearch" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id83">[73]</a></td><td>Elastic Search. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Elasticsearch">https://en.wikipedia.org/wiki/Elasticsearch</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-logstash" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id84">[74]</a></td><td>Logstash. Webpage. URL: <a class="reference external" href="https://www.elastic.co/guide/en/logstash/current/introduction.html">https://www.elastic.co/guide/en/logstash/current/introduction.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tableau-tutorial" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id85">[75]</a></td><td>Tableau tutorial. Web Page. URL: <a class="reference external" href="https://casci.umd.edu/wp-content/uploads/2013/12/Tableau-Tutorial.pdf">https://casci.umd.edu/wp-content/uploads/2013/12/Tableau-Tutorial.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tableau-web" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id86">[76]</a></td><td>Tableau official website. Web Page. URL: <a class="reference external" href="https://www.tableau.com/products/technology">https://www.tableau.com/products/technology</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-dcjs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id87">[77]</a></td><td>Dc.js - dimensional charting javascript library. Web Page. accessed: 2017-01-21. URL: <a class="reference external" href="https://dc-js.github.io/dc.js/">https://dc-js.github.io/dc.js/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tensorflow-paper-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id88">[78]</a></td><td>M.&nbsp;Abadi, A.&nbsp;Agarwal, P.&nbsp;Barham, E.&nbsp;Brevdo, Z.&nbsp;Chen, C.&nbsp;Citroand&nbsp;GS. Corrado, A.&nbsp;Davis, J.&nbsp;Dean, M.&nbsp;Devin, S.&nbsp;Ghemawat, I.&nbsp;Goodfellow, A.&nbsp;Harp, G.&nbsp;Irving, M.&nbsp;Isard, Y.&nbsp;Jia, R.&nbsp;Jozefowicz, L.&nbsp;Kaiser, M.&nbsp;Kudlur, J.&nbsp;Levenberg, D.&nbsp;Mane, R.&nbsp;Monga, S.&nbsp;Moore, D.&nbsp;Murray, C.&nbsp;Olah, M.&nbsp;Schuster, J.&nbsp;Shlens, B.&nbsp;Steiner, I.&nbsp;Sutskever, K.&nbsp;Talwar, P.&nbsp;Tucker, V.&nbsp;Vanhoucke, V.&nbsp;Vasudevan, F.&nbsp;Viegas, O.&nbsp;Vinyals, P.&nbsp;Warden, M.&nbsp;Wattenberg, M.&nbsp;Wicke, Y.&nbsp;Yu, and X.&nbsp;Zheng. Tensorflow: large-scale machine learning on heterogeneous distributed systems. Technical Report, Cornell University, March 2016. Accessed: 2017-1-24.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tensorflow" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id89">[79]</a></td><td>TensorFlow. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://www.tensorflow.org">https://www.tensorflow.org</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-gae" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id90">[80]</a></td><td>Google app engine. Web Page. URL: <a class="reference external" href="https://cloud.google.com/appengine/">https://cloud.google.com/appengine/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-appscale" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id91">[81]</a></td><td>AppScale. What-is-appscale. Web Page, 2016. URL: <a class="reference external" href="https://www.appscale.com/community/what-is-appscale/">https://www.appscale.com/community/what-is-appscale/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-developers-openshift" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id94">[82]</a></td><td>developers-openshift components. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://developers.openshift.com/">https://developers.openshift.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openshift" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id95">[83]</a></td><td>openshift components. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://www.openshift.org/">https://www.openshift.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-heroku" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id97">[84]</a></td><td>Heroku. Web Page. Accessed:1/27/2017. URL: <a class="reference external" href="https://devcenter.heroku.com/">https://devcenter.heroku.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cedar" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id98">[85]</a></td><td>Cedar stack. Web Page. Accessed:1/27/2017. URL: <a class="reference external" href="https://devcenter.heroku.com/articles/stack#cedar">https://devcenter.heroku.com/articles/stack#cedar</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-aero" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id99">[86]</a></td><td>Aerobatic - overview. Web Page. accessed: 2017-01-25. URL: <a class="reference external" href="https://www.aerobatic.com/docs/overview/">https://www.aerobatic.com/docs/overview/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikipedia-cloud" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id100">[87]</a></td><td>wikipedia.org. Web Page. accessed 2017-01-21. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Cloud_computing">https://en.wikipedia.org/wiki/Cloud_computing</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azure-msft" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[88]</td><td><em>(<a class="fn-backref" href="#id101">1</a>, <a class="fn-backref" href="#id102">2</a>, <a class="fn-backref" href="#id108">3</a>)</em> Microsoft Corp. Web Page. accessed 2017-01-21. URL: <a class="reference external" href="https://azure.microsoft.com/en-us/">https://azure.microsoft.com/en-us/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sec-edgar-msft" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id103">[89]</a></td><td>Microsoft Corp. Web Page, July 2016. accessed 2017-01-21. URL: <a class="reference external" href="https://www.sec.gov/Archives/edgar/data/789019/000119312516662209/d187868d10k.htm">https://www.sec.gov/Archives/edgar/data/789019/000119312516662209/d187868d10k.htm</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-aws-amzn" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id104">[90]</a></td><td>Amazon.com, Inc. Web Page. accessed 2017-01-25. URL: <a class="reference external" href="https://aws.amazon.com">https://aws.amazon.com</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-softlayer-ibm" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id105">[91]</a></td><td>IBM Corp. Web Page. accessed 2017-01-25. URL: <a class="reference external" href="www.softlayer.com">www.softlayer.com</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bluemix-ibm" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id106">[92]</a></td><td>IBM Corp. Web Page. accessed 2017-01-25. URL: <a class="reference external" href="https://www.ibm.com/cloud-computing/bluemix/">https://www.ibm.com/cloud-computing/bluemix/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloud-google" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id107">[93]</a></td><td>Web Page. accessed 2017-01-25. URL: <a class="reference external" href="https://cloud.google.com">https://cloud.google.com</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ninefoldsite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id109">[94]</a></td><td>Ninefold website. Web Page. Accessed: 2017-1-28. URL: <a class="reference external" href="http://ninefold.com/news/">http://ninefold.com/news/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jelastic-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id110">[95]</a></td><td>Jelastic. Web Page, December 2016. Page Version ID: 754931676. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Jelastic&amp;oldid=754931676">https://en.wikipedia.org/w/index.php?title=Jelastic&amp;oldid=754931676</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jelastic-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id111">[96]</a></td><td>Grow Hosting Business with Software Platform. Web Page. URL: <a class="reference external" href="https://jelastic.com/cloud-business-for-hosting-providers/">https://jelastic.com/cloud-business-for-hosting-providers/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wee" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[97]</td><td><em>(<a class="fn-backref" href="#id112">1</a>, <a class="fn-backref" href="#id116">2</a>)</em> Pau&nbsp;Kiat Wee. <em>Instant AppFog</em>., chapter 1. Packt Publishing, July 2013. URL: <a class="reference external" href="https://www.packtpub.com/mapt/book/Web-Development/9781782167624/1/ch01lvl1sec03/So,+what+is+AppFog">https://www.packtpub.com/mapt/book/Web-Development/9781782167624/1/ch01lvl1sec03/So,+what+is+AppFog</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="kepes" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[98]</td><td><em>(<a class="fn-backref" href="#id113">1</a>, <a class="fn-backref" href="#id114">2</a>)</em> Ben Kepes. Understanding the cloud computing stack: saas, paas, iaas. white paper, Racspace, 2015. URL: <a class="reference external" href="https://support.rackspace.com/white-paper/understanding-the-cloud-computing-stack-saas-paas-iaas/">https://support.rackspace.com/white-paper/understanding-the-cloud-computing-stack-saas-paas-iaas/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="appfog" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id115">[99]</a></td><td>appfog. Overview. Online. URL: <a class="reference external" href="https://www.ctl.io/appfog/#Overview">https://www.ctl.io/appfog/#Overview</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tweney" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id117">[100]</a></td><td>Dylan Tweney. Appfog gives developers an easier way to deploy cloud apps (interview). Online, May 2012. URL: <a class="reference external" href="http://venturebeat.com/2012/05/15/appfog-gives-developers-an-easier-way-to-deploy-cloud-apps-interview/">http://venturebeat.com/2012/05/15/appfog-gives-developers-an-easier-way-to-deploy-cloud-apps-interview/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id118">[101]</a></td><td>Wikipedia. Cloudcontrol. Wiki Page, February 2016. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/CloudControl">https://en.wikipedia.org/wiki/CloudControl</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-dotcloud" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id119">[102]</a></td><td>Jordan Novet. Dotcloud. Web Page, January 2016. Accessed: 2017-1-31. URL: <a class="reference external" href="http://venturebeat.com/2016/01/22/dotcloud-the-cloud-service-that-gave-birth-to-docker-is-shutting-down-on-february-29/">http://venturebeat.com/2016/01/22/dotcloud-the-cloud-service-that-gave-birth-to-docker-is-shutting-down-on-february-29/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="agave-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[103]</td><td><em>(<a class="fn-backref" href="#id120">1</a>, <a class="fn-backref" href="#id122">2</a>)</em> Liya Wang, Peter&nbsp;Van Buren, and Doreen Ware. Architecting a distributed bioinformatics platform with irods and iplant agave api. In <em>2015 International Conference on Computational Science and Computational Intelligence (CSCI)</em>. December 2015.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-agaveapi-features" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id121">[104]</a></td><td>Agave api home  features tab. webpage. Accessed : 02-04-2017. URL: <a class="reference external" href="https://agaveapi.co/platform/features/">https://agaveapi.co/platform/features/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-at1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id123">[105]</a></td><td>At1. Web Page. Accessed: 2017-1-22. URL: <a class="reference external" href="http://www.cyverse.org/atmosphere">http://www.cyverse.org/atmosphere</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-at2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id124">[106]</a></td><td>Lisa Martin Charis Cook Naim Matasci Jason Williams&nbsp;Ruth Bastow. Data mining with iplant:. Paper, Oct 2014. URL: <a class="reference external" href="https://academic.oup.com/jxb/article/66/1/1/2893405/Data-mining-with-iPlantA-meeting-report-from-the">https://academic.oup.com/jxb/article/66/1/1/2893405/Data-mining-with-iPlantA-meeting-report-from-the</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apache-tajo" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id125">[107]</a></td><td>Apache tajo. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="https://tajo.apache.org">https://tajo.apache.org</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tutorialspoint-tajo" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[108]</td><td><em>(<a class="fn-backref" href="#id126">1</a>, <a class="fn-backref" href="#id127">2</a>)</em> Apache tajo quick guide. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.tutorialspoint.com/apache_tajo/apache_tajo_quick_guide.htm">https://www.tutorialspoint.com/apache_tajo/apache_tajo_quick_guide.htm</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-phoenix-cloudera" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[109]</td><td><em>(<a class="fn-backref" href="#id128">1</a>, <a class="fn-backref" href="#id132">2</a>)</em> Justin Kestelyn. Phoenix in 15 Minutes or Less. Web Page, March 2013. accessed 2017-01-25. URL: <a class="reference external" href="http://blog.cloudera.com/blog/2013/03/phoenix-in-15-minutes-or-less/">http://blog.cloudera.com/blog/2013/03/phoenix-in-15-minutes-or-less/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-phoenix-wikipedia" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id129">[110]</a></td><td>Anon. Apache Phoenix. Web Page. accessed 2017-01-25. URL: <a class="reference external" href="https://en.m.wikipedia.org/wiki/Apache_Phoenix">https://en.m.wikipedia.org/wiki/Apache_Phoenix</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apachephoenix-org" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id130">[111]</a></td><td>Anon. Web Page. accessed 2017-01-25. URL: <a class="reference external" href="http://phoenix.apache.org/">http://phoenix.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-phoenix-salesforcedev" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id131">[112]</a></td><td>Adam Seligman. Apache Phoenix - A Small Step for Big Data. Web Page, May 2014. accessed 2017-01-25. URL: <a class="reference external" href="https://developer.salesforce.com/blogs/developer-relations/2014/05/apache-phoenix-small-step-big-data.html">https://developer.salesforce.com/blogs/developer-relations/2014/05/apache-phoenix-small-step-big-data.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-phoenix-infoq" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id133">[113]</a></td><td>Abel Avram. Phoenix: Running SQL Queries on Apache HBase [Updated]. Web Page, January 2013. accessed 2017-01-25. URL: <a class="reference external" href="https://www.infoq.com/news/2013/01/Phoenix-HBase-SQL">https://www.infoq.com/news/2013/01/Phoenix-HBase-SQL</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-phoenix-bighadoop" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id134">[114]</a></td><td>Anon. Apache Phoenix - An SQL Driver for HBase. Web Page, May 2014. accessed 2017-01-23. URL: <a class="reference external" href="https://bighadoop.wordpress.com/2014/05/17/apache-phoenix-an-sql-driver-for-hbase/">https://bighadoop.wordpress.com/2014/05/17/apache-phoenix-an-sql-driver-for-hbase/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apachemrql" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[115]</td><td><em>(<a class="fn-backref" href="#id135">1</a>, <a class="fn-backref" href="#id136">2</a>)</em> Apache Software Foundation. Apache mrql. Web Page, April 2016. accessed 2017-01-29. URL: <a class="reference external" href="https://mrql.incubator.apache.org/">https://mrql.incubator.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mrqlhadoop" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id137">[116]</a></td><td>Edward&nbsp;J. Yoon. Mrql - a sql on hadoop miracle. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="http://www.hadoopsphere.com/2013/04/mrql-sql-on-hadoop-miracle.html">http://www.hadoopsphere.com/2013/04/mrql-sql-on-hadoop-miracle.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apacheincubator" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id138">[117]</a></td><td>Apache Software Foundation. Apache incubator. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="http://incubator.apache.org/">http://incubator.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sap-hana" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id139">[118]</a></td><td>SAP. What is sap hana. Web Page. Accessed: 2017-1-17. URL: <a class="reference external" href="http://www.sap.com/product/technology-platform/hana.html">http://www.sap.com/product/technology-platform/hana.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="olofson-2014" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id140">[119]</a></td><td>Carl&nbsp;W Olofson. The analytic-transactional data platform: enabling the real-time enterprise (idc). Technical Report, International Data Corporation (IDC), Dec 2014. Accessed: 2017-1-17. URL: <a class="reference external" href="http://www.sap.com/documents/2016/08/3c4e546e-817c-0010-82c7-eda71af511fa.html">http://www.sap.com/documents/2016/08/3c4e546e-817c-0010-82c7-eda71af511fa.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-presto" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id141">[120]</a></td><td>Presto. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://prestodb.io/">https://prestodb.io/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="presto-paper-2014" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id142">[121]</a></td><td>Yueguo Chen, Xiongpai Qin, Haoqiong Bian, Jun Chen, Zhaoan Dong, Xiaoyong Du, Yanjie Gao, Dehai Liu, Jiaheng Lu, and Huijie Zhang. <em>A Study of SQL-on-Hadoop Systems</em>., pages 154166. Springer International Publishing, 2014.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-apachedrill" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id143">[122]</a></td><td>Apache drill. Web Page. Accessed:2/4/2017. URL: <a class="reference external" href="https://drill.apache.org/">https://drill.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kyotocabinet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id144">[123]</a></td><td>Kyoto cabinet. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="http://fallabs.com/kyotocabinet/">http://fallabs.com/kyotocabinet/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="google-sawzall" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[124]</td><td><em>(<a class="fn-backref" href="#id145">1</a>, <a class="fn-backref" href="#id147">2</a>)</em> Rob Pike, Sean Dorward, Robert Griesemer, and Sean Quinlan. Interpreting the data: parallel analysis with sawzall. <em>Sci. Program.</em>, 13(4):277298, October 2005. URL: <a class="reference external" href="https://research.google.com/archive/sawzall-sciprog.pdf">https://research.google.com/archive/sawzall-sciprog.pdf</a>, <a class="reference external" href="http://dx.doi.org/10.1155/2005/962135">doi:10.1155/2005/962135</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bytemining-sawzall" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id146">[125]</a></td><td>Anon. Exciting Tools for Big Data: S4, Sawzall and mrjob! Web Page, November 2010. accessed 2017-01-30. URL: <a class="reference external" href="http://www.bytemining.com/2010/11/exciting-tools-for-big-data-s4-sawzall-and-mrjob/">http://www.bytemining.com/2010/11/exciting-tools-for-big-data-s4-sawzall-and-mrjob/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-google-code-wiki-sawzall" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id148">[126]</a></td><td>Google Code Archive. szl - Overview.wiki. Web Page. accessed 2017-01-30. URL: <a class="reference external" href="https://code.google.com/archive/p/szl/wikis/Overview.wiki">https://code.google.com/archive/p/szl/wikis/Overview.wiki</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="data-flow1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[127]</td><td><em>(<a class="fn-backref" href="#id149">1</a>, <a class="fn-backref" href="#id150">2</a>, <a class="fn-backref" href="#id152">3</a>)</em> Google. Data_flow1. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="https://cloud.google.com/dataflow/">https://cloud.google.com/dataflow/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="dataconomy" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id151">[128]</a></td><td>Eileen McNulty. Dataconomy. Blog. Accessed: 02/03/2016. URL: <a class="reference external" href="http://dataconomy.com/2014/08/google-cloud-dataflow/">http://dataconomy.com/2014/08/google-cloud-dataflow/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="storm-paper-ijctt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id153">[129]</a></td><td>Muhammad&nbsp;Hussain Iqbal and Tariq&nbsp;Rahim Soomro. Big data analysis: apache storm perspective. In <em>International Journal of Computer Trends and Technology (IJCTT)-2015</em>. January 2015.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-storm-home-concepts" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id154">[130]</a></td><td>Apache storm homepage. webpage. Accessed : 01-22-2017. URL: <a class="reference external" href="http://storm.apache.org/releases/1.0.2/Concepts.html">http://storm.apache.org/releases/1.0.2/Concepts.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-samza-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[131]</td><td><em>(<a class="fn-backref" href="#id155">1</a>, <a class="fn-backref" href="#id158">2</a>)</em> Apache Samza. Web Page, February 2017. Page Version ID: 764035647. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Apache_Samza&amp;oldid=764035647">https://en.wikipedia.org/w/index.php?title=Apache_Samza&amp;oldid=764035647</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-samza-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id156">[132]</a></td><td>Samza. Web Page. URL: <a class="reference external" href="http://samza.apache.org/">http://samza.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-samza-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id157">[133]</a></td><td>Apache Samza, LinkedIn&#8216;s Framework for Stream Processing. Web Page, January 2015. URL: <a class="reference external" href="https://thenewstack.io/apache-samza-linkedins-framework-for-stream-processing/">https://thenewstack.io/apache-samza-linkedins-framework-for-stream-processing/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-granules" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id159">[134]</a></td><td>Shrideep Pallickara Thilina Buddhika Matthew Malensek&nbsp;Ryan Stern. Granules. Project, July 2016. URL: <a class="reference external" href="http://granules.cs.colostate.edu/">http://granules.cs.colostate.edu/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kinesis" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id160">[135]</a></td><td>Kinesis, components. Web Page. Accessed: 2017-01-17. URL: <a class="reference external" href="http://docs.aws.amazon.com/streams/latest/dev/key-concepts.html/">http://docs.aws.amazon.com/streams/latest/dev/key-concepts.html/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="big-data-analytics-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id161">[136]</a></td><td>Sumit&nbsp;Gupta Shilpi&nbsp;Saxena. <em>Real-Time Big Data Analytics</em>. Packt Publishing, 35 Livery Street, Birmingham B3 2PB, UK, 1st edition, 2016. ISBN 9781784391409.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="twitterheronopen" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id162">[137]</a></td><td>Twittter. Open sourcing twitter heron. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://blog.twitter.com/2016/open-sourcing-twitter-heron">https://blog.twitter.com/2016/open-sourcing-twitter-heron</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="twitterheron" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id163">[138]</a></td><td>Twittter. Flying faster with twitter heron. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://blog.twitter.com/2015/flying-faster-with-twitter-heron">https://blog.twitter.com/2015/flying-faster-with-twitter-heron</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azurestreamanalytics" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id164">[139]</a></td><td>Microsoft Azure real-time data analytics. Web Page. Accessed: 2017-2-03. URL: <a class="reference external" href="https://azure.microsoft.com/en-us/services/stream-analytics/">https://azure.microsoft.com/en-us/services/stream-analytics/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-docs-microsoft" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id165">[140]</a></td><td>Microsoft Docs azure stream analytics documentation. Web Page. Accessed: 2017-2-03. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/stream-analytics/">https://docs.microsoft.com/en-us/azure/stream-analytics/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-github-azure" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id166">[141]</a></td><td>Github azure/ azure-stream-analytics. Web Page. Accessed: 2017-2-03. URL: <a class="reference external" href="https://github.com/Azure/azure-stream-analytics/">https://github.com/Azure/azure-stream-analytics/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-spark" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id167">[142]</a></td><td>Abdul&nbsp;Ghaffar Shoro and Tariq&nbsp;Rahim Soomro. Big data analysis: apache spark perspective. <em>Global Journal of Computer Science and Technology</em>, 2015. Accessed: 2017-1-21. URL: <a class="reference external" href="http://www.computerresearch.org/index.php/computer/article/view/1137/1124">http://www.computerresearch.org/index.php/computer/article/view/1137/1124</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mapreducempi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id168">[143]</a></td><td>Mr-mpi. Web Page, 2017. URL: <a class="reference external" href="http://mapreduce.sandia.gov/doc">http://mapreduce.sandia.gov/doc</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-reef" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id169">[144]</a></td><td>Reef. Webpage. Accessed: 2017-01-28. URL: <a class="reference external" href="https://wiki.apache.org/incubator/ReefProposal">https://wiki.apache.org/incubator/ReefProposal</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="apache-hama" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id170">[145]</a></td><td>Apache. web-page. URL: <a class="reference external" href="https://hama.apache.org/">https://hama.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-hama" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id171">[146]</a></td><td>James J. (Jong&nbsp;Hyuk) Park, Hai Jin, Young-Sik Jeong, and Muhammad&nbsp;Khurram Khan. <em>Advanced Multimedia and Ubiquitous Engineering</em>. Springer, 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-galoissite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id172">[147]</a></td><td>Galois website. Web Page. Accessed: 2017-1-28. URL: <a class="reference external" href="www-galoisSite: http://iss.ices.utexas.edu/?p=projects/galois">www-galoisSite: http://iss.ices.utexas.edu/?p=projects/galois</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="taoparallelismpaper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id173">[148]</a></td><td>Keshav Pingali, Donald Nguyen, Milind Kulkarni, Martin Burtscher, M.&nbsp;Amber Hassaan, Rashid Kaleem, Tsung-Hsien Lee, Andrew Lenharth, Roman Manevich, Mario Mendez-Lojo, Dimitrios Prountzos, and Xin Sui. The tao of parallelism in algorithms. In <em>The Tao of Parallelism in Algorithms</em>, 114. June 2011. URL: <a class="reference external" href="http://iss.ices.utexas.edu/Publications/Papers/pingali11.pdf">http://iss.ices.utexas.edu/Publications/Papers/pingali11.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hpx-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id174">[149]</a></td><td>High performance parallex (hpx-5). Web Page. Accessed: 2017-1-17. URL: <a class="reference external" href="https://hpx.crest.iu.edu/">https://hpx.crest.iu.edu/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hpx-5-user-guide" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id175">[150]</a></td><td><em>HPX-5: user guide</em>. HPX-5. Accessed: 2017-1-17. URL: <a class="reference external" href="https://hpx.crest.iu.edu/users_guide">https://hpx.crest.iu.edu/users_guide</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-harp" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id176">[151]</a></td><td>Harp. Webpage. Accessed: 2017-01-28. URL: <a class="reference external" href="http://harpjs.com">http://harpjs.com</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-netty" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id177">[152]</a></td><td>Netty site. Web Page. URL: <a class="reference external" href="http://netty.io/">http://netty.io/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="netty-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id178">[153]</a></td><td>Norman Maurer and Marvin Wolfthal. <em>Netty in Action</em>. Manning Publications, 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zeromq" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[154]</td><td><em>(<a class="fn-backref" href="#id179">1</a>, <a class="fn-backref" href="#id181">2</a>)</em> Distributed messaging. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://zeromq.org">http://zeromq.org</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zeromq2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id180">[155]</a></td><td>0mq - the guide. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://zguide.zeromq.org/page:all">http://zguide.zeromq.org/page:all</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-rabbitmq" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id182">[156]</a></td><td>RabbitMQ, components. Web Page. Accessed: 2017-01-19. URL: <a class="reference external" href="https://www.rabbitmq.com/">https://www.rabbitmq.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ampq-article" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id183">[157]</a></td><td>John O&#8217;Hara. Toward a commodity enterprise middleware. <em>Queue</em>, 5(4):4855, 2007.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jms-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id184">[158]</a></td><td>Wikipedia link for jms. webpage. Accessed : 01-18-2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Java_Message_Service">https://en.wikipedia.org/wiki/Java_Message_Service</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jms-oracle-docs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id185">[159]</a></td><td>Oracle documentation on jms. webpage. Accessed : 01-18-2017. URL: <a class="reference external" href="http://docs.oracle.com/javaee/6/tutorial/doc/bnceh.html">http://docs.oracle.com/javaee/6/tutorial/doc/bnceh.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-amqp" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id186">[160]</a></td><td>Amqp is the internet protocol for business messaging. Web Page. accessed: 2017-01-31. URL: <a class="reference external" href="http://www.amqp.org/about/what">http://www.amqp.org/about/what</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mqtt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id187">[161]</a></td><td>Mqtt. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="http://mqtt.org/">http://mqtt.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-floodnet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id188">[162]</a></td><td>Mqtt floodnet. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="http://mqtt.org/projects/floodnet">http://mqtt.org/projects/floodnet</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sns" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id189">[163]</a></td><td>Amazon sns. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="https://aws.amazon.com/sns/">https://aws.amazon.com/sns/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="sns-blog" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id190">[164]</a></td><td>Amazon sns blog. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="https://aws.amazon.com/blogs/aws/introducing-the-amazon-simple-notification-service/">https://aws.amazon.com/blogs/aws/introducing-the-amazon-simple-notification-service/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="sns-faq" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id191">[165]</a></td><td>Amazon sns faqs. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="https://aws.amazon.com/sns/faqs/">https://aws.amazon.com/sns/faqs/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-google-pub-sub" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id192">[166]</a></td><td>What-is-google-pub-sub. Web Page. Accessed: 2017-2-09. URL: <a class="reference external" href="https://cloud.google.com/pubsub/docs/overview">https://cloud.google.com/pubsub/docs/overview</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-google-pub-sub-features" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id193">[167]</a></td><td>Google-pub-sub-scalable-messaging-middleware. Web Page. Accessed: 2017-2-10. URL: <a class="reference external" href="https://cloud.google.com/pubsub/">https://cloud.google.com/pubsub/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="silberschatz1998operating" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id194">[168]</a></td><td>Abraham Silberschatz, Peter&nbsp;B Galvin, Greg Gagne, and A&nbsp;Silberschatz. <em>Operating system concepts</em>. volume 4. Addison-wesley Reading, 1998.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azurequeue-web" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id195">[169]</a></td><td>Azure queue website. Web Page. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/storage/storage-dotnet-how-to-use-queues">https://docs.microsoft.com/en-us/azure/storage/storage-dotnet-how-to-use-queues</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tutorialspoint" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id196">[170]</a></td><td>Tutorialspoint website. Web Page. URL: <a class="reference external" href="https://www.tutorialspoint.com/microsoft_azure/microsoft_azure_queues.htm">https://www.tutorialspoint.com/microsoft_azure/microsoft_azure_queues.htm</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-gora" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id197">[171]</a></td><td>Gora, components. Web Page. Accessed: 2017-01-18. URL: <a class="reference external" href="http://gora.apache.org/">http://gora.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-memcached" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id198">[172]</a></td><td>Memcached. Web Page. Accessed: 2017-01-30. URL: <a class="reference external" href="http://www.memcached.org/">http://www.memcached.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-keyvalue" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id199">[173]</a></td><td>Wikipedia. Key-value database. WebPage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Key-value_database">https://en.wikipedia.org/wiki/Key-value_database</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-relationaldb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id200">[174]</a></td><td>Wikipedia. Relational database. WebPage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Relational_database">https://en.wikipedia.org/wiki/Relational_database</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-lmdb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id201">[175]</a></td><td>Matthew Hardin. Understanding lmdb database file sizes and memory utilization. WebPage, May 2016. URL: <a class="reference external" href="https://symas.com/understanding-lmdb-database-file-sizes-and-memory-utilization/">https://symas.com/understanding-lmdb-database-file-sizes-and-memory-utilization/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikihazel" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[176]</td><td><em>(<a class="fn-backref" href="#id202">1</a>, <a class="fn-backref" href="#id204">2</a>, <a class="fn-backref" href="#id205">3</a>)</em> Wikipedia. Hazelcast. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Hazelcast">https://en.wikipedia.org/wiki/Hazelcast</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-githubhazel" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id203">[177]</a></td><td>Hazelcast. Open source in-memory data grid. Code Repository, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="https://github.com/hazelcast/hazelcast">https://github.com/hazelcast/hazelcast</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ehcache-features" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id206">[178]</a></td><td>Ehcache - features. Web Page. Accessed: 2017-01-21. URL: <a class="reference external" href="http://www.ehcache.org/about/features.html">http://www.ehcache.org/about/features.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ehcache-documentation" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id207">[179]</a></td><td>Ehcache - documentation. Web Page. Accessed: 2017-01-21. URL: <a class="reference external" href="http://www.ehcache.org/documentation/3.2/getting-started.html">http://www.ehcache.org/documentation/3.2/getting-started.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hstore" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id208">[180]</a></td><td>H-store. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="http://hstore.cs.brown.edu/">http://hstore.cs.brown.edu/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="kallman2008" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id209">[181]</a></td><td>Robert Kallman, Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alexander Rasin, Stanley Zdonik, Evan P.&nbsp;C. Jones, Samuel Madden, Michael Stonebraker, Yang Zhang, John Hugg, and Daniel&nbsp;J. Abadi. H-Store: a high-performance, distributed main memory transaction processing system. <em>Proc. VLDB Endow.</em>, 1(2):14961499, 2008. URL: <a class="reference external" href="http://hstore.cs.brown.edu/papers/hstore-demo.pdf">http://hstore.cs.brown.edu/papers/hstore-demo.pdf</a>, <a class="reference external" href="http://dx.doi.org/http://doi.acm.org/10.1145/1454159.1454211">doi:http://doi.acm.org/10.1145/1454159.1454211</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hstorewiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id210">[182]</a></td><td>H-storewiki. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/H-Store">https://en.wikipedia.org/wiki/H-Store</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-eclipselink" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id211">[183]</a></td><td>Eclipselink. Accessed: 02-06-2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/EclipseLink">https://en.wikipedia.org/wiki/EclipseLink</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="datanucleuswiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id212">[184]</a></td><td>Wikipedia. Datanucleus wiki. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/DataNucleus">https://en.wikipedia.org/wiki/DataNucleus</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="datanucleus" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id213">[185]</a></td><td>DataNucleus. Datanucleus. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="http://www.datanucleus.com/">http://www.datanucleus.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="datanucleusperformance" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id214">[186]</a></td><td>JPAB. Jpa performance benchmark. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="http://www.jpab.org/DataNucleus.html">http://www.jpab.org/DataNucleus.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="uima-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id215">[187]</a></td><td>Wikipedia. Uima_wiki. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/UIMA">https://en.wikipedia.org/wiki/UIMA</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="uima-ss" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id216">[188]</a></td><td>Slideshare. Uima_ss. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="http://www.slideshare.net/teofili/apache-uima-introduction">http://www.slideshare.net/teofili/apache-uima-introduction</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tika" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id217">[189]</a></td><td>Apache tika. Web Page. URL: <a class="reference external" href="https://tika.apache.org/">https://tika.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sqlserver-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id218">[190]</a></td><td>Sql server wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Microsoft_SQL_Server">https://en.wikipedia.org/wiki/Microsoft_SQL_Server</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azuresql" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id219">[191]</a></td><td>Sql server azure. Web Page. URL: <a class="reference external" href="https://azure.microsoft.com/en-us/services/sql-database/?b=16.50">https://azure.microsoft.com/en-us/services/sql-database/?b=16.50</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-sqlserver" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id220">[192]</a></td><td>Ross Mistry and Stacia Misner. <em>Introducing Microsoft SQL Server 2014 Technical Overview</em>. Microsoft Press, 2014. ISBN 978-0-7356-8475-1.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="devmysql" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[193]</td><td><em>(<a class="fn-backref" href="#id221">1</a>, <a class="fn-backref" href="#id222">2</a>, <a class="fn-backref" href="#id223">3</a>, <a class="fn-backref" href="#id224">4</a>)</em> mysql. Mysql 5.7 reference manual, what is mysql. Online. URL: <a class="reference external" href="https://dev.mysql.com/doc/refman/5.7/en/what-is-mysql.html">https://dev.mysql.com/doc/refman/5.7/en/what-is-mysql.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="howmysql" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id225">[194]</a></td><td>HowStuffWorks.com. What are relational databases? Online, March 2001. URL: <a class="reference external" href="http://computer.howstuffworks.com/question599.htm">http://computer.howstuffworks.com/question599.htm</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cubrid" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id226">[195]</a></td><td>Cubrid. Web Page, 2017. URL: <a class="reference external" href="http://www.cubrid.org/">http://www.cubrid.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-galera-cluster" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id227">[196]</a></td><td>Galera cluster. Web Page, 2017. URL: <a class="reference external" href="http://galeracluster.com/">http://galeracluster.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="amazonrds" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id228">[197]</a></td><td>Amazon RDS. Amazonrds. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="http://searchaws.techtarget.com/definition/Amazon-Relational-Database-Service-RDS">http://searchaws.techtarget.com/definition/Amazon-Relational-Database-Service-RDS</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="amazonrdscomponents" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id229">[198]</a></td><td>Amazon RDS. What is amazon rds. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html#Welcome.Concepts">http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html#Welcome.Concepts</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibm-dash-db-com" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id230">[199]</a></td><td>5 things to know about dashdb. web page. URL: <a class="reference external" href="https://www.ibm.com/developerworks/community/blogs/5things/entry/5_things_to_know_about_dashdb_placeholder?lang=en">https://www.ibm.com/developerworks/community/blogs/5things/entry/5_things_to_know_about_dashdb_placeholder?lang=en</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibm-analytics-com" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id231">[200]</a></td><td>Ibm dashdb. Web Page. URL: <a class="reference external" href="https://www.ibm.com/analytics/us/en/technology/cloud-data-services/dashdb/">https://www.ibm.com/analytics/us/en/technology/cloud-data-services/dashdb/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-lucene" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id232">[201]</a></td><td>Apache lucene. Web Page, January 2017. URL: <a class="reference external" href="http://lucene.apache.org/">http://lucene.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-solandra" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id233">[202]</a></td><td>Solandra. Webpage. URL: <a class="reference external" href="https://github.com/tjake/Solandra/wiki/Solandra-Wiki">https://github.com/tjake/Solandra/wiki/Solandra-Wiki</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-solandra2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id234">[203]</a></td><td>Solandra. Webpage. URL: <a class="reference external" href="https://github.com/tjake/Solandra">https://github.com/tjake/Solandra</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-voldemort" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id235">[204]</a></td><td>LinkedIn. Project voldemort. Web Page. Accessed: 2017-1-17. URL: <a class="reference external" href="http://www.project-voldemort.com/voldemort/">http://www.project-voldemort.com/voldemort/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="rabl-sadoghi-jacobsen-2012" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id236">[205]</a></td><td>Tilmann Rabl, Sergio Gmez-Villamor, Mohammad Sadoghi, Victor Munts-Mulero, Hans-Arno Jacobsen, and Serge Mankovskii. Solving big data challenges for enterprise application performance management. <em>Proc. VLDB Endow.</em>, 5(12):17241735, August 2012. URL: <a class="reference external" href="https://arxiv.org/pdf/1208.4167.pdf">https://arxiv.org/pdf/1208.4167.pdf</a>, <a class="reference external" href="http://dx.doi.org/10.14778/2367502.2367512">doi:10.14778/2367502.2367512</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-riak-kv" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id237">[206]</a></td><td>Riak-kv - nosql key value database. Web Page. Accessed: 2017-01-20. URL: <a class="reference external" href="http://basho.com/products/riak-kv/">http://basho.com/products/riak-kv/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-riak-ts" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id238">[207]</a></td><td>Riak-ts - nosql time series database. Web Page. Accessed: 2017-01-20. URL: <a class="reference external" href="http://basho.com/products/riak-ts/">http://basho.com/products/riak-ts/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-riak-s2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id239">[208]</a></td><td>Riak-s2 - cloud object storage software. Web Page. Accessed: 2017-01-20. URL: <a class="reference external" href="http://basho.com/products/riak-s2/">http://basho.com/products/riak-s2/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="datasys" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id240">[209]</a></td><td>Illinois&nbsp;Institute of&nbsp;Technology Department&nbsp;of Computer&nbsp;Science. Zht: a zero-hop distributed hashtable. Online. URL: <a class="reference external" href="http://datasys.cs.iit.edu/projects/ZHT/">http://datasys.cs.iit.edu/projects/ZHT/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wiley" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[210]</td><td><em>(<a class="fn-backref" href="#id241">1</a>, <a class="fn-backref" href="#id242">2</a>, <a class="fn-backref" href="#id243">3</a>)</em> Brandon Wiley. Distributed hash ttable, part 1. <em>Linux Journal</em>, October 2003. From issue number 114. URL: <a class="reference external" href="http://www.linuxjournal.com/article/6797?page=0,0">http://www.linuxjournal.com/article/6797?page=0,0</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="li" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[211]</td><td><em>(<a class="fn-backref" href="#id244">1</a>, <a class="fn-backref" href="#id245">2</a>, <a class="fn-backref" href="#id246">3</a>)</em> T.&nbsp;Li, X.&nbsp;Zhou, K.&nbsp;Brandstatter, D.&nbsp;Zhao, K.&nbsp;Wang, A.&nbsp;Rajendran, Z.&nbsp;Zhang, and I.&nbsp;Raicu. Zht: a light-weight reliable persistent dynamic scalable zero-hop distributed hash table. In <em>2013 IEEE 27th International Symposium on Parallel and Distributed Processing</em>, 775787. May 2013. URL: <a class="reference external" href="http://datasys.cs.iit.edu/publications/2013_IPDPS13_ZHT.pdf">http://datasys.cs.iit.edu/publications/2013_IPDPS13_ZHT.pdf</a>, <a class="reference external" href="http://dx.doi.org/10.1109/IPDPS.2013.110">doi:10.1109/IPDPS.2013.110</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bdb-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id247">[212]</a></td><td>Berkeley db wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Berkeley_DB">https://en.wikipedia.org/wiki/Berkeley_DB</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bdb-stanford" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id248">[213]</a></td><td>Stanford course website. Web Page. URL: <a class="reference external" href="https://web.stanford.edu/class/cs276a/projects/docs/berkeleydb/reftoc.html">https://web.stanford.edu/class/cs276a/projects/docs/berkeleydb/reftoc.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bdb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id249">[214]</a></td><td>Oracle website. Web Page. URL: <a class="reference external" href="http://www.oracle.com/technetwork/database/database-technologies/berkeleydb">http://www.oracle.com/technetwork/database/database-technologies/berkeleydb</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tokyo-cabinet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[215]</td><td><em>(<a class="fn-backref" href="#id250">1</a>, <a class="fn-backref" href="#id252">2</a>)</em> Tokyo cabinet. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="http://fallabs.com/tokyocabinet/">http://fallabs.com/tokyocabinet/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kyoto-cabinet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id251">[216]</a></td><td>Kyoto cabinet. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="http://fallabs.com/kyotocabinet/">http://fallabs.com/kyotocabinet/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tycoon-fl" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id253">[217]</a></td><td>Fal labs. Tycoon_fl. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="http://fallabs.com/kyototycoon/">http://fallabs.com/kyototycoon/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tycoon-cf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id254">[218]</a></td><td>Cloudflare. Tycoon_cf. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="https://blog.cloudflare.com/kyoto-tycoon-secure-replication/">https://blog.cloudflare.com/kyoto-tycoon-secure-replication/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tycoon-fl2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id255">[219]</a></td><td>Fal labs. Tycoon_fl2. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="http://fallabs.com/kyotocabinet/">http://fallabs.com/kyotocabinet/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tyrant-blog" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id256">[220]</a></td><td>Tyrant blog. Web Page. URL: <a class="reference external" href="https://www.percona.com/blog/2009/10/19/mysql_memcached_tyrant_part3/">https://www.percona.com/blog/2009/10/19/mysql_memcached_tyrant_part3/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tyrant-fal-labs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id257">[221]</a></td><td>Tyrant fallabs. Web Page. URL: <a class="reference external" href="http://fallabs.com/tokyotyrant/">http://fallabs.com/tokyotyrant/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kyoto-tycoon" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id258">[222]</a></td><td>Kyoto tycoon. Web Page. URL: <a class="reference external" href="http://fallabs.com/kyototycoon/">http://fallabs.com/kyototycoon/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-infoworld-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id259">[223]</a></td><td>Rick Grehan. NoSQL Showdown: MongoDB vs. Couchbase. Web Page, March 2013. accessed 2017-01-29. URL: <a class="reference external" href="http://www.infoworld.com/article/2613970/nosql/nosql-showdown--mongodb-vs--couchbase.html">http://www.infoworld.com/article/2613970/nosql/nosql-showdown&#8211;mongodb-vs&#8211;couchbase.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-safaribooks-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id260">[224]</a></td><td>Martin Brown. The Technology Behind Couchbase. Web Page, March 2012. accessed 2017-01-29. URL: <a class="reference external" href="https://www.safaribooksonline.com/blog/2012/03/01/the-technology-behind-couchbase/">https://www.safaribooksonline.com/blog/2012/03/01/the-technology-behind-couchbase/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-erlangcentral-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id261">[225]</a></td><td>Couchbase Performance and Scalability: Iterating with DTrace Observability. Web Page, March 2012. accessed 2017-01-29. URL: <a class="reference external" href="http://erlangcentral.org/videos/couchbase-performance-and-scalability-iterating-with-dtrace-observability/#.WI5uYephnRY">http://erlangcentral.org/videos/couchbase-performance-and-scalability-iterating-with-dtrace-observability/#.WI5uYephnRY</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikipedia-erlang-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id262">[226]</a></td><td>Erlang (programming language. Web Page. accessed: 2017-01-29. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Erlang_(programming_language)">https://en.wikipedia.org/wiki/Erlang_(programming_language)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-couchbase-blog-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id263">[227]</a></td><td>Sean Lynch. Why Membase Uses Erlang. Web Page, October 2010. accessed 2017-01-29. URL: <a class="reference external" href="https://blog.couchbase.com/why-membase-uses-erlang">https://blog.couchbase.com/why-membase-uses-erlang</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hightower-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id264">[228]</a></td><td>Riyad Kalla. Well put! When should you use MongoDB vs Couchbase versus Redis... Web Page, October 2011. accessed 2017-01-29. URL: <a class="reference external" href="http://rick-hightower.blogspot.com/2014/04/well-put-when-should-you-use-mongodb-vs.html">http://rick-hightower.blogspot.com/2014/04/well-put-when-should-you-use-mongodb-vs.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-quora-cbs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id265">[229]</a></td><td>Russell Smith. What are the advantages and disadvantages of using MongoDB vs CouchDB vs Cassandra vs Redis? Web Page, November 2015. accessed 2017-01-29. URL: <a class="reference external" href="https://www.quora.com/What-are-the-advantages-and-disadvantages-of-using-MongoDB-vs-CouchDB-vs-Cassandra-vs-Redis">https://www.quora.com/What-are-the-advantages-and-disadvantages-of-using-MongoDB-vs-CouchDB-vs-Cassandra-vs-Redis</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-gemfire" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id266">[230]</a></td><td>About pivotal gemfire. Web Page. Accessed: 2017-01-28. URL: <a class="reference external" href="http://gemfire.docs.pivotal.io/gemfire/getting_started/gemfire_overview.html">http://gemfire.docs.pivotal.io/gemfire/getting_started/gemfire_overview.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hbase" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id267">[231]</a></td><td>Apache hbase. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://hbase.apache.org/">https://hbase.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudbigtable" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[232]</td><td><em>(<a class="fn-backref" href="#id268">1</a>, <a class="fn-backref" href="#id269">2</a>)</em> Google. Cloud bigtable. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="https://cloud.google.com/bigtable/">https://cloud.google.com/bigtable/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikibigtable" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[233]</td><td><em>(<a class="fn-backref" href="#id270">1</a>, <a class="fn-backref" href="#id271">2</a>)</em> Wikipedia. Bigtable. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Bigtable">https://en.wikipedia.org/wiki/Bigtable</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikispanner" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id272">[234]</a></td><td>Wikipedia. Spanner (database). Web Page, October 2016. accessed 2017-01-29. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Spanner_(database)">https://en.wikipedia.org/wiki/Spanner_(database)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="corbett-spanner" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id273">[235]</a></td><td>James&nbsp;C Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, Jeffrey&nbsp;John Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, and others. Spanner: googles globally distributed database. <em>ACM Transactions on Computer Systems (TOCS)</em>, 31(3):8, 2013. URL: <a class="reference external" href="http://dl.acm.org/ft_gateway.cfm?id=2491245&amp;type=pdf">http://dl.acm.org/ft_gateway.cfm?id=2491245&amp;type=pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-magastore-spanner" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id274">[236]</a></td><td>Magastore, Spanner, components. Web Page. Accessed: 2017-01-28. URL: <a class="reference external" href="http://blog.mikiobraun.de/2013/03/more-google-papers-megastore-spanner-voted-commits.html">http://blog.mikiobraun.de/2013/03/more-google-papers-megastore-spanner-voted-commits.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cassandra" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id275">[237]</a></td><td>Apache cassandra. Web Page, 2016. URL: <a class="reference external" href="http://cassandra.apache.org/">http://cassandra.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="punnoose" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[238]</td><td><em>(<a class="fn-backref" href="#id276">1</a>, <a class="fn-backref" href="#id277">2</a>, <a class="fn-backref" href="#id279">3</a>, <a class="fn-backref" href="#id280">4</a>)</em> Roshan Punnoose, Adina Crainiceanu, and David Rapp. Rya: a scalable rdf triple store for the clouds. In <em>Proceedings of the 1st International Workshop on Cloud Intelligence</em>, Cloud-I &#8216;12, 4:14:8. New York, NY, USA, 2012. ACM. URL: <a class="reference external" href="http://doi.acm.org/10.1145/2347673.2347677">http://doi.acm.org/10.1145/2347673.2347677</a>, <a class="reference external" href="http://dx.doi.org/10.1145/2347673.2347677">doi:10.1145/2347673.2347677</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="w3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id278">[239]</a></td><td>RDF&nbsp;Working Group. Resource description framework (rdf). Online, February 2014. URL: <a class="reference external" href="https://www.w3.org/RDF/">https://www.w3.org/RDF/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="apacherya" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id281">[240]</a></td><td>Apache Rya. Apache rya. Online. URL: <a class="reference external" href="https://rya.apache.org/">https://rya.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-graphdb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id282">[241]</a></td><td>GraphDb. Web Page. Accessed: 2017-01-30. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Graph_database/">https://en.wikipedia.org/wiki/Graph_database/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-allegro" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id283">[242]</a></td><td>Allegro. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="http://allegrograph.com/">http://allegrograph.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-allegrow" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id284">[243]</a></td><td>Allegrow. Web Page. Accessed: 2017-1-20. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/AllegroGraph">https://en.wikipedia.org/wiki/AllegroGraph</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-titan" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id285">[244]</a></td><td>Titan db. Web Page. Accessed:2/4/2017. URL: <a class="reference external" href="http://titan.thinkaurelius">http://titan.thinkaurelius</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-tinkerpop" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id286">[245]</a></td><td>Tinkerpop. Web Page. Accessed:2/6/2017. URL: <a class="reference external" href="http://tinkerpop.apache.org/">http://tinkerpop.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="jena-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id287">[246]</a></td><td>w3. Jena_wiki. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="https://www.w3.org/2001/sw/wiki/Apache_Jena">https://www.w3.org/2001/sw/wiki/Apache_Jena</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="jena-blog" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[247]</td><td><em>(<a class="fn-backref" href="#id288">1</a>, <a class="fn-backref" href="#id289">2</a>)</em> Trimac&nbsp;NLP Blog. Jena_blog. Web Page. Accessed: 02/03/2016. URL: <a class="reference external" href="http://trimc-nlp.blogspot.com/2013/06/introduction-to-jena.html">http://trimc-nlp.blogspot.com/2013/06/introduction-to-jena.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-rdf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id290">[248]</a></td><td>RDF components. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://www.w3.org/RDF/">https://www.w3.org/RDF/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sesame" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id291">[249]</a></td><td>sesame components. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://projects.eclipse.org/projects/technology.rdf4j">https://projects.eclipse.org/projects/technology.rdf4j</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="sesame-paper-2013" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id292">[250]</a></td><td>Ana Lucia&nbsp;Varbanescu Jianbin&nbsp;Fang and Henk Sips. Sesame: a user-transparent optimizing framework for many-core processors. In <em>IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing</em>, 7073. 2013.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-what-to-use" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id293">[251]</a></td><td>blog.wouldbetheologian.com. Azure: What to Use, What to Avoid. Web Page, October 2014. accessed 2017-01-28. URL: <a class="reference external" href="http://blog.wouldbetheologian.com/2014/10/azure-what-to-use-what-to-avoid.html">http://blog.wouldbetheologian.com/2014/10/azure-what-to-use-what-to-avoid.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-blobqueuetable" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id294">[252]</a></td><td>www.thewindowsclub.com. Understanding Blob, Queue and Table Storage for Windows Azure. Web Page. accessed 2017-01-28; no published data available. URL: <a class="reference external" href="http://www.thewindowsclub.com/understanding-blobqueuetable-storage-windows-azure">http://www.thewindowsclub.com/understanding-blobqueuetable-storage-windows-azure</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-scalable-partitioning" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id295">[253]</a></td><td>www.microsoft.com. Designing a Scalable Partitioning Strategy for Azure Table Storage. Web Page, January 2017. accessed 2017-01-28. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/rest/api/storageservices/fileservices/designing-a-scalable-partitioning-strategy-for-azure-table-storage">https://docs.microsoft.com/en-us/rest/api/storageservices/fileservices/designing-a-scalable-partitioning-strategy-for-azure-table-storage</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fits-nasa" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id296">[254]</a></td><td>Fits nasa. web. URL: <a class="reference external" href="https://fits.gsfc.nasa.gov/">https://fits.gsfc.nasa.gov/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-news-fits-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id297">[255]</a></td><td>Fits news. Web Page. URL: <a class="reference external" href="https://fits.gsfc.nasa.gov/fits_standard.html">https://fits.gsfc.nasa.gov/fits_standard.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fits-vatican-library" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id298">[256]</a></td><td>Fits vatican library. Web Page. URL: <a class="reference external" href="https://www.vatlib.it/home.php?pag=digitalizzazione&amp;ling=eng">https://www.vatlib.it/home.php?pag=digitalizzazione&amp;ling=eng</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fits-matlab" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id299">[257]</a></td><td>Fits matlab. Web Page. URL: <a class="reference external" href="https://www.mathworks.com/help/matlab/import_export/importing-flexible-image-transport-system-fits-files.html?requestedDomain=www.mathworks.com">https://www.mathworks.com/help/matlab/import_export/importing-flexible-image-transport-system-fits-files.html?requestedDomain=www.mathworks.com</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-fits-2011" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id300">[258]</a></td><td><em>Astronomical Image Processing with Hadoop</em>, volume 442, Astronomical Data Analysis Software and Systems XX. ASP Conference Proceedings, July 2011. URL: <a class="reference external" href="http://adsabs.harvard.edu/abs/2011ASPC..442...93W">http://adsabs.harvard.edu/abs/2011ASPC..442...93W</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-rcfile" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[259]</td><td><em>(<a class="fn-backref" href="#id301">1</a>, <a class="fn-backref" href="#id302">2</a>)</em> Rcfilecat - apache hive. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="https://cwiki.apache.org/confluence/display/Hive/RCFileCat">https://cwiki.apache.org/confluence/display/Hive/RCFileCat</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="he2011rcfile" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id303">[260]</a></td><td>Yongqiang He, Rubao Lee, Yin Huai, Zheng Shao, Namit Jain, Xiaodong Zhang, and Zhiwei Xu. Rcfile: a fast and space-efficient data placement structure in mapreduce-based warehouse systems. In <em>Data Engineering (ICDE), 2011 IEEE 27th International Conference on</em>, 11991208. IEEE, 2011.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-orc-docs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id304">[261]</a></td><td>Background. web-page. URL: <a class="reference external" href="https://orc.apache.org/docs/">https://orc.apache.org/docs/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-parquet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id305">[262]</a></td><td>Parquet. Accessed: 02-06-2017. URL: <a class="reference external" href="https://parquet.apache.org/documentation/latest">https://parquet.apache.org/documentation/latest</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-bittorrent" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id306">[263]</a></td><td>Bittorrent. Web Page, 2017. URL: <a class="reference external" href="https://www.lifewire.com/how-torrent-downloading-works-2483513">https://www.lifewire.com/how-torrent-downloading-works-2483513</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ftp-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id307">[264]</a></td><td>Ftp wikipedia. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="https://cwiki.apache.org/confluence/display/Hive/RCFileCat">https://cwiki.apache.org/confluence/display/Hive/RCFileCat</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-rfc114" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id308">[265]</a></td><td>Rfc114 specification. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="https://cwiki.apache.org/confluence/display/Hive/RCFileCat">https://cwiki.apache.org/confluence/display/Hive/RCFileCat</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ssh-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id309">[266]</a></td><td>Ssh - wikipedia. Web Page. Accessed: 2017-01-26. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Secure_Shell">https://en.wikipedia.org/wiki/Secure_Shell</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openssh-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id310">[267]</a></td><td>Openssh - wikipedia. Web Page. Accessed: 2017-01-26. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/OpenSSH">https://en.wikipedia.org/wiki/OpenSSH</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-globusonline" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id311">[268]</a></td><td>Globus online (gridftp). Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/GridFTP">https://en.wikipedia.org/wiki/GridFTP</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ibm-flume" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id313">[269]</a></td><td>IBM. What is flume? web page. URL: <a class="reference external" href="https://www-01.ibm.com/software/data/infosphere/hadoop/flume/">https://www-01.ibm.com/software/data/infosphere/hadoop/flume/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sqoop" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id314">[270]</a></td><td>The Apache&nbsp;Software Foundation. Web. URL: <a class="reference external" href="http://sqoop.apache.org/">http://sqoop.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="sqoop-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id315">[271]</a></td><td>Wikipedia. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Sqoop">https://en.wikipedia.org/wiki/Sqoop</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-mesos" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id316">[272]</a></td><td>Mesos site. Web Page. URL: <a class="reference external" href="http://mesos.apache.org/">http://mesos.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-mesos-abu-dbai-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id317">[273]</a></td><td>Abed Abu-Dbai, David Breitgand, Gidon Gershinsky, Alex Glikson, and Khalid Ahmed. Enterprise resource management in mesos clusters. In <em>Proceedings of the 9th ACM International on Systems and Storage Conference</em>, SYSTOR &#8216;16, 17:117:1. New York, NY, USA, 2016. ACM. URL: <a class="reference external" href="http://doi.acm.org/10.1145/2928275.2933272">http://doi.acm.org/10.1145/2928275.2933272</a>, <a class="reference external" href="http://dx.doi.org/10.1145/2928275.2933272">doi:10.1145/2928275.2933272</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="paper-mesos-ghodsi2011dominant" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id318">[274]</a></td><td>Ali Ghodsi, Matei Zaharia, Benjamin Hindman, Andy Konwinski, Scott Shenker, and Ion Stoica. Dominant resource fairness: fair allocation of multiple resource types. In <em>NSDI</em>, volume 11, 2424. 2011.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-mesos-ignazio-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id319">[275]</a></td><td>Roger Ignazio. <em>Mesos in Action</em>. Manning Publications Co., Greenwich, CT, USA, 1st edition, 2016. ISBN 1617292923, 9781617292927. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?id=3006364">http://dl.acm.org/citation.cfm?id=3006364</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudera" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id320">[276]</a></td><td>cloudera. Untangling apache hadoop yarn part 1 cluster and yarn basics. Web Page, 2015. URL: <a class="reference external" href="https://blog.cloudera.com/blog/2015/09/untangling-apache-hadoop-yarn-part-1/">https://blog.cloudera.com/blog/2015/09/untangling-apache-hadoop-yarn-part-1/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-architecture" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id321">[277]</a></td><td>Difference between application manager and application master in yarn? StackOverflow, 2015. URL: <a class="reference external" href="http://stackoverflow.com/questions/30967247/difference-between-application-manager-and-application-master-in-yarn">http://stackoverflow.com/questions/30967247/difference-between-application-manager-and-application-master-in-yarn</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hadoopapache" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id322">[278]</a></td><td>Hadoop Apache. Apache software foundation. Web Page, 2016. URL: <a class="reference external" href="https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/YARN.html">https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/YARN.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ibm-helix-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id323">[279]</a></td><td>Exploring big data with helix: finding needles in a big haystack. Web Page. URL: <a class="reference external" href="https://sigmodrecord.org/publications/sigmodRecord/1412/pdfs/09_industry_Ellis.pdf">https://sigmodrecord.org/publications/sigmodRecord/1412/pdfs/09_industry_Ellis.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-slurm" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id324">[280]</a></td><td>Slurm. Web Page. URL: <a class="reference external" href="https://slurm.schedmd.com/">https://slurm.schedmd.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-slurmschedmdsite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id325">[281]</a></td><td>Slurm website. Web Page. Accessed: 2017-1-28. URL: <a class="reference external" href="https://slurm.schedmd.com/overview.html">https://slurm.schedmd.com/overview.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-slurmplatformssite" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id326">[282]</a></td><td>Slurm supported platforms. Web Page. Accessed: 2017-1-28. URL: <a class="reference external" href="https://slurm.schedmd.com/platforms.html">https://slurm.schedmd.com/platforms.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pilot-job-falkon-paper-2007" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id327">[283]</a></td><td>Ioan Raicu, Yong Zhao, Catalin Dumitrescu, Ian Foster, and Mike Wilde. Falkon: a fast and light-weight task execution framework. In <em>ACM SC07</em>. 2007.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pilot-job-htcaas-paper-2007" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id328">[284]</a></td><td>Jik-Soo Kim, Seungwoo Rho, Seoyoung Kim, Sangwan Kim, Seokkyoo Kim, and Soonwook Hwang. Htcaas: leveraging distributed supercomputing infrastructures for large-scale scientific computing. In <em>ACM MTAGS 13</em>. 2013.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-pilot-job-paper-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id329">[285]</a></td><td>Matteo Turilli, Mark Santcroos, and Shantenu Jha. A comprehensive perspective on pilot-job systems. In <em>ACM arXiv</em>, 126. March 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wiki-cinder" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[286]</td><td><em>(<a class="fn-backref" href="#id330">1</a>, <a class="fn-backref" href="#id332">2</a>)</em> Cinder - openstack. Web Page. Accessed: 2017-1-21. URL: <a class="reference external" href="https://wiki.openstack.org/wiki/Cinder">https://wiki.openstack.org/wiki/Cinder</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="book-cinder" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id331">[287]</a></td><td>Dan Radez. <em>OpenStack Essentials</em>. Packt Publishing Ltd., 2015. ISBN 978-1-78398-708-5. URL: <a class="reference external" href="http://ebook.konfigurasi.net/Openstack/OpenStack">http://ebook.konfigurasi.net/Openstack/OpenStack</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-fuse" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id333">[288]</a></td><td>Fuse site. Web Page. URL: <a class="reference external" href="http://fuse.sourceforge.net">http://fuse.sourceforge.net</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="fuse-paper-hptfs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id334">[289]</a></td><td>Xianbo Zhang, David Du, Jim Hughes, Ravi Kavuri, and Sun StorageTek. Hptfs: a high performance tape file system. In <em>Proceedings of 14th NASA Goddard/23rd IEEE conference on Mass Storage System and Technologies</em>. 2006. URL: <a class="reference external" href="https://www.dtc.umn.edu/publications/reports/2006_11.pdf">https://www.dtc.umn.edu/publications/reports/2006_11.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="fuse-paper-cloudbb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id335">[290]</a></td><td>T.&nbsp;Xu, K.&nbsp;Sato, and S.&nbsp;Matsuoka. Cloudbb: scalable i/o accelerator for shared cloud storage. In <em>2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)</em>, 509518. Dec 2016. <a class="reference external" href="http://dx.doi.org/10.1109/ICPADS.2016.0074">doi:10.1109/ICPADS.2016.0074</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-lustre" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id336">[291]</a></td><td>Lustre. Webpage. Accessed: 2017-01-28. URL: <a class="reference external" href="http://lustre.org/">http://lustre.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikigpfs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[292]</td><td><em>(<a class="fn-backref" href="#id337">1</a>, <a class="fn-backref" href="#id338">2</a>, <a class="fn-backref" href="#id339">3</a>, <a class="fn-backref" href="#id340">4</a>)</em> Wikipedia. Ibm general parallel file system. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/IBM_General_Parallel_File_System">https://en.wikipedia.org/wiki/IBM_General_Parallel_File_System</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-spectrumscale" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id341">[293]</a></td><td>IBM. Ibm spectrum scale. Web Page, January 2017. accessed 2017-01-29. URL: <a class="reference external" href="http://www-03.ibm.com/systems/storage/spectrum/scale/">http://www-03.ibm.com/systems/storage/spectrum/scale/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-gffs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id342">[294]</a></td><td>Gffs. Webpage. Accessed: 2017-01-28. URL: <a class="reference external" href="http://genesis2.virginia.edu/wiki/Main/GFFS">http://genesis2.virginia.edu/wiki/Main/GFFS</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-amazon-s3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id343">[295]</a></td><td>Amazon s3. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="https://aws.amazon.com/s3/">https://aws.amazon.com/s3/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-amazon-s3-docs" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id344">[296]</a></td><td>Using Amazon S3. Web Page. Accessed: 2017-1-27. URL: <a class="reference external" href="http://docs.aws.amazon.com/AmazonS3/latest/gsg/CopyingAnObject.html">http://docs.aws.amazon.com/AmazonS3/latest/gsg/CopyingAnObject.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azure-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[297]</td><td><em>(<a class="fn-backref" href="#id345">1</a>, <a class="fn-backref" href="#id347">2</a>)</em> An Introduction to Windows Azure BLOB Storage. Web Page, July 2013. URL: <a class="reference external" href="https://www.simple-talk.com/cloud/cloud-data/an-introduction-to-windows-azure-blob-storage/">https://www.simple-talk.com/cloud/cloud-data/an-introduction-to-windows-azure-blob-storage/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-azure-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id346">[298]</a></td><td>Get started with Azure Blob storage (object storage) using .NET \textbar  Microsoft Docs. Web Page. URL: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/storage/storage-dotnet-how-to-use-blobs">https://docs.microsoft.com/en-us/azure/storage/storage-dotnet-how-to-use-blobs</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-google-cloud-storage" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id348">[299]</a></td><td>Google cloud storage. Accessed: 02-06-2017. URL: <a class="reference external" href="https://cloud.google.com/storage/docs">https://cloud.google.com/storage/docs</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="cloud-portability-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id349">[300]</a></td><td>Antonio&nbsp;Esposito Beniamino Di&nbsp;Martino, Giuseppina&nbsp;Cretella. <em>Cloud Portability and Interoperability</em>. Springer International Publishing, New York City, USA, illustrated edition, 2015. ISBN 331913700X, 9783319137001.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jclouds" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id350">[301]</a></td><td>JClouds, components. Web Page. Accessed: 2017-01-20. URL: <a class="reference external" href="https://jclouds.apache.org/">https://jclouds.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-occi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id351">[302]</a></td><td>Open&nbsp;Grid Forum. Open cloud computing interface. Web Page. Accessed: 2017-1-17. URL: <a class="reference external" href="http://occi-wg.org/">http://occi-wg.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nyren-edmonds-papaspyrou-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id352">[303]</a></td><td>Ralf Nyren, Andy Edmonds, Alesander Papaspyrou, Thijs Metsch, and Boris Parak. Open cloud computing interface core. OGF Published Document GWD-R-P.221, Global Grid Forum, Open Grid Forum, P.O. Box 1738, Muncie IN 47308, USA, September 2016. Accessed: 2017-1-17. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.221.pdf">https://www.ogf.org/documents/GFD.221.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="drescher-parak-wallom-2015" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id353">[304]</a></td><td>Michel Drescher, Boris Parak, and David Wallom. Occi compute resource templates profile. OGF Published Document GWD-R-P.222, Global Grid Forum, Open Grid Forum, P.O. Box 1738, Muncie IN 47308, USA, April 2015. Accessed: 2017-1-17. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.222.pdf">https://www.ogf.org/documents/GFD.222.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nyren-edmonds-metsch-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id354">[305]</a></td><td>Ralf Nyren, Andy Edmonds, Thijs Metsch, and Boris Parak. Open cloud computing interface - http protocol. OGF Published Document GWD-R-P.223, Global Grid Forum, Open Grid Forum, P.O. Box 1738, Muncie IN 47308, USA, September 2016. Accessed: 2017-1-17. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.223.pdf">https://www.ogf.org/documents/GFD.223.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nyren-feldhaus-parak-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id355">[306]</a></td><td>Ralf Nyren, Florian Feldhaus, Boris Parak, and Zdenek Sustr. Open cloud computing interface -json rendering. OGF Published Document GWD-R-P.226, Global Grid Forum, Open Grid Forum, P.O. Box 1738, Muncie IN 47308, USA, September 2016. Accessed: 2017-1-17. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.226.pdf">https://www.ogf.org/documents/GFD.226.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="edmonds-metsch-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id356">[307]</a></td><td>Andy Edmonds and Thijs Metsch. Open cloud computing interface - text rendering. OGF Published Document GWD-R-P.229, Global Grid Forum, Open Grid Forum, P.O. Box 1738, Muncie IN 47308, USA, September 2016. Accessed: 2017-1-17. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.229.pdf">https://www.ogf.org/documents/GFD.229.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="saga-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[308]</td><td><em>(<a class="fn-backref" href="#id357">1</a>, <a class="fn-backref" href="#id359">2</a>)</em> Shantenu Jha, Hartmut Kaiser, Andre Merzky, and Ole Weidner. Grid interoperability at the application level using saga. In <em>07 Proceedings of the Third IEEE International Conference on e-Science and Grid Computing</em>. December 2007.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-saga-ogf-document" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id358">[309]</a></td><td>Open grid forum - document. webpage. Accessed : 02-01-2017. URL: <a class="reference external" href="https://www.ogf.org/documents/GFD.90.pdf">https://www.ogf.org/documents/GFD.90.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-puppet-wiki-puppet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[310]</td><td><em>(<a class="fn-backref" href="#id360">1</a>, <a class="fn-backref" href="#id363">2</a>)</em> Puppet software. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Puppet_(software)">https://en.wikipedia.org/wiki/Puppet_(software)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-puppet-puppet-site" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id361">[311]</a></td><td>Puppet faq. Web Page. URL: <a class="reference external" href="https://puppet.com/product/faq">https://puppet.com/product/faq</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-puppet-slashroot" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id362">[312]</a></td><td>How puppet works. Web Page. URL: <a class="reference external" href="http://www.slashroot.in/puppet-tutorial-how-does-puppet-work">http://www.slashroot.in/puppet-tutorial-how-does-puppet-work</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="chef-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id364">[313]</a></td><td>Matthias Marschall. <em>Chef Infrastructure Automation Cookbook</em>. Packt Publishing, 2013. ISBN 9351105164 and 9789351105169.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-chef-commercial" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id365">[314]</a></td><td>Chef commercial support. Web Page. URL: <a class="reference external" href="https://www.chef.io/support/">https://www.chef.io/support/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ansible" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id366">[315]</a></td><td>Ansible. Webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Ansible_(software)">https://en.wikipedia.org/wiki/Ansible_(software)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ansible2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id367">[316]</a></td><td>Ansible. Webpage. URL: <a class="reference external" href="https://docs.ansible.com/ansible/index.html">https://docs.ansible.com/ansible/index.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-boto" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id368">[317]</a></td><td>boto components. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="http://boto.cloudhackers.com/en/latest/">http://boto.cloudhackers.com/en/latest/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-boto-github" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id369">[318]</a></td><td>boto-github components. Web Page. Accessed: 2017-1-25. URL: <a class="reference external" href="https://github.com/boto/boto3">https://github.com/boto/boto3</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-boto3-documentation" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id370">[319]</a></td><td>boto3-documentation components. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://boto3.readthedocs.io/en/latest/">https://boto3.readthedocs.io/en/latest/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-boto-amazon-python-sdk" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id371">[320]</a></td><td>boto-amazon-python-sdk components. Web Page. Accessed: 2017-1-26. URL: <a class="reference external" href="https://aws.amazon.com/sdk-for-python/">https://aws.amazon.com/sdk-for-python/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cobbler" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id372">[321]</a></td><td>Cobbler. Web Page. Accessed: 2017-02-05. URL: <a class="reference external" href="http://www.theregister.co.uk/2008/06/19/red_hat_summit_2008_cobbler/">http://www.theregister.co.uk/2008/06/19/red_hat_summit_2008_cobbler/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="razorwiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id373">[322]</a></td><td>PuppetLabsRazor. Puppetlabsrazor. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://github.com/puppetlabs/Razor/wiki">https://github.com/puppetlabs/Razor/wiki</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="razorpuppet" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id374">[323]</a></td><td>PuppetLabsRazor. Puppetlabsrazor. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://puppet.com/blog/introducing-razor-a-next-generation-provisioning-solution">https://puppet.com/blog/introducing-razor-a-next-generation-provisioning-solution</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="juju-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id375">[324]</a></td><td>Kent Baxley, JD&nbsp;la&nbsp;Rosa, and Mark Wenning. Deploying workloads with juju and maas in ubuntu 14.04 lts. In <em>Deploying workloads with Juju and MAAS in Ubuntu 14.04 LTS</em>. Dell Inc, Technical White Paper, may 2014.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-juju" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id376">[325]</a></td><td>Canonical. Juju site. Web Page. URL: <a class="reference external" href="https://www.ubuntu.com/cloud/juju">https://www.ubuntu.com/cloud/juju</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-heat-blog-introduction" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id377">[326]</a></td><td>Blog on introduction to heat. webpage. Accessed : 01-15-2017. URL: <a class="reference external" href="http://blog.scottlowe.org/2014/05/01/an-introduction-to-openstack-heat/">http://blog.scottlowe.org/2014/05/01/an-introduction-to-openstack-heat/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-heat-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id378">[327]</a></td><td>Wikipedia link for heat. webpage. Accessed : 01-15-2017. URL: <a class="reference external" href="https://wiki.openstack.org/wiki/Heat">https://wiki.openstack.org/wiki/Heat</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openstack" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id379">[328]</a></td><td>Openstack. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="https://www.openstack.org/">https://www.openstack.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sahara" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id380">[329]</a></td><td>Sahara. Web Page. Accessed:1/16/2017. URL: <a class="reference external" href="http://docs.openstack.org/developer/sahara/">http://docs.openstack.org/developer/sahara/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="cis1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id381">[330]</a></td><td>Cloud and systems management. webpage. URL: <a class="reference external" href="http://www.cisco.com/c/en/us/products/cloud-systems-management">http://www.cisco.com/c/en/us/products/cloud-systems-management</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="cis2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id382">[331]</a></td><td>Stuart Miniman. Cisco moves up the cloud stack with intelligent automation. webpage, 2011. URL: <a class="reference external" href="http://wikibon.org/wiki/v/Cisco_Moves_Up_the_Cloud_Stack_with_Intelligent_Automation">http://wikibon.org/wiki/v/Cisco_Moves_Up_the_Cloud_Stack_with_Intelligent_Automation</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wikichef" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id383">[332]</a></td><td>Wikipedia. Chef (software). Web Page, January 2017. accessed 2017-01-26. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Chef_(software)">https://en.wikipedia.org/wiki/Chef_(software)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-awsopsworks" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id384">[333]</a></td><td>Amazon. Aws opsworks. Web Page, January 2017. accessed 2017-01-25. URL: <a class="reference external" href="https://aws.amazon.com/opsworks/">https://aws.amazon.com/opsworks/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kubernetesdoc" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id385">[334]</a></td><td>Kubernetes site. Web Page. URL: <a class="reference external" href="https://kubernetes.io/docs/whatisk8s/">https://kubernetes.io/docs/whatisk8s/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-kuberneteswiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id386">[335]</a></td><td>Kubernetes wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Kubernetes">https://en.wikipedia.org/wiki/Kubernetes</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="plassnig-2015" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id387">[336]</a></td><td>Moritz Plassnig. Heroku-style application deployments with docker - dzone cloud. Web Page, Nov 2015. Accessed: 2017-1-17. URL: <a class="reference external" href="https://dzone.com/articles/heroku-style-application-deployments-with-docker">https://dzone.com/articles/heroku-style-application-deployments-with-docker</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="gonzalez-2015" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id388">[337]</a></td><td>Jose Gonzalez and Jeff Lindsay. Buildstep. Web Page, Jul 2015. Accessed: 2017-1-24. URL: <a class="reference external" href="https://github.com/progrium/buildstep">https://github.com/progrium/buildstep</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-winery" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id389">[338]</a></td><td>Eclipse winery. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://projects.eclipse.org/projects/soa.winery">https://projects.eclipse.org/projects/soa.winery</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="winery-paper-2013" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[339]</td><td><em>(<a class="fn-backref" href="#id390">1</a>, <a class="fn-backref" href="#id391">2</a>, <a class="fn-backref" href="#id392">3</a>)</em> Oliver Kopp, Tobias Binz, Uwe Breitenbcher, and Frank Leymann. Winery  a modeling tool for tosca-based cloud applications. In <em>11\textsuperscript th International Conference on Service-Oriented Computing</em>, 700704. Springer, December 2013.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-blueprints" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id393">[340]</a></td><td>Exercise: analyze business processes with ibm bpm blueprint. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="http://www.ibm.com/developerworks/downloads/soasandbox/blueprint.html">http://www.ibm.com/developerworks/downloads/soasandbox/blueprint.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-blueworks-live2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[341]</td><td><em>(<a class="fn-backref" href="#id394">1</a>, <a class="fn-backref" href="#id396">2</a>)</em> Blueworkslive. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://www.blueworkslive.com/home">https://www.blueworkslive.com/home</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-blueworks-live" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id395">[342]</a></td><td>Ibm blueworks live. Web Page. Accessed: 2017-1-24. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/IBM_Blueworks_Live">https://en.wikipedia.org/wiki/IBM_Blueworks_Live</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-terraform" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id397">[343]</a></td><td>Terraform components. Web Page. URL: <a class="reference external" href="https://www.terraform.io/intro/index.html">https://www.terraform.io/intro/index.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-terraform-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id398">[344]</a></td><td>James Turnbull. <em>The Terraform Book</em>. number&nbsp;9780988820258. Turnbull Press; 110 edition (November 26, 2016), 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wettinger-any2api" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id399">[345]</a></td><td>Johannes Wettinger, Uwe Breitenbcher, and Frank Leymann. Any2api - automated apification. In <em>Proceedings of the 5th International Conference on Cloud Computing and Services Science</em>, 475486. SciTePress, 2015. URL: <a class="reference external" href="https://pdfs.semanticscholar.org/1cd4/4b87be8cf68ea5c4c642d38678a7b40a86de.pdf">https://pdfs.semanticscholar.org/1cd4/4b87be8cf68ea5c4c642d38678a7b40a86de.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-any2api" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id400">[346]</a></td><td>Any2Api, components. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="http://www.any2api.org/">http://www.any2api.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-xen-wikipedia" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id401">[347]</a></td><td>Xen - wikipedia. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Xen">https://en.wikipedia.org/wiki/Xen</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-xen-overview" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id402">[348]</a></td><td>Xen project overview. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://wiki.xenproject.org/wiki/Xen_Project_Software_Overview">https://wiki.xenproject.org/wiki/Xen_Project_Software_Overview</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-xen-fl" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id403">[349]</a></td><td>Xen feature list. Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://wiki.xenproject.org/wiki/Xen_Project_4.7_Feature_List">https://wiki.xenproject.org/wiki/Xen_Project_4.7_Feature_List</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hypervisor" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id404">[350]</a></td><td>Hypervisor. WebPage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Hypervisor">https://en.wikipedia.org/wiki/Hypervisor</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-qemu" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id405">[351]</a></td><td>Qemu. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/QEMU">https://en.wikipedia.org/wiki/QEMU</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-qemuwiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id406">[352]</a></td><td>Qemu. WebPage. URL: <a class="reference external" href="http://wiki.qemu-project.org/index.php/Main_Page">http://wiki.qemu-project.org/index.php/Main_Page</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openvz-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id407">[353]</a></td><td>OpenVZ. Web Page, February 2017. Page Version ID: 764498783. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=OpenVZ&amp;oldid=764498783">https://en.wikipedia.org/w/index.php?title=OpenVZ&amp;oldid=764498783</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openvz-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id408">[354]</a></td><td>How To Create OpenVZ Container In OpenVZ \textbar  Unixmen. Web Page. URL: <a class="reference external" href="https://www.unixmen.com/how-to-create-openvz-container-in-openvz/">https://www.unixmen.com/how-to-create-openvz-container-in-openvz/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-openvz-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id409">[355]</a></td><td>Features. Web Page. URL: <a class="reference external" href="https://openvz.org/Features">https://openvz.org/Features</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-wiki-lxc" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id410">[356]</a></td><td>Linux containers. web page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/LXC">https://en.wikipedia.org/wiki/LXC</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-jpablo" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id411">[357]</a></td><td>Why use lxc (linux containers) ? web page. URL: <a class="reference external" href="http://www.jpablo128.com/why-use-lxc-linux-containers/">http://www.jpablo128.com/why-use-lxc-linux-containers/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-infoworld" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id412">[358]</a></td><td>Linux containers in use. web page. URL: <a class="reference external" href="http://www.infoworld.com/article/3072929/linux/containers-101-linux-containers-and-docker-explained.html">http://www.infoworld.com/article/3072929/linux/containers-101-linux-containers-and-docker-explained.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-nimbus-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id413">[359]</a></td><td>Nimbus wiki. Web Page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Nimbus_(cloud_computing)">https://en.wikipedia.org/wiki/Nimbus_(cloud_computing)</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-nimbus" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id414">[360]</a></td><td>Nimbus. Web Page. URL: <a class="reference external" href="http://www.nimbusproject.org/doc/nimbus/platform/">http://www.nimbusproject.org/doc/nimbus/platform/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nimbus-paper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id415">[361]</a></td><td><em>Rebalancing in a multi-cloud environment in Science Cloud &#8216;13</em>, ACM, 2013. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?id=2465854">http://dl.acm.org/citation.cfm?id=2465854</a>, <a class="reference external" href="http://dx.doi.org/10.1145/2465848.2465854">doi:10.1145/2465848.2465854</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudstack" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id416">[362]</a></td><td>Cloud Stack. Webpage. URL: <a class="reference external" href="https://cloudstack.apache.org/">https://cloudstack.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudstack2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id417">[363]</a></td><td>Cloud Stack. Webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Apache_CloudStack">https://en.wikipedia.org/wiki/Apache_CloudStack</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-core" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id418">[364]</a></td><td>Why coreos. Web Page. accessed: 2017-01-23. URL: <a class="reference external" href="https://coreos.com/why/">https://coreos.com/why/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-coreos" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id419">[365]</a></td><td>Coreos. Web Page. Accessed:1/27/2017. URL: <a class="reference external" href="https://www.coreos.com/">https://www.coreos.com/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-github-rkt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id420">[366]</a></td><td>Coreos/rkt. Web Page. Accessed:1/27/2017. URL: <a class="reference external" href="https://github.com/coreos/rkt/">https://github.com/coreos/rkt/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wiki-vmwareesxi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id421">[367]</a></td><td>Wikipedia. web-page. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/VMware_ESXi">https://en.wikipedia.org/wiki/VMware_ESXi</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="vmware-esxi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id422">[368]</a></td><td>VMware. web-page. URL: <a class="reference external" href="http://www.vmware.com/products/esxi-and-esx.html">http://www.vmware.com/products/esxi-and-esx.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-hortonworks-ambari" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id423">[369]</a></td><td>Hortonworks apache ambari. Web Page. Accessed: 2017-2-04. URL: <a class="reference external" href="http://hortonworks.com/apache/ambari/">http://hortonworks.com/apache/ambari/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ambari" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id424">[370]</a></td><td>Ambari. Web Page. Accessed: 2017-2-04. URL: <a class="reference external" href="https://ambari.apache.org/">https://ambari.apache.org/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-github-ambari" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id425">[371]</a></td><td>Github apache/ ambari. Web Page. Accessed: 2017-2-04. URL: <a class="reference external" href="https://github.com/apache/ambari/">https://github.com/apache/ambari/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-nagios" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id426">[372]</a></td><td>Nagios components. Web Page. Accessed: 2017-1-11. URL: <a class="reference external" href="https://www.nagios.org/projects/">https://www.nagios.org/projects/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nagios-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id427">[373]</a></td><td>David Josephsen. <em>Nagios: Building Enterprise-Grade Monitoring Infrastructures for Systems and Networks</em>. Prentice Hall Press, Upper Saddle River, NJ, USA, 2nd edition, 2013. ISBN 013313573X, 9780133135732.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nagios-paper-2012" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id428">[374]</a></td><td>C.&nbsp;Issariyapat, P.&nbsp;Pongpaibool, S.&nbsp;Mongkolluksame, and K.&nbsp;Meesublak. Using nagios as a groundwork for developing a better network monitoring system. In <em>2012 Proceedings of PICMET &#8216;12: Technology Management for Emerging Technologies</em>, 27712777. July 2012.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="inca-book" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id429">[375]</a></td><td>Jinjun&nbsp;Chen Lizhe&nbsp;Wang, Wei&nbsp;Jie. <em>Grid Computing: Infrastructure, Service, and Applications</em>. Taylor &amp; Francis, 6000 Broken Sound Parkway NW, Suite 300 Boca Raton, FL 33487, 2009. ISBN 1420067664.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-inca" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id430">[376]</a></td><td>Inca, components. Web Page. Accessed: 2017-01-16. URL: <a class="reference external" href="http://inca.sdsc.edu/">http://inca.sdsc.edu/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-eduroam" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id431">[377]</a></td><td>Eduroam. Web Page. URL: <a class="reference external" href="https://www.eduroam.org/about/">https://www.eduroam.org/about/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="eduroam-paper-2005" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id432">[378]</a></td><td>Licia Florio and Klaas Wierenga. Eduroam, providing mobility for roaming users. <em>TERENA</em>, 2005. URL: <a class="reference external" href="https://www.terena.org/activities/tf-mobility/docs/ppt/eunis-eduroamfinal-LF.pdf">https://www.terena.org/activities/tf-mobility/docs/ppt/eunis-eduroamfinal-LF.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-keystone-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id433">[379]</a></td><td>Keystone wiki. Web Page. URL: <a class="reference external" href="https://wiki.openstack.org/wiki/Keystone">https://wiki.openstack.org/wiki/Keystone</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="cui2015security" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id434">[380]</a></td><td>Baojiang Cui and Tao Xi. Security analysis of openstack keystone. In <em>Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), 2015 9th International Conference on</em>, 283288. IEEE, 2015.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-cloudberrylab-kstn" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id435">[381]</a></td><td>Cloudberrylab website. Web Page. URL: <a class="reference external" href="https://www.cloudberrylab.com/blog/openstack-keystone-authentication-explained/">https://www.cloudberrylab.com/blog/openstack-keystone-authentication-explained/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-keystone" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id436">[382]</a></td><td>Openstack website. Web Page. URL: <a class="reference external" href="http://docs.openstack.org/developer/keystone/architecture.html">http://docs.openstack.org/developer/keystone/architecture.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-ldap" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id437">[383]</a></td><td>LDAP. Web Page. Accessed: 2017-02-02. URL: <a class="reference external" href="http://searchmobilecomputing.techtarget.com/definition/LDAP">http://searchmobilecomputing.techtarget.com/definition/LDAP</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-sentry" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id438">[384]</a></td><td>Apache sentry website. Web Page. URL: <a class="reference external" href="https://cwiki.apache.org/confluence/display/SENTRY/Sentry+Tutorial">https://cwiki.apache.org/confluence/display/SENTRY/Sentry+Tutorial</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ope1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id439">[385]</a></td><td>Openid. website. URL: <a class="reference external" href="http://openid.net/">http://openid.net/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ope2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id440">[386]</a></td><td>Openid. webpage. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/OpenID">https://en.wikipedia.org/wiki/OpenID</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="saml" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id441">[387]</a></td><td>Abhijeet Sandil. Sso strategy: authentication (saml) vs authorization (oauth). Web Page. Accessed: 2017-02-04. URL: <a class="reference external" href="https://www.linkedin.com/pulse/sso-strategy-authentication-vs-authorization-saml-oauth-sandil">https://www.linkedin.com/pulse/sso-strategy-authentication-vs-authorization-saml-oauth-sandil</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-chubby" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id442">[388]</a></td><td>Chubby site. Web Page. URL: <a class="reference external" href="https://research.google.com/archive/chubby.html">https://research.google.com/archive/chubby.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="chubby-paper-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id443">[389]</a></td><td>A.&nbsp;Ailijiang, A.&nbsp;Charapko, and M.&nbsp;Demirbas. Consensus in the cloud: paxos systems demystified. In <em>2016 25th International Conference on Computer Communication and Networks (ICCCN)</em>, 110. Aug 2016. URL: <a class="reference external" href="http://ieeexplore.ieee.org/abstract/document/7568499/">http://ieeexplore.ieee.org/abstract/document/7568499/</a>, <a class="reference external" href="http://dx.doi.org/10.1109/ICCCN.2016.7568499">doi:10.1109/ICCCN.2016.7568499</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zoo-overiew" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id444">[390]</a></td><td>Zookeeper - overview. Web Page. Accessed: 2017-01-23. URL: <a class="reference external" href="https://zookeeper.apache.org/doc/trunk/zookeeperOver.html">https://zookeeper.apache.org/doc/trunk/zookeeperOver.html</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zoo-wiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id445">[391]</a></td><td>Zookeeper - wikipedia. Web Page. Accessed: 2017-01-23. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Apache_ZooKeeper">https://en.wikipedia.org/wiki/Apache_ZooKeeper</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-zoo-ibm" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id446">[392]</a></td><td>Ibm - what is zookeeper. Web Page. Accessed: 2017-01-23. URL: <a class="reference external" href="http://www-01.ibm.com/software/data/infosphere/hadoop/zookeeper/">http://www-01.ibm.com/software/data/infosphere/hadoop/zookeeper/</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="giraffepaper" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id447">[393]</a></td><td>Xuanhua Shi, Haohong Lin, Hai Jin, Bing&nbsp;Bing Zhou, Zuoning Yin, Sheng Di, and Song Wu. Giraffe: a scalable distributed coordination service for large-scale systems. In <em>GIRAFFE: A Scalable Distributed Coordination Service for Large-scale Systems</em>, 110. 2014. URL: <a class="reference external" href="http://www.mcs.anl.gov/papers/P5157-0714.pdf">http://www.mcs.anl.gov/papers/P5157-0714.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="www-protobuf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id448">[394]</a></td><td>Protocol buffer. Web Page, September 2016. URL: <a class="reference external" href="https://developers.google.com/protocol-buffers/">https://developers.google.com/protocol-buffers/</a>.</td></tr>
</tbody>
</table>
</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2016, Gregor von Laszewski.<br/>
    </p>
  </div>
</footer>
  </body>
</html>